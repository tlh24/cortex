May 13 2021

The mode was (not so much is anyore..) decent at nulling the input neurons w inhibition, 
but is absoulte crap at the route-cause: it divergese to flat, all units identical.
It appears that rci (and, by extension, w_rci) degenerates ito flat before rce. 
Wait .. w_rci is not updated! It's ones - diag matrix, scaled. 
Yeah that might not be a good regularization... 
Also, with time rce becomes flat; starts out diverse enough. 
Ok, apparently the problem is that, in w_rc, weights corresponding to the input go to zero, while weights corresponding to output seem to saturate.  
This is definitey not the behavior we want, and is likely due to a combination of the different scales of the input->rc weights and the output->rc weights.  
wrc_update is simple hebbian outer-product of rc activation and concatenated ll & l2.
(aka only local information)
passgate update is similarly hebbian outer-product between in, out, and rc.  

Well,  what do we want?  
'rc' stands for route-cause: they should route (multiply) input to output (causes) conditional on both.  
Thus, it would seem that they need faster dynamics than the object neurons. 
But also one-hot dynamics?  Does this need to be iterated? Or does the w_rci matrix need to update too? Needs to be an asymmetry relative to the out neurons. 

June 9 2021

Testing the very simplified model with four input patterns, and only positive generator synapses, yields a network that (usually) predicts well (sometimes NaNs creep in there) -- but it's not sparse.  
One output neuron always takes care of the dominant mode of the data vector, the DC component i guess; it doesn't make sense to combat this, since the data is not independent! 
Should the network be whitening the data?  
In biology, sensory input is very typically whitened, often at the receptors themselves, due to adaptation. In ANN, this term is taken care of with a bias term (1.0).. 

On the one-hot four-pattern task, 
With dynamic inhibition: error mean 0.0570 / std 0.0146 rep 0.0471 std 0.0079
Without dynamic inhibition: error mean 0.0572 / std 0.0192 rep 0.0621 / 0.0201

So, the dynamic inhibition seems to be doing something 'useful'. 
But ... why is it not driving the error to zero? 
There are only four patterns, this should be possible.

OK so -- supplying the true information to l2 results in zero error, 
but only if the cube learning rule is turned off. 
If the true information is not supplied, and the learning rule is linear, then the error mean 0.0184 / std 0.0095 
This is with symmetric learning rates on the forward and reverse matrices. 

Making the backward learning slower than the forward makes the error greater. 
Making the forward learning slower than the backward makes the error less: 
tensor(0.0100) tensor(0.0096)
Making the inhibition learning faster than the forward makes error worse. 
Making the inhibition learning slower than forward makes error worse, too
tensor(0.0191) tensor(0.0099)
Increasing the backward learning to 0.013 slightly degrades performance
tensor(0.0122) tensor(0.0107)

If we extend the training time to 30k samples, Usually it solves the task exactly. 
tensor(0.0037) tensor(0.0063)
but sometimes it doesn't ...
If we increase the learning rates to 0.008 (forward) 0.008 (inhib) 0.014 (backward), it's again worse -- seems to occasionally fail (?)

This network, which can usually solve the four-pattern one-hot, cannot accurately solve the free pattern: 
tensor(0.0606) tensor(0.0065)
Why is this?? 

Can we solve it with linear algebra? 
Yes, clearly we can solve the linear section ('gen') not the inverse section ('invgen') -- this results in (unsurprsinigly) half the error: 
tensor(0.0352) tensor(0.0098)
@ 10k steps. 
Increasing the number of steps to 20k does not improve it much:
tensor(0.0293) tensor(0.0032)
it just reduces the variance. 
'Cheating' by supplying g2 to l2 pushes the error to zero, as it should. 
tensor(0.0009) tensor(0.0005)
further reducing the learning rate to 0.003 on forward weights @ 10k increases error: 
tensor(0.0532) tensor(0.0090)
same learning rates @ 20k: 
tensor(0.0218) tensor(0.0083)
@ 30k: 
tensor(0.0294) tensor(0.0061)

Ok will stick with 0.005 / 0.005 / 0.01 @ 10k. 

Increasing the input size to 16 (from 8) slightly reduces the error: 
tensor(0.0275) tensor(0.0052)
This makes sense as it's easier to estimate the 4 causes with more predictors. 

It does seem .. for this linear problem .. that disabling inhibition improves things. 
tensor(0.0284) tensor(0.0095)

What about changing the dimensionality of the output layer? 
size 6, 
tensor(0.0279) tensor(0.0076)
size 4 (same as generator)
tensor(0.0340) tensor(0.0091)
(of course then the network is doing much less compression..
zero compression should be perfect: nope!
tensor(0.0244) tensor(0.0055)

I guess now should think about multiple layers & further nonlinearities? 


June 18 2021

Fiddled around with the 2-axis task. 
With noise, we can reliably get solutions -- good!  I was discouraged last night. 
Notably, the other forms of homeostasis (slow homeostasis) don't work; they cause weight instabilities.  
This may be the cause of the problem with the other tasks.  
Still using the cubic hebbian learning rule on this binary task, 
since that worked so well for the XOR task.  
Also still using asymmetric learning rules, as discovered in the vectoral learning task; solutions are sensitive to these learning rates.  Doubling them breaks the network!  (And it does learn slowly already.. though I'bve not explored this extensively..)

This shows clearly how important small, interpretable, comprehensible tasks are.  The 2-axis task, I can solve easily 'by hand'. 
We should continue this development avenue with small numbers of inputs and outputs, while also testing on larger equivalent tasks! 

Note: linear hebbian learning rule does not work on this task. 
Note: inhibition is not required to solve the task, but it does not seem to effect the solution either. 
@ noise level 0.025, it does sometimes fail!  But this seems to be a sweet spot c.f. 0.05 or 0.002. 
Depends on the initialization..

Dec 2021 .. 

next test would be to see if, given both l2 and rcdes, the pg net can output l1i. 
Need to do that above.. 

yes, that seems to work as well, to a very high fidelity 
-- can predict l1 reliably with rc one-hot encoding. 
however however, it does not seem to set l2 to a static representation. 
will need to penalize (?) to get this behavior ? 

Dec 16 2021 
let's try one update step instead of five for improving continuity / smoothness in L2 representation.

Yes, one update step does work -- but it doesn't improve continutiy per se. 

Also what works: clamping the RC activation, but propagating the gradients fully through l2. 
If anything, this seems to work better than blocking the gradients. 
Works: error is ~ 5e-4 after 60k steps w batches of 8. 
Next, need to try propagating the gradients through all, including RC. 

++ Running the network twice, with no gradient-stop, and smoothing the activations of rc and l2 seems to further improve the performance of the network, without significantly slowing execution. 
batch size = 16.
slowloss ~ 2e-4 at 140k iterations. 
GPU load is only 2% when rc size = 14..

++ Trying to increase the size of the rc layer, see how that changes things. 
++ need to add a better form of logging -- eg print out gifs every 20k iterations or so. 
-- Huh, even with this very small network, am using an inordinate ammount of vram - 8gb!  what??!! 
-- Increasing rc size to 32 did not slow computation at all. GPU usage si now 3%, from 2%. 

Dec 22 2021

Tried a few things: 
-- clamping the PG weights to be [-0.001 1.0]
-- adding a ReLU to the matmul(w_, rc)
This revealed something interesting: 
the network has sufficient capacity to represent all stimuli just in the RC layer; 
(green); no l2 activation (black) is required!  
Black / l2, in this case, is all high / on. 
This is consistent with there being only 41 vision sensors, and the spatial frequency of the stimuli being well below f_s / 2 -- so, in practice, there are less than 16 independent degrees of variation in the data. 
Thus, even though this experiment 'failed', it revealed something useful:
we can't expect the network to generalize well if it doesn't need to. 
Loss gradually bottoms out to 6e-5 at 4e6 steps. 

Dec 30 2021

Tried dramatically decreasing the network size -- 4 l2 units, 8 rc units. 
Keeping the weight clamps resulted in L2 staying high all the time (near 1). 
Trying now with weights clamped to [-1 .. 1]
... 
This did not change L2, it's still always high. 
Try removing the ReLU when forming w2 from w_ and rc. 

Dec 31 2021

4 l2, 8 rc seems to be working (unrestricted..)
Let's try making the objects move faster so the contextual window is shorter! 
(Might need to add explicit set / reset..)
Increasing the velocity by 2.5x: 
velocity = (1.75 + randn() * 0.35) * scl * -2.5

Jan 1 2022
That seemed to make things worse!  prediction was more accurate and 'smoother' with the orginal speed.  
What if we increase the iterations? 
	from one to two. 
		obviously this is a good bit slower... 

Other idea: add dynamics to the synapses (or dendrites) themselves. 

Jan 2 2022

I've run the network for a long time now -- 
20440800 loss: 1.763e-03; (20e6 steps!)
And it does seem to be representing the 1D field with something that appears to be a shifter in the RC network, and something that appears to be more constant, in the l2 network. 
This is with two passes through the network in forward(self, inp)
And also spatial bluring on both l2 (2 passes) and rc (3 passes). 

I think, per 1999 Schmidhuber Feature extraction through Lococode, that we need to ammend the network such that the objective function forces sparse or factorial coding. 
Factorial coding is definitely what we want here ...
If we need to implement their rather complicated objective function modification, then so be it. 
First, I think, we need to replicate the 5 x 5 bar experiment, both with SGD and with their fancy modifications. 

Options: 
-- Change the objective function to improve source separation. 
	-- Problem: there is no guarantee that things will be topological; indeed it may be asking too much to assume that the arrived-at representation is topological... 
		-- Schmidhuber result did not find topological mappings
			It also hasn't been cited that much. 
		-- Well the cortex seems to do this just fine ... need to replicate some special sauce there! 
-- Add to the network additional layers that can un-permute the arbitrary mappings found by SGD
	-- Also consistent with above.  

Jan 20 2022
Spent yesterday trying to get MNIST working via the simplified feedback path. 
It works, sorta -- but is prone to instability, possibly chaotic instability. 
Dialing it back & trying a stimuli with a known distribution : just a bunch of gaussian bumps
It's still very unstable. 
But, l1e activations don't seem to decay to zero (at least)
(This problem should be trivially solvable with PCA)

Alright, it's working better -- but I can't be sure about the inhibition though. 
It takes a long time to wind up, and once it is wound up, it seems unstable / diverges. hm. 

Two current problems: 
-- The inhibition is sensitive to tuning / it does not completely abolish correlational structure in the sequence. 
-- Learning rate very much depends on the number of inputs, which determines the strength of the resulting activation. 
	While the first layer seems to have a broad plateau of stability, the second layer does not; it likes to either go to all-on E or all off E (input)
	Meanwhile, there are still modulations to S. 
	
Todo presently: 
	** Test & verify the function of inhibition to approximate the inverse covaraince matrix. 
	Might need multiple steps.. 
	** Make sure learning rate scaling makes sense no matter the input - output size of a neuron
	
Jan 21 2022
OK, the inhibition is decidedly not working the way we expect or want -- it does not reliably converge to the desired number of of underlying factors. 
(However, in combination with the feedforward hebbian rule, it might ...)
Nonetheless, keeping the inhibition in the network generally seems to improve network function. 

The first layer now is broadly stable, it just tends to have low-activity tuning artifacts, due to the nonlinearity in the hebbian rule: infrequent strong LTP compensates for frequent small LTD.  
Might be able to tune a meta-parameter to correct for this. 
	In the brain, I suspect this is additionally handled with space: synapses cluster together on dendrites, which spatially organize (maybe???) to allow for the nonlinear interactions suporting local filter maintenance / removal of non spatially concordant info.

Need to tackle the problem of learning rate depending on the number of inputs. 
	In the brain: most neurons of a given class have about the same number of synapses? 
Can't you just scale the learning rate by the 1/(number of inputs)? 
This would allow equivalent steps to the resulting activity. 

March 11 2022

Been awhile ... eh wells.  
In further pondering of the task, it seems unreasonable or difficult to force invariance transformations, such as translations, to adopt a one-hot encoding.  
In the case of a 32x32 input, 32x32 output, and one-hot 1D translation vector, this requires storage of a full 32MB translation tensor.  Quite insane.  

Instead, we know we have ~ 86e9 neurons, of which 19-23e9 neurons are in the neocortex.  
Allegedly the cortex contains about 1.5e14 synapses (150 trillion) .. I don't know why i seem to recall there being 100 trillion synapses in the brain. 
This equates to 7500 synapses / neuron, which is maybe in the right ballpark. 
Obviously there are a good number in the cerebellum, thalamus, hippocampus, etc.
Meanwhile, the cerebellum allegedly has 70 billion neurons (this doesnt add up?) 

Anyway, if we have a naive shifter architecture, there are 1k input neurons, 1k output neurons, 32 translation neurons, and 32M three-part synapses, which equates to 15k synapses / neuron.  
The problem is that the number of synapses scales as the third power of the input dimensionality, which clearly doesn't work
n synapses per neuron = (n * n) * (n * n) * n / (2*n*n +n) ~= n^3
Yeah, not ok. 

In comparison, using activity rather than space to encode location, you need like one operation to shift things around, and only a handful of synapses. 
	That said, learning in the dumb af expansion above is easy, whereas learning with activity = location is ???

March 15 2022
Some shower thoughts today:
-- Conjunctive features = signals that co-occur, and hence are easy to extract with nonlinear, Hebbian synapses.  This is done.
	-- This is similar to a linear "AND" gate -- signals co-occur commonly. Patches ala Olshausen. 
	-- The reason why we can't do anything else with conjunctive extraction on the second layer is that there are no more easily accessible statistics. (??)
	-- Discriminative learning does have purchase on these problems; it will push apart clusters with different labels & build up discrimination (=classification) in this way.  We don't have this luxury with unsupervised learning.
		Maybe we do have it with self-supervised learning?
		It certainly seems to work with GANs (very well) / VAEs (pretty well)
-- Need an equivalent to the "OR" gate, which can 'destroy' information ala a max-pooling layer. OR = invariance; ignore the patterns of variation within a weight matrix.
	Thinking: in a biological neuron, everything in the set of inputs that create a spike can be considered to be in the invariant set.
		This of course is nonlinear and sorta additive: two inputs that are both in the projection set will add sub-linearly
		-- A normal ReLU artificial neuron has a threshold hyperplane which divides up the set, and planes parallel to this division are invariant. (and planes on the left side are straight ignored!)
	In MNIST, when you train with augmented data (small translations and rotations), you explicitly 'teach' the network that these patterns of variation don't matter.
	-- This could be done in a different way, though: if you move your eyes around, one assumption is that the world has not changed, and hence that the observed patterns of variation you can OR over.
		The other idea is that the patterns of variation can be well explained by oculomotor behavior, head motion, or self motion.
		-- Want generated behavior to "explain away" factors of variation of the sensory input,
			And for motor-generated factors to train up a more general purpose extractor in the case of non-self-generated behavior.
			"Self supervised" in this sense, or at least "scientifically (experiments) supervised"

A priori, how do we know that certain axes are irrelevant?
	If we do know, then can move the discrimination plane in a perceptron to be parallel to these axes.
		There is the added complexity of inhibition & plastic inhibitory synapses; originally I was thinking of using these as purely sparsifying, but that proved to be non-essential.  They can of course effect the invariant planes (dramatically!)
	Without supervisory signals, there is no way to know what is irrelevant!? 
		Other than things that do-not co-occur (manifold learning)
	With supervision (eg. knowing all the digits in a given class), it becomes easier to know dimensions to be irrelevant, and to OR over them.
What if we approach it a different way?
	In a given digit, dark pixels say in the middle of the image can curve to the left or right or up; these depend of course on what digit the image is part of.  From the data, you can infer that completely random instances of dark patches do not co-occur; they tend to have higher level structure.  
	Why does the cortex_mnist.py not sparsely extract these features?  
	It should be possible just with the basic thresholding nonlinearity, right? 
		This would only implicitly start “oring” irrelevant dimensions.  But that should be OK. 
	The problem is that -- when there is an “or” of irrelevant dimensions, **we don’t know which one to recreate.**
		In the trivial sense, we have multiple translations or variations of the same digit. 
		If you know that one patch is active, then you can update your prior that neighboring patches are also active, conditional on these patches, and or conditional on higher-level features, hypothetically with a high degree of sparsity.  
	Then, with BTSP and-or STDP, you can ‘store’ the linkages (causes) of the activations, and thereby store information like location in a scene.  
		I would think that nonlinear interaction of synapses in dendrites to be a critical element of this algorithm. 

March 17 2022
For the AND circuit, i guess log-transform?  but then have to exponentiate..
really want:
	weight * AND ( outer ( input_vector ))
or even:
	weight * AND ( outer ( outer (input_vector), input_vector) )
In the shape task, just a single outer product should suffice:
the number of terms in individual AND expressions is two.
but more broadly, we need something with more expressive power,
something that allows arbitrary number of elements into the expressions.
(i'm thinking this mostly spatially, in terms of dendritic trees..)
but maybe for now we just stick with outer()..

If we want to "reasonably" approximate biophysics:
1 -- Expand the input vector length N by sampling, with replacement, Np times.
2 -- Take a given input vector and project through a random Npx3 matrix into 3d space.
3 -- Do the 1 & 2 for the next layer, size M and Mp.
4 -- These are the locations of the synapses. Run a partnering algorithm over the location to set up the presence - possibility matrix.
	(It's not all-to-all, but might be close in a sparse sense)
5 -- When simuulating, record the location as well as post-synaptic cell identity of each presynaptic potential.
6 -- Convolve the PSPs with a spatial (temporal) filter; if the 'membrane potential' exceeds some threshold (how is this adapted or learned??), then emit a dendritic spike.
7 -- Forward the local dendritic membrane potential to the soma / SIZ for the "OR" operation.

THe aspect which most impresses me is that the biological realizaton is smoothly sloppy: there is a continuum of sets of synapses which may interact nonlinearly, due to spatial location.
To accurately emulate this, would need to do a maybe 2d projection of sparse sampling w replacement of the presynaptic neurons, convolve this with a gaussian filter (discretized?), threshold.
	The convolution of course can be precomputed based on location and represented as a matrix-vector product (i think)

Maybe a way to emulate this sort of phenomena is to use hashing:
Np is hashed into Mp buckets.
nonlinear thresholding operates over the buckets (and learning also operates over the buckets?!)
This doesn't offer a continuum, but it might work well enough for binary-type problems.
For more spatial problems we can do the embedding and pre-bake a convolution matrix.
	You still need continuous variables tp project into..
		The convolution may help with generalization, I think!

Well, anyway, start with outer, maybe move to hashing, then maybe 2d
Need to make sure learning works.
And inhibition / inversion is nonobvious / has not be fully understood quite yet.
	Maybe it's easy when we have negative weights .. ?

March 17 2022

Thinking we might need a learnable bias here -- for the hidden variables (3),
you need their complement (boolean NOT) for conjunctive learning,
When learning starts, the magnitude of the hidden variables are small,
so the magnitude of the naive complements are large,
which skews the learning.
What if we just invert the signals and allow for negative weights?
No, negative weights did not work well in the past..
from this experience, staying with positive reals is better. (and biologically realistic..)
The other option is to modulate the inverse based on 2x the moving average.
So if the average is @ target, 0.5, then the inverse is the boolean NOT
If the average is lower, say 0.1, then the inverse is 0.2
	Maybe this is where plastic inhibition comes in
		It means that generally inverse variables are only local
		(GABA neurons usually don't have long axons)
		which is consistent with biology at least..

Ok ... that maybe works.  But the forward weights seem to be getting stuck!
Let's check.
Yes, definitely getting stuck. This is a primary problem.
Also a problem: hebbian learning rule from cortex_mist strongly favors sparse representations.
	The backwards network works perfectly.
	The weight matrix is not sparse, though.. it ideally should be only a few terms, but is about 20% full.
	Maybe this is ok?
		Changing the scaling on the cube nonlinearity does not seem to change much ...
		it learns the same statistics.
	Yet, when i write it out by hand, I don't need all these terms.
		It doesn't help that I haven't eliminated the 2x redundancy in the second outer() application.
Observation:
	Clearly, the bins are unnecessary.
	If a AND-product is on, then it can instantly sum, and we don't need ddendritic bins.
Other thought:
	In biology the weights are before the AND; here we have the weights after.
	Either one might be workable.
	Right now after seems to be sufficient: just make it work!
---
Wonder if even the most trivial gating (e.g. which dendrite has residual calcium -> enable LTP/LTD) might be sufficient for forward and reverse credit-assignment?

March 21 2022
Spent most of the day looking over Baldwin effect 1992 and Symmetry / Simplicity in evolved structures,
Then about an hour working on cortex_andor.py
Enabled supervision, and in this case - when the network is taught to encode the 8 different images in 3 bits, it seems to work just fine.
This experiment did uncover the fact that it's sensitive to the learning rate.
So, need to make sure that supervised learning works before proceeding to unsupervised learning.
Now running unsupervised, and the second layer has a strong tendency to get stuck in encoding schemes (= forward weight matrices) that poorly represent the data / poorly encode the space.
Clearly, need to think about this more, and possibly consider how 'errors' and 'reinforcement' is propagated around this (rather simple) two-layer network.
Will rest on it!
	Aside: with the supervised task, as time goes on the backward weights tend to sparsify = reverse projection is simpler hence more likely to be getting at the true structure of the data (?!)

March 22 2022.
So many twos!
Ok, need to work on the 'stuck' effect.
Supervised: yes it works, very quickly.
Unsupervised: seems to depend on the noise level; lower noise slows learning and gives more time for better codes?

OK, so reducing the problem to just two images (both in the upper left hand side) does not cause convergence.
it *should* just be a one-bit task, super duper easy.
	(the backward pass is still good, obviously)
Right .. it does seem to work just fine, with alternating presentations.

Ugh, the system is very sensitive to parameters.
For example, it doesn't work well if the allowable negative weights are clamped to
	0.25
	or
	0.0025
	but it works if it's clamped to
	0.025
	why??
		This is not necessarily a good sign;
		Might need to think about symmetries
			perhaps some levels of clamp allow escape w noise?

Reparameterized the network, to allow for 4 bits of hidden state ..
Seems to solve the 4-symbol task (sometimes .. hmm)
What about 8-symbol?
Still seems to get stuck!

What about adding push-pull capability?
E.g. the forward regresses toward the 'n' variable (with negation)
If there is strong residual error, you can push the weights in the opposite direction from the positive hebbian term:
just reverse the error term, and complement the hidden state.
-- This doesn't provide much more information, alas.
The problem is that if a unit is 'off', you can't turn it more off. Hah!

Definitely the problem is gettign stuck in local minima -- when iterating through data presentations, the + dw exactly counterbalances the - dw.
At least, with identical ordering of the stimulus.

Another problem: the system is not long-term stable, it seems to degenerate to a two-state represenation.  Hm. One thing at a time, now.

March 23 2022

Ok, trying to figure out this metastable / stuck problem.
<didn’t make it far, apparently.. 
think I need to just implement biology hah>

March 27 2022

Woke up this morning with a pretty clear idea of what needs to happen at the company / what the pitch needs to be: 

1. How do you ‘solve’ arbitrarily hard problems, perhaps problems with difficult scaling properties, e.g. large or unbounded action / perception dimensionality?  
1.1 RL works to some degree, and has ‘solved’ tasks like Atari (Agent 57, etc).  This is a very significant advancement.  But the action space is small (a few buttons), and the perception space is constrained (8 colors, limited resolution, 2D sprite world, simple game mechanics, etc). 
1.2 Humans obviously can solve these problems, and most importantly they become very effective at solving them. 
1.3 How? 
2. One avenue is that humans factorize distributions in both perceptual and motor domains.  Perception is perhaps most obvious: we factorize objects into attributes; the flip side of this process is the ‘binding’ problem, whereby attributes are bound into hierarchies of objects.  These attributes include important details like pose, lighting, etc.  (Details that Hinton’s Capsule nets / GLOM model try to get at..)
2.1 A deeper philosophical question is *why* the world tends to be readily factorizable into low-dimension components, almost universally (but see: our brain; and also see, gene regulatory networks)
2.2 Motor is sorta the opposite direction: the brain factorizes patterns of output over the space of muscles and their synergies.  
2.3 Of course there are a lot of additional details to both problems .. 
2.4 ... but an element that can readily solve both action and perception problems is a memory that can be indexed either by “output” or “input” (e.g. auto-associator, old idea here), and, most importantly -- to counter the curse of dimensionality and the aforementioned factorizable nature of the physical world -- the “output” addressing is factorized / disentangled. 
2.4.1 This in turn allows low-power learning algorithms (like RL) to have enough leverage to do things like fast-mapping (in dogs) and interaction with the physical world... and maybe even language / grammar?  
3. Very well, so we need a self-factorizing auto-associator.  I propose this is what the cortex is doing in mammals. 
3.1 And probably plenty of other things... but still. 
3.2 What is the cortex, then?  Laminar structure, with some columnar structure, notable input (layer 4) association (1, 2-3) output (5 & 6); each layer has ~80% excitatory neurons and 20% inhibitory neurons.  The inhibitory neurons have a great number of specializations, both morphologically, electrochemically, and gene-expression wise.  Three major classes: PV, SST, VIP.  Exitatory neurons in contrast are of fewer classes and firing properties (at least, to my limited knowledge).  The excitatory neurons project, the inhibitory neurons generally do not, though there are exceptions here. 
3.3  Strong motifs in the cortex: 
3.3.1  E-I balance.  Most neurons, of both polarities, are in a constant state of push-pull, where usually excitatory and inhibitory input balances
3.3.2  Hebbian or local-type plasticity: synaptic ‘weight’ or coupling changes happen based on almost-entirely local changes, with modulation from retrograde signals, neuromodulators, iternal signaling pathways, etc. 
3.3.2.1  Some of these pre & post changes are very quick, so called behavioral timescale plasticity.  
3.3.3 Dales law: neurons generally emit one primary neurotransmitter; glutamate or GABA.  This of course has exceptions & many release multiple neurotransmitters -- but is a useful starting heuristic. 
3.3.4 Highly nonlinear membranes, inclusive the dendrites.  Some (many?) dendrites have dendritic spikes / plateau potentials, which can change the way signals are transmitted to the soma / spike initiation zone.  
3.3.5 1k - 10k synapses per neuron, roughly.  Most synapses have about the same ‘weight’, and the distribution of these weights is not agreed upon but does not have the resolution of 32-bit or even 16-bit floating point numbers.  (Nvidia suggests 8-bit logarithmic..)
3.3.5.1 Yet synapses certainly have more state! 
3.3.6 (Most obvious) The same rough cytoarchitecture throughout, with specializations say in auditory cortex (faster firing rates / high resolution timing information?) or in striate cortex (axons, e.g. -- probably to represent long-distance spatial relationships?)
3.3.7 Speaking of timing, there is a preponderance of spike-timing dependent plasticity.  Though the degree to which this exists, and how it works in the overall function of the cortex, is not agreed on.  STDP likely depends on much more than just pre-post timing; pre-post-pre and post-pre-post triplets have their own changes. 
3.3.8 Prediction *seems* to be a common motif, particularly for sensory encoding. This has obvious utility for animals interacting with the world (or simulating it, in the case of cognition)
3.3.9 Most feedback is local and low-order (in the poles and zeros sense).  Homeostatic effects are ubiquitous in biology; it’s just a good way to build a system. 
(.....)
4. OK, how to synthesize all these disparate details?  
4.1 It’s a feedback control system that tries to compress and predict (aka represent) the “input data” (say vision). 
4.2 Compression is via factorization, per above, to fight the curse of dimensionality. 
4.3 Factorization allows for comprehension, interaction, and manipulation. 
4.4 This is learned in a wholly *unsupervised way*
4.4.1 This is yet an unsolved problem in machine learning!  
4.4.1.1 But is it partly solved?  Seems like there isn’t a scientific consensus.  High-dimensional vector representations definitely work (see Dall-E 2, PaLM; each of these are transformers and they do seem to compose the physical world (visual or linguistic) pretty well, with perhaps limitations due to their feed-forward structure hence what relationships that they can represent. 
4.4.2 Sure, these self-supervised transformers seem to get at it, albeit indirectly, and they are sample / data / energy inefficient (ish ... they work). 
4.3 Yet transformers have a key to the puzzle: they multiply activities, not just weights by activities.  Hinton pointed this out (as I’m sure other people have earlier).  The remarkably impressive Nvidia StyleGAN 1 & 2 also relies on activity-activity multiplication.  
4.4 Activity-activity or activity-weight-activity multiplication is possible if not easy in a biological neural network; indeed many of the nonlinear dendritic effects (spikes) and morphology (inhibitory synapses on the spine neck) directly support this. 
4.5 I propose that (this is not unique) dendrites act as both activity-weight products (linearly summing the dot product between incoming activities and synaptic weighs) and activity-weight-activity products (taking the morphology-aware product of activity*weight groups, and (possibly nonlinearly) summing that to the soma.  
4.5.1 Probably this offers a continuum: as the connections grow, their nonlinear summation / multiplication increases. 
4.5.2 The seemingly random encoding possibly works as a hashing function, which has been shown to be super effective in eg super-resolution and NERF training. (of course part of this effect is the adroit mapping to hardware.)
4.6  Put these things together: it should be possible to make a feedback control system (E-I balance) that uses only local learning (biologically plausible), *_a continuum between weight-activity and weight-activity-activity multiplication_*. 
4.6.1 Gradients not required!  Can use feedback-alignment trick: the forward and reverse (perhaps higher order) weights can be encouraged to mirror each other.  This effects a degree of weight transport or error propagation; fun things like visual illusions may well be the result of slow bottom-top-bottom feedback loops oscillating.  (This also suggests that feedback readily spans layers)
4.7 And this is what I’m working on now! The race is most definitely on; so many bright people working very hard in this field.

April 1 2022

Doing effectively regression testing. 
It seems -- very importantly -- that we need one 'bias' bit, to allow explaining DC activity, without the limitation of the PID activity controller.
Yeah, that didn't change anything; there were already always-on bits by virtue of the complement system. 
So, there has to be something else keeping the error from going all the way to zero? 
From inspection, it looks like the center pixel is oscillating between positive and negative changes... 
Hm. I guess we don't really need the DC term.  
The middle pixel is on 5/8 of the time, which is kinda a weird cycle. 
it's 1,2,3,4,7
Let me see if i can make the system more expressive .. ? 
Meh, going in circles a little bit here. 
Think I need to rewind & think concretely about the task / what sort of codes need to be extracted. 
It could be that I'm working on a far too difficult task, where everything is perfectly entangled & unblanaced to boot... 
We have the representations sliding around well, check.  But this seems to be an effect of noise, not directionality; the way to get the 4-symbol task to work is to anneal the noise with time, which is not a good sign! 
If you leave the noise variance constant, it's always bopping around (albeit with some positive convergence properties; the solutions / encoding are ''close'' just not perfect. 
:-/
(...)
Maybe need separate noise on the forward and reverse passes? 
..
At least I've improved the plotting. 

April 13 2022

Back from CSHL NAISys -- mostly finished intake of references from all the talks.  Trying desperately to synthesize all the data presented; quite a wide variety of studies to build upon (and plenty that might be pure distractors..). 

Overshadowing all was the presentation by Blaise Aguera y Arcas which described, at a survey level, the very recent advancements made by Google’s PaTH and OpenAI’s Dall-E 2.  Both are foundation models, with huge numbers of parameters, and massive training sets.  Path was trained on 45TB of cleaned data, has 500B parameters, and was trained across 6144 TPUv4s.  Rather than offering key theoretical insights or methodological breakthroughs, the model seems to be succeeding purely due to scale and engineering effort.  A lot of people have put a lot of time into this, the silicon, JAX and XLA frameworks, compute resources, cleaning the data, and making all the details of the model stable and train well (e.g hyperparameters, batch sizes, network widths, parallelization.. deeply inspiring when you think of it all!)  (Aside: seems  daunting, but best to have a youthful attitude here and excess of confidence.)  Furthermore, Blaise suggested that these are yet again only the beginning, and even better models will allow for comprehension / modeling / generation of multi-sensory data, including video and self motion.  

These models are transformative, and some at the conference were quipping “it’s been solved! what do we do now?”, yet many questions remain: 
* Is there a more efficient way of training a model from this data?  PaTH cost something like $15M to train, while the brain appears to have much more in the way of priors hence data efficiency (and power efficiency -- though this is more a hardware
factor).  
** While it is hard to directly challenge backprop, as historically & mathematically there seems to be nothing better than following the gradient, provided that a problem can be formulated as an optimization.  Had a long discussion with Brian Cheung about this at one of the lunches at CSHL; I argued that breaking the global problem into a hierarchical optimization or “data organization is at least more representative of what the brain is doing, and should increase parallelization & data locality (but does not avoid the coordination problem).  With SGD, data needs to flow forward while recording activations, then backwards; the two can be interleaved but it requires some shoehorning.  The brain does not have this form of locking, so far as we know.  
*** There is the example of brain rhythms, but conflicting evidence here: according to someone at the conference (I forget who), some people don’t have an alpha rhythm.  Yet other studies have shown that the phase of microstimulation with respect to the theta rhythm affects memory encoding. 
** Furthermore, the whole idea of using SGD in a self-supervised method -- which is 99% of what foundation models are doing -- also seems like shoehorning.  Certainly there is a more direct way of going about it, and arguably this is what the brain is doing. It doesn’t need gradients in the same way as it’s occupied with storing and generalizing data directly.  (Or rather: this happens as an emergent phenomena of what it does, just like it happens with SGD and MLPs as an emergent phenomena / due to the simplicity bias.  
* Is there a better way of representing statistical dependencies?  Transformers are brilliant in that they natively can encode two-factor conditional probabilities (and hence in many-layered networks, can represent conditional functions of arbitrary numbers of terms), but I think this is a general instance of activity*activity multiplication combined with weight*activity projection.  Both of these individual neurons, or even individual dendrites, are capable of representing both [], though this is just now being explored for deep learning.  
** Biological neurons seem to be able to extract invariances and equivariances, as opposed to ANNs where convolution determines weight sharing and                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               and invariance is approximated through MLP layers or max-pooling.  So far as I know, this remains an open problem, one that is related to but probably not to be confused with disentanglement, factorization, and compositionality.  
* The transformer architecture is feedforward, which makes inference and backprop simpler (recurrent networks are harder to run backprop on), but at the same time this limits their computational expressivity.  Psychophysics has shown that the brain solves some problems in a parallel fashion (e.g. search for outliers in an oriented field of lines), and some problems serially (search for a particular shape or object, usually).  That said, this limitation may be justified as the training demands are already excessive. 
* Data is fed during training in a random order, and the learning agent does not directly interact with it; it cannot perform experiments like a child.  These sorts of natural experiments are very likely to ‘’dramatically’’ simplify problems of disambiguation and disentanglement, but joint sensory-motor learning is, as mentioned, just beginning to be worked on. 

In aggregate, it ‘’should’’ still be possible to improve machine learning models with insights from biology and theory, and these improvements can go to the ‘’bottom’’ of the technology stack. Otherwise, most changes will be at the ‘top’ of the technology stack, eg scaling.  This circulates back to what to change, and how to change it.  

* Backprop.  Many many people have been working for a while on this and - lo! - it is possible.  There a large variety of alternatives now to backprop, like feedback alignment, brainprop[], error correction through emergent phenomena [], eligability traces / use of auxillary variables, RPE-gated plasticity etc, but none seem to approach the simplicity and generality of backprop + SGD.  Not a small factor in this is that the chainrule is built into several highly performant ML packages, and most problems are ordinal and can be formulated as an optimization (they are duals). 
* Network topology.  I actually think that, if you stay with backprop, there are incremental, moderate gains to be had here.  Definitely worth the engineering effort.  For example, Nvidia presented using 8-bit logarithmically-scaled weights to reduce weight storage / bandwidth.  (This was biologically motivated, too.).  
** If you forgo backprop, the sky is the limit!  But lack of constraint means that exploring the space and selecting starting points is hard.  Wolfgang Maass’s Probabalistic Skeleton work is inspiring here: sure it took a supercomputer, but evolutionary strategies was well capable of finding robust solutions to make an ant walk.  (With zero learning as well)
*** I think their cleverness was to make the encoding straightforward: a vector of developmental weights, which create the probabilities of connection, which creates the overall computational structure. 
* Training schedule.  Make it closed loop, as mentioned.  Very open and interesting field here.  Maass’s probabalistic skeletons 

The world is hierarchically structured, the brain is also hierarchicially structured and modular; want to be able to edit the data flow graph accordingly (which will perhaps take some cleverness). 

April 14 2022

PS2
	Fixed parameters
		num neurons
		num neuron types
		num dendritic branches
		Topological dimensions available
		num synaptic learning rules (blends?)
	Variable parameters
		Instance locations (N-d space... ? )
		Instance connectivity (very sparse)
		Instance synaptic location / coupling (also sparsre)
		Instance types

Steps in evaluating a given PS
	Childhood, N steps
		Episode, K steps
			Set the input neurons to image
			Propagate signal around
			Update synaptic weights
	Compression test
		Inner, J
			Set input to image
			Record input, output, true encoding
		Linear regression to see how well the compression works.
			(might need a slightly better disentanglement metric here)
	Prediction test
		Inner, L
			Set input to corrupted image (known image + noise)
			Record error / novelty neurons
		Straight MSE over all examples


Thinking about representing the whole recurrent network as a sparse matrix multiply, which of course simplifies forward propagation by a lot!
Then the development simply entails unpacking the code (which presumably will be simple MLPs over input space) to fill the matrix.
This should set the default synaptic weight / presence thereof.

But then how do we represent weight changes / synaptic plasticity?
At present it's a factor of pre and post activity, exclusively.
In practice it's a function of pre, post, neuromodulator, Ca+2 in dendrite / spine, eligability trace, ???.
	(biology also likes to gate presynapses.  Not here, not yet; can do with postsynapse)

Simulation Flow:
	presynaptic activity is broadcast to all axon terminals
	postsynaptic activity is computed by multiplying by weight
	postsynaptic activity is computed as a function of input and space (e.g. threshold function)
		I'm not sure about this .. segments of dendrites are supposed to implement the AND gate, and the overall tree implements OR, but ..
		(Needed boolean gates:
			AND, OR, NOT, PASSGATE.
			MUL, ADD, INV, IF.
				(FOR, WHILE, *PTR, TEMP, LET, LET REC, .. the standard substrates of computation.  Only we want this to be meta-computation.)
		Passgate = PV spine neck inhibition.
	Postsynaptic net activity is computed
	Synapses are updated based on rule..

Synapse update basically means a kernel which takes in the various factors and spits out a delta, same as what we did earlier in Mojave musings in JAX.  (could be faster..)
This means using kernels, rather than sparse matrices.
Then development is basically populating the 1-D array of synapses, aka the COO sparse matrix format.
Lots of scatter-gather operations.. meh, so long as it can be made embarassingly parallel, its ok!
	Seems like it's conceptually simpler to do gather operations, so will thing this way for now..
Dimension for iterating:
	1. synapse number along dendritic segment
	2. dendritic segment in neuron
	3. neuron in organism
	4. organism in population

So, a 4-d tensor of synapses (which can be reprojected into 1-D, of course..)
Simulation Flow, again:
	+ Update activity
		- Over all synapses, perform GET operation for the presynaptic activity from the vector (matrix, population) of activity.
		- Over all segments, perform nonlinear sum (multiplication). Emit Ca+2
		- Over all neurons, compute output, and write to vector (pop matrix)
	+ Update synapses
		- Over all synapses, perform GET operation for pre, post (dendrite, soma).  Emit weight update. Emit state update (as per jax implentation).

I really have no idea how well this all will work, or if it will work; need to scale gradually, starting from very toylike models. (glider task!)
Suppose it makes sense to do it in jax, for sure.
	Will have to turn off the gradient accumulation.

April 30 2022

Have the JAX code up and working!  Though it does not seem to be evolving complex behavior...
I think one problem is that connection probability is necessarily normalized -- neurons 'have to' be connected to other neurons, instead of defaulting to no connection.
Defaulting to no connection / making the probabilities absolute should fix this, but we'll need a 'null' connection & need to think of how to implement it.

Also, it's not clear why the learning rules & biases are all clustered around zero.  These are effectively under brownian motion, so should not matter at all; they should only be constrained by the limits, yet they do not seem to bounce into them at all .

May 2 2022

OK need to present Springtail! 

The situation has been evolving (pun intended?), and it’s worth going back and reflecting on the path. 

Last year: 
-- Current ML methods adopt a grossly simplified version of what synapses & dendrites are.  There are three areas rife for disruption: 
--- If we were to add behavioral timescale plasticity, it should serve well as ephermenal ‘binding variables’, much like the ‘attentional’ variables in the now-prevalent Transformer architecture.  
---- Notably, this is very much at odds with the current ML approaches, which adopt high-dimensional vector representations.  (Are these really so bad? If we have only a few examples, say one positive, one negative, it’s easy to measure the difference between the two: it’s a vector.  And, if the representations are truly disentangled, then there won’t be much per-class off-axis variance.  That is, the vector will be invariant and true. Hmm.  So there is not really such a problem with vectoral representations, which is much more consistent with modern ML approaches.  Provided, of course, that they are disentangled. 
--- This also solves the (infamous) binding problem: if neurons are feature detectors, and their activity represents the presence (or absence) of an object, how do you represent the linkages between parts to form a whole? 
--- Dendrites are inherently nonlinear, and have plateau potentials & forward / back-propagating spikes.  This can flexibly serve to allow for activity-activity multiplication, which again is an essential component of Transformers.  It should also support some basic logical operations: if the soma (summation node) is ‘or’, then the dendrites can be ‘and’.  (Add in ‘not’ and you can express any boolean logic.)
--- The nervous system is notoriously closed-loop; the balance of excitation and inhibition is just one example of this. In comparison, most machine learning models are sequential, with one feedback loop: that of error propagation through SGD.  Finer granularity in feedback permits much higher level of parallelization, and supports perceptual-infill (e.g. visual illusions; the creation of an approximate scene graph in the first place.  


-------
Random thoughts: 
* Why do this at all?  why not just allow for the current pace of ML to continue?  It’s hyper competitive, which is super bad.  Lots of plenty bright, ambitious people working in the are.  And, as the CSHL NAIsys conference showed, it’s all different bits of optimization and search; one of the reasons that academic projects don’t get so far is that they simply don’t apply enough effort. 

Generally the idea is that biology supports higher levels of parallelzation and sample efficiency.  E.g. it seems like a bit of a shoehorn to use SGD with self-supervised methods.  But then again, this works!  Also, the degree of invariance compression / equivariance extraction in ANNs is still somewhat weakly defined.  For the 

In comparison, the brain basically 

May 3 2022

Well I seem to have stopped on that last part and got back to work on skeleton.py .. ahhah
All tests with population of 3k.

train_jit:
	1.906 sec / iter.
train no jit but jit on sim_step_update and no scan:
	1.16 sec / iter. (faster!!!)
train no jit and no jit on sim_step_update:
	1.16 sec/iter (interesting -- does jax jit everything internally?)
train with jit, no jit on sim_step_update:
	1.88 sec (??!)
train with no jit, no jit on sim_step_update, but scan on:
	1.18 sec or 1.17 sec
		*** this is the winner.
		( slightly slower if sim_step_update is jitted )
(double check) train no jit, no jit on sim_step_update, no scan:
	26.46 sec (!!!)
(double check) train no jit, sim_step_update_jit:
	1.91 sec

eval_gar_jit:
	3.37 sec
eval_gar no jit:
	1.42 sec

POP is 7680
float32:
	train 2.412 make_pop 0.003 eval_gar 4.596
float16:
	train 0.795 make_pop 0.0006 eval_gar 2.574

Ok so ... we need to figure out how to make float16 work!
Much much faster and or can run larger models / larger populations!
	(might well be limited by memory bandwidth here..)

Note note:
Needed to add the following lines to the kernel boot flags to allow the (nearly) full memory of the 3090 to be used:
> sudo nano /etc/default/grub
GRUB_CMDLINE_LINUX="pci=assign-busses,realloc,hpmemsize=128M,hpbussize=0x33,hpmmiosize=128M,hpmmioprefsize=16G"
> sudo update-grub
On the 11Gb 2080Ti, I can get a 3k population size before running out of memory;
On the 3090 w/ above, it's possible to get 6k, with the current neuron type configuration.
(No changes were made to bios).
To get nvidia-settings to display something,
sudo nano /etc/X11/xorg.conf.d/80-igpu-primary-egpu-offload.conf
:
Section "Device"
    Identifier "Device0"
    Driver     "modesetting"
EndSection

Section "Device"
    Identifier "Device1"
    Driver     "nvidia"
    BusID      "PCI:05:0:0"                 # Edit according to lspci, translate from hex to decimal.
    Option     "AllowExternalGpus" "True"    # Required for proprietary NVIDIA driver.
EndSection
:
That all seems to work and is better than looking at nvidia-smi.
Success!
Now need to encode an objective function for the 'output' neurons.
(& this one needs to be rotation invariant)


May 5 2022

Tried out pure simulated annealing overnight, best solution seems to be:
correct 42.475266 err 40.931347 out 26.354053  /  109.76066
Let's compare this to search with recombination:
seems to oscillate a whole bunch
correct 87.72359 err 58.33085 out 31.88125  /  177.93568
take that solution and switch back to simulated annealing
Doesn't seem to be working any better!
Annealing is also oscillating around 170-180 total loss.
correct 87.258545 err 59.643234 out 31.868845  /  178.77063

May 6 2022
After reading a bunch of Kenneth Stanley's papers, I've concluded that we need some form of novelty search -- to push the networks out of local minima.
Propose doing this by recording activity on the evaluation phase, storing it in a large matrix, and sorting this matrix based on self similarity over time..
Problem is that we *probably* need speciation or so ... novelty will encourage diversity, but crossover and recombination will collapse this through in-mixing (??)
Will still need to address the problem of recombination: how to keep effective cabals / coordinated genes around. hmm.

May 9 2022
Can we get rid of the only-positive acitivities limitation?
That's how ANNs work .. but of course it's not how BNNs function.
It would simplify and streamline things considerably, of course: the rules could allocate however many excitatory and inhibitory neurons as needed.

The limitation is seeing if the learning rules can be stable / or how to formulate them so that they are stable.

It likely makes sense to revisit some of the old experiments of hand-crafted 'cortical' networks to see what and how the subtraction is happening.

(This thinking is
