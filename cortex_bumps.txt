May 13 2021

The mode was (not so much is anyore..) decent at nulling the input neurons w inhibition, 
but is absoulte crap at the route-cause: it divergese to flat, all units identical.
It appears that rci (and, by extension, w_rci) degenerates ito flat before rce. 
Wait .. w_rci is not updated! It's ones - diag matrix, scaled. 
Yeah that might not be a good regularization... 
Also, with time rce becomes flat; starts out diverse enough. 
Ok, apparently the problem is that, in w_rc, weights corresponding to the input go to zero, while weights corresponding to output seem to saturate.  
This is definitey not the behavior we want, and is likely due to a combination of the different scales of the input->rc weights and the output->rc weights.  
wrc_update is simple hebbian outer-product of rc activation and concatenated ll & l2.
(aka only local information)
passgate update is similarly hebbian outer-product between in, out, and rc.  

Well,  what do we want?  
'rc' stands for route-cause: they should route (multiply) input to output (causes) conditional on both.  
Thus, it would seem that they need faster dynamics than the object neurons. 
But also one-hot dynamics?  Does this need to be iterated? Or does the w_rci matrix need to update too? Needs to be an asymmetry relative to the out neurons. 

June 9 2021

Testing the very simplified model with four input patterns, and only positive generator synapses, yields a network that (usually) predicts well (sometimes NaNs creep in there) -- but it's not sparse.  
One output neuron always takes care of the dominant mode of the data vector, the DC component i guess; it doesn't make sense to combat this, since the data is not independent! 
Should the network be whitening the data?  
In biology, sensory input is very typically whitened, often at the receptors themselves, due to adaptation. In ANN, this term is taken care of with a bias term (1.0).. 

On the one-hot four-pattern task, 
With dynamic inhibition: error mean 0.0570 / std 0.0146 rep 0.0471 std 0.0079
Without dynamic inhibition: error mean 0.0572 / std 0.0192 rep 0.0621 / 0.0201

So, the dynamic inhibition seems to be doing something 'useful'. 
But ... why is it not driving the error to zero? 
There are only four patterns, this should be possible.

OK so -- supplying the true information to l2 results in zero error, 
but only if the cube learning rule is turned off. 
If the true information is not supplied, and the learning rule is linear, then the error mean 0.0184 / std 0.0095 
This is with symmetric learning rates on the forward and reverse matrices. 

Making the backward learning slower than the forward makes the error greater. 
Making the forward learning slower than the backward makes the error less: 
tensor(0.0100) tensor(0.0096)
Making the inhibition learning faster than the forward makes error worse. 
Making the inhibition learning slower than forward makes error worse, too
tensor(0.0191) tensor(0.0099)
Increasing the backward learning to 0.013 slightly degrades performance
tensor(0.0122) tensor(0.0107)

If we extend the training time to 30k samples, Usually it solves the task exactly. 
tensor(0.0037) tensor(0.0063)
but sometimes it doesn't ...
If we increase the learning rates to 0.008 (forward) 0.008 (inhib) 0.014 (backward), it's again worse -- seems to occasionally fail (?)

This network, which can usually solve the four-pattern one-hot, cannot accurately solve the free pattern: 
tensor(0.0606) tensor(0.0065)
Why is this?? 

Can we solve it with linear algebra? 
Yes, clearly we can solve the linear section ('gen') not the inverse section ('invgen') -- this results in (unsurprsinigly) half the error: 
tensor(0.0352) tensor(0.0098)
@ 10k steps. 
Increasing the number of steps to 20k does not improve it much:
tensor(0.0293) tensor(0.0032)
it just reduces the variance. 
'Cheating' by supplying g2 to l2 pushes the error to zero, as it should. 
tensor(0.0009) tensor(0.0005)
further reducing the learning rate to 0.003 on forward weights @ 10k increases error: 
tensor(0.0532) tensor(0.0090)
same learning rates @ 20k: 
tensor(0.0218) tensor(0.0083)
@ 30k: 
tensor(0.0294) tensor(0.0061)

Ok will stick with 0.005 / 0.005 / 0.01 @ 10k. 

Increasing the input size to 16 (from 8) slightly reduces the error: 
tensor(0.0275) tensor(0.0052)
This makes sense as it's easier to estimate the 4 causes with more predictors. 

It does seem .. for this linear problem .. that disabling inhibition improves things. 
tensor(0.0284) tensor(0.0095)

What about changing the dimensionality of the output layer? 
size 6, 
tensor(0.0279) tensor(0.0076)
size 4 (same as generator)
tensor(0.0340) tensor(0.0091)
(of course then the network is doing much less compression..
zero compression should be perfect: nope!
tensor(0.0244) tensor(0.0055)

I guess now should think about multiple layers & further nonlinearities? 


June 18 2021

Fiddled around with the 2-axis task. 
With noise, we can reliably get solutions -- good!  I was discouraged last night. 
Notably, the other forms of homeostasis (slow homeostasis) don't work; they cause weight instabilities.  
This may be the cause of the problem with the other tasks.  
Still using the cubic hebbian learning rule on this binary task, 
since that worked so well for the XOR task.  
Also still using asymmetric learning rules, as discovered in the vectoral learning task; solutions are sensitive to these learning rates.  Doubling them breaks the network!  (And it does learn slowly already.. though I'bve not explored this extensively..)

This shows clearly how important small, interpretable, comprehensible tasks are.  The 2-axis task, I can solve easily 'by hand'. 
We should continue this development avenue with small numbers of inputs and outputs, while also testing on larger equivalent tasks! 

Note: linear hebbian learning rule does not work on this task. 
Note: inhibition is not required to solve the task, but it does not seem to effect the solution either. 
@ noise level 0.025, it does sometimes fail!  But this seems to be a sweet spot c.f. 0.05 or 0.002. 
Depends on the initialization..

Dec 2021 .. 

next test would be to see if, given both l2 and rcdes, the pg net can output l1i. 
Need to do that above.. 

yes, that seems to work as well, to a very high fidelity 
-- can predict l1 reliably with rc one-hot encoding. 
however however, it does not seem to set l2 to a static representation. 
will need to penalize (?) to get this behavior ? 

Dec 16 2021 
let's try one update step instead of five for improving continuity / smoothness in L2 representation.

Yes, one update step does work -- but it doesn't improve continutiy per se. 

Also what works: clamping the RC activation, but propagating the gradients fully through l2. 
If anything, this seems to work better than blocking the gradients. 
Works: error is ~ 5e-4 after 60k steps w batches of 8. 
Next, need to try propagating the gradients through all, including RC. 

++ Running the network twice, with no gradient-stop, and smoothing the activations of rc and l2 seems to further improve the performance of the network, without significantly slowing execution. 
batch size = 16.
slowloss ~ 2e-4 at 140k iterations. 
GPU load is only 2% when rc size = 14..

++ Trying to increase the size of the rc layer, see how that changes things. 
++ need to add a better form of logging -- eg print out gifs every 20k iterations or so. 
-- Huh, even with this very small network, am using an inordinate ammount of vram - 8gb!  what??!! 
-- Increasing rc size to 32 did not slow computation at all. GPU usage si now 3%, from 2%. 

Dec 22 2021

Tried a few things: 
-- clamping the PG weights to be [-0.001 1.0]
-- adding a ReLU to the matmul(w_, rc)
This revealed something interesting: 
the network has sufficient capacity to represent all stimuli just in the RC layer; 
(green); no l2 activation (black) is required!  
Black / l2, in this case, is all high / on. 
This is consistent with there being only 41 vision sensors, and the spatial frequency of the stimuli being well below f_s / 2 -- so, in practice, there are less than 16 independent degrees of variation in the data. 
Thus, even though this experiment 'failed', it revealed something useful:
we can't expect the network to generalize well if it doesn't need to. 
Loss gradually bottoms out to 6e-5 at 4e6 steps. 

Dec 30 2021

Tried dramatically decreasing the network size -- 4 l2 units, 8 rc units. 
Keeping the weight clamps resulted in L2 staying high all the time (near 1). 
Trying now with weights clamped to [-1 .. 1]
... 
This did not change L2, it's still always high. 
Try removing the ReLU when forming w2 from w_ and rc. 

Dec 31 2021

4 l2, 8 rc seems to be working (unrestricted..)
Let's try making the objects move faster so the contextual window is shorter! 
(Might need to add explicit set / reset..)
Increasing the velocity by 2.5x: 
velocity = (1.75 + randn() * 0.35) * scl * -2.5

Jan 1 2022
That seemed to make things worse!  prediction was more accurate and 'smoother' with the orginal speed.  
What if we increase the iterations? 
	from one to two. 
		obviously this is a good bit slower... 

Other idea: add dynamics to the synapses (or dendrites) themselves. 

Jan 2 2022

I've run the network for a long time now -- 
20440800 loss: 1.763e-03; (20e6 steps!)
And it does seem to be representing the 1D field with something that appears to be a shifter in the RC network, and something that appears to be more constant, in the l2 network. 
This is with two passes through the network in forward(self, inp)
And also spatial bluring on both l2 (2 passes) and rc (3 passes). 

I think, per 1999 Schmidhuber Feature extraction through Lococode, that we need to ammend the network such that the objective function forces sparse or factorial coding. 
Factorial coding is definitely what we want here ...
If we need to implement their rather complicated objective function modification, then so be it. 
First, I think, we need to replicate the 5 x 5 bar experiment, both with SGD and with their fancy modifications. 

Options: 
-- Change the objective function to improve source separation. 
	-- Problem: there is no guarantee that things will be topological; indeed it may be asking too much to assume that the arrived-at representation is topological... 
		-- Schmidhuber result did not find topological mappings
			It also hasn't been cited that much. 
		-- Well the cortex seems to do this just fine ... need to replicate some special sauce there! 
-- Add to the network additional layers that can un-permute the arbitrary mappings found by SGD
	-- Also consistent with above.  

Jan 20 2022
Spent yesterday trying to get MNIST working via the simplified feedback path. 
It works, sorta -- but is prone to instability, possibly chaotic instability. 
Dialing it back & trying a stimuli with a known distribution : just a bunch of gaussian bumps
It's still very unstable. 
But, l1e activations don't seem to decay to zero (at least)
(This problem should be trivially solvable with PCA)

Alright, it's working better -- but I can't be sure about the inhibition though. 
It takes a long time to wind up, and once it is wound up, it seems unstable / diverges. hm. 

Two current problems: 
-- The inhibition is sensitive to tuning / it does not completely abolish correlational structure in the sequence. 
-- Learning rate very much depends on the number of inputs, which determines the strength of the resulting activation. 
	While the first layer seems to have a broad plateau of stability, the second layer does not; it likes to either go to all-on E or all off E (input)
	Meanwhile, there are still modulations to S. 
	
Todo presently: 
	** Test & verify the function of inhibition to approximate the inverse covaraince matrix. 
	Might need multiple steps.. 
	** Make sure learning rate scaling makes sense no matter the input - output size of a neuron
	
Jan 21 2022
OK, the inhibition is decidedly not working the way we expect or want -- it does not reliably converge to the desired number of of underlying factors. 
(However, in combination with the feedforward hebbian rule, it might ...)
Nonetheless, keeping the inhibition in the network generally seems to improve network function. 

The first layer now is broadly stable, it just tends to have low-activity tuning artifacts, due to the nonlinearity in the hebbian rule: infrequent strong LTP compensates for frequent small LTD.  
Might be able to tune a meta-parameter to correct for this. 
	In the brain, I suspect this is additionally handled with space: synapses cluster together on dendrites, which spatially organize (maybe???) to allow for the nonlinear interactions suporting local filter maintenance / removal of non spatially concordant info.

Need to tackle the problem of learning rate depending on the number of inputs. 
	In the brain: most neurons of a given class have about the same number of synapses? 
Can't you just scale the learning rate by the 1/(number of inputs)? 
This would allow equivalent steps to the resulting activity. 

March 11 2022

Been awhile ... eh wells.  
In further pondering of the task, it seems unreasonable or difficult to force invariance transformations, such as translations, to adopt a one-hot encoding.  
In the case of a 32x32 input, 32x32 output, and one-hot 1D translation vector, this requires storage of a full 32MB translation tensor.  Quite insane.  

Instead, we know we have ~ 86e9 neurons, of which 19-23e9 neurons are in the neocortex.  
Allegedly the cortex contains about 1.5e14 synapses (150 trillion) .. I don't know why i seem to recall there being 100 trillion synapses in the brain. 
This equates to 7500 synapses / neuron, which is maybe in the right ballpark. 
Obviously there are a good number in the cerebellum, thalamus, hippocampus, etc.
Meanwhile, the cerebellum allegedly has 70 billion neurons (this doesnt add up?) 

Anyway, if we have a naive shifter architecture, there are 1k input neurons, 1k output neurons, 32 translation neurons, and 32M three-part synapses, which equates to 15k synapses / neuron.  
The problem is that the number of synapses scales as the third power of the input dimensionality, which clearly doesn't work
n synapses per neuron = (n * n) * (n * n) * n / (2*n*n +n) ~= n^3
Yeah, not ok. 

In comparison, using activity rather than space to encode location, you need like one operation to shift things around, and only a handful of synapses. 
	That said, learning in the dumb af expansion above is easy, whereas learning with activity = location is ???

March 15 2022
Some shower thoughts today:
-- Conjunctive features = signals that co-occur, and hence are easy to extract with nonlinear, Hebbian synapses.  This is done.
	-- This is similar to a linear "AND" gate -- signals co-occur commonly. Patches ala Olshausen. 
	-- The reason why we can't do anything else with conjunctive extraction on the second layer is that there are no more easily accessible statistics. (??)
	-- Discriminative learning does have purchase on these problems; it will push apart clusters with different labels & build up discrimination (=classification) in this way.  We don't have this luxury with unsupervised learning.
		Maybe we do have it with self-supervised learning?
		It certainly seems to work with GANs (very well) / VAEs (pretty well)
-- Need an equivalent to the "OR" gate, which can 'destroy' information ala a max-pooling layer. OR = invariance; ignore the patterns of variation within a weight matrix.
	Thinking: in a biological neuron, everything in the set of inputs that create a spike can be considered to be in the invariant set.
		This of course is nonlinear and sorta additive: two inputs that are both in the projection set will add sub-linearly
		-- A normal ReLU artificial neuron has a threshold hyperplane which divides up the set, and planes parallel to this division are invariant. (and planes on the left side are straight ignored!)
	In MNIST, when you train with augmented data (small translations and rotations), you explicitly 'teach' the network that these patterns of variation don't matter.
	-- This could be done in a different way, though: if you move your eyes around, one assumption is that the world has not changed, and hence that the observed patterns of variation you can OR over.
		The other idea is that the patterns of variation can be well explained by oculomotor behavior, head motion, or self motion.
		-- Want generated behavior to "explain away" factors of variation of the sensory input,
			And for motor-generated factors to train up a more general purpose extractor in the case of non-self-generated behavior.
			"Self supervised" in this sense, or at least "scientifically (experiments) supervised"

A priori, how do we know that certain axes are irrelevant?
	If we do know, then can move the discrimination plane in a perceptron to be parallel to these axes.
		There is the added complexity of inhibition & plastic inhibitory synapses; originally I was thinking of using these as purely sparsifying, but that proved to be non-essential.  They can of course effect the invariant planes (dramatically!)
	Without supervisory signals, there is no way to know what is irrelevant!? 
		Other than things that do-not co-occur (manifold learning)
	With supervision (eg. knowing all the digits in a given class), it becomes easier to know dimensions to be irrelevant, and to OR over them.
What if we approach it a different way?
	In a given digit, dark pixels say in the middle of the image can curve to the left or right or up; these depend of course on what digit the image is part of.  From the data, you can infer that completely random instances of dark patches do not co-occur; they tend to have higher level structure.  
	Why does the cortex_mnist.py not sparsely extract these features?  
	It should be possible just with the basic thresholding nonlinearity, right? 
		This would only implicitly start “oring” irrelevant dimensions.  But that should be OK. 
	The problem is that -- when there is an “or” of irrelevant dimensions, **we don’t know which one to recreate.**
		In the trivial sense, we have multiple translations or variations of the same digit. 
		If you know that one patch is active, then you can update your prior that neighboring patches are also active, conditional on these patches, and or conditional on higher-level features, hypothetically with a high degree of sparsity.  
	Then, with BTSP and-or STDP, you can ‘store’ the linkages (causes) of the activations, and thereby store information like location in a scene.  
		I would think that nonlinear interaction of synapses in dendrites to be a critical element of this algorithm. 

March 17 2022
For the AND circuit, i guess log-transform?  but then have to exponentiate..
really want:
	weight * AND ( outer ( input_vector ))
or even:
	weight * AND ( outer ( outer (input_vector), input_vector) )
In the shape task, just a single outer product should suffice:
the number of terms in individual AND expressions is two.
but more broadly, we need something with more expressive power,
something that allows arbitrary number of elements into the expressions.
(i'm thinking this mostly spatially, in terms of dendritic trees..)
but maybe for now we just stick with outer()..

If we want to "reasonably" approximate biophysics:
1 -- Expand the input vector length N by sampling, with replacement, Np times.
2 -- Take a given input vector and project through a random Npx3 matrix into 3d space.
3 -- Do the 1 & 2 for the next layer, size M and Mp.
4 -- These are the locations of the synapses. Run a partnering algorithm over the location to set up the presence - possibility matrix.
	(It's not all-to-all, but might be close in a sparse sense)
5 -- When simuulating, record the location as well as post-synaptic cell identity of each presynaptic potential.
6 -- Convolve the PSPs with a spatial (temporal) filter; if the 'membrane potential' exceeds some threshold (how is this adapted or learned??), then emit a dendritic spike.
7 -- Forward the local dendritic membrane potential to the soma / SIZ for the "OR" operation.

THe aspect which most impresses me is that the biological realizaton is smoothly sloppy: there is a continuum of sets of synapses which may interact nonlinearly, due to spatial location.
To accurately emulate this, would need to do a maybe 2d projection of sparse sampling w replacement of the presynaptic neurons, convolve this with a gaussian filter (discretized?), threshold.
	The convolution of course can be precomputed based on location and represented as a matrix-vector product (i think)

Maybe a way to emulate this sort of phenomena is to use hashing:
Np is hashed into Mp buckets.
nonlinear thresholding operates over the buckets (and learning also operates over the buckets?!)
This doesn't offer a continuum, but it might work well enough for binary-type problems.
For more spatial problems we can do the embedding and pre-bake a convolution matrix.
	You still need continuous variables tp project into..
		The convolution may help with generalization, I think!

Well, anyway, start with outer, maybe move to hashing, then maybe 2d
Need to make sure learning works.
And inhibition / inversion is nonobvious / has not be fully understood quite yet.
	Maybe it's easy when we have negative weights .. ?

March 17 2022

Thinking we might need a learnable bias here -- for the hidden variables (3),
you need their complement (boolean NOT) for conjunctive learning,
When learning starts, the magnitude of the hidden variables are small,
so the magnitude of the naive complements are large,
which skews the learning.
What if we just invert the signals and allow for negative weights?
No, negative weights did not work well in the past..
from this experience, staying with positive reals is better. (and biologically realistic..)
The other option is to modulate the inverse based on 2x the moving average.
So if the average is @ target, 0.5, then the inverse is the boolean NOT
If the average is lower, say 0.1, then the inverse is 0.2
	Maybe this is where plastic inhibition comes in
		It means that generally inverse variables are only local
		(GABA neurons usually don't have long axons)
		which is consistent with biology at least..

Ok ... that maybe works.  But the forward weights seem to be getting stuck!
Let's check.
Yes, definitely getting stuck. This is a primary problem.
Also a problem: hebbian learning rule from cortex_mist strongly favors sparse representations.
	The backwards network works perfectly.
	The weight matrix is not sparse, though.. it ideally should be only a few terms, but is about 20% full.
	Maybe this is ok?
		Changing the scaling on the cube nonlinearity does not seem to change much ...
		it learns the same statistics.
	Yet, when i write it out by hand, I don't need all these terms.
		It doesn't help that I haven't eliminated the 2x redundancy in the second outer() application.
Observation:
	Clearly, the bins are unnecessary.
	If a AND-product is on, then it can instantly sum, and we don't need ddendritic bins.
Other thought:
	In biology the weights are before the AND; here we have the weights after.
	Either one might be workable.
	Right now after seems to be sufficient: just make it work!
---
Wonder if even the most trivial gating (e.g. which dendrite has residual calcium -> enable LTP/LTD) might be sufficient for forward and reverse credit-assignment?

March 21 2022
Spent most of the day looking over Baldwin effect 1992 and Symmetry / Simplicity in evolved structures,
Then about an hour working on cortex_andor.py
Enabled supervision, and in this case - when the network is taught to encode the 8 different images in 3 bits, it seems to work just fine.
This experiment did uncover the fact that it's sensitive to the learning rate.
So, need to make sure that supervised learning works before proceeding to unsupervised learning.
Now running unsupervised, and the second layer has a strong tendency to get stuck in encoding schemes (= forward weight matrices) that poorly represent the data / poorly encode the space.
Clearly, need to think about this more, and possibly consider how 'errors' and 'reinforcement' is propagated around this (rather simple) two-layer network.
Will rest on it!
	Aside: with the supervised task, as time goes on the backward weights tend to sparsify = reverse projection is simpler hence more likely to be getting at the true structure of the data (?!)

March 22 2022.
So many twos!
Ok, need to work on the 'stuck' effect.
Supervised: yes it works, very quickly.
Unsupervised: seems to depend on the noise level; lower noise slows learning and gives more time for better codes?

OK, so reducing the problem to just two images (both in the upper left hand side) does not cause convergence.
it *should* just be a one-bit task, super duper easy.
	(the backward pass is still good, obviously)
Right .. it does seem to work just fine, with alternating presentations.

Ugh, the system is very sensitive to parameters.
For example, it doesn't work well if the allowable negative weights are clamped to
	0.25
	or
	0.0025
	but it works if it's clamped to
	0.025
	why??
		This is not necessarily a good sign;
		Might need to think about symmetries
			perhaps some levels of clamp allow escape w noise?

Reparameterized the network, to allow for 4 bits of hidden state ..
Seems to solve the 4-symbol task (sometimes .. hmm)
What about 8-symbol?
Still seems to get stuck!

What about adding push-pull capability?
E.g. the forward regresses toward the 'n' variable (with negation)
If there is strong residual error, you can push the weights in the opposite direction from the positive hebbian term:
just reverse the error term, and complement the hidden state.
-- This doesn't provide much more information, alas.
The problem is that if a unit is 'off', you can't turn it more off. Hah!

Definitely the problem is gettign stuck in local minima -- when iterating through data presentations, the + dw exactly counterbalances the - dw.
At least, with identical ordering of the stimulus.

Another problem: the system is not long-term stable, it seems to degenerate to a two-state represenation.  Hm. One thing at a time, now.

March 23 2022

Ok, trying to figure out this metastable / stuck problem.
<didn’t make it far, apparently.. 
think I need to just implement biology hah>

March 27 2022

Woke up this morning with a pretty clear idea of what needs to happen at the company / what the pitch needs to be: 

1. How do you ‘solve’ arbitrarily hard problems, perhaps problems with difficult scaling properties, e.g. large or unbounded action / perception dimensionality?  
1.1 RL works to some degree, and has ‘solved’ tasks like Atari (Agent 57, etc).  This is a very significant advancement.  But the action space is small (a few buttons), and the perception space is constrained (8 colors, limited resolution, 2D sprite world, simple game mechanics, etc). 
1.2 Humans obviously can solve these problems, and most importantly they become very effective at solving them. 
1.3 How? 
2. One avenue is that humans factorize distributions in both perceptual and motor domains.  Perception is perhaps most obvious: we factorize objects into attributes; the flip side of this process is the ‘binding’ problem, whereby attributes are bound into hierarchies of objects.  These attributes include important details like pose, lighting, etc.  (Details that Hinton’s Capsule nets / GLOM model try to get at..)
2.1 A deeper philosophical question is *why* the world tends to be readily factorizable into low-dimension components, almost universally (but see: our brain; and also see, gene regulatory networks)
2.2 Motor is sorta the opposite direction: the brain factorizes patterns of output over the space of muscles and their synergies.  
2.3 Of course there are a lot of additional details to both problems .. 
2.4 ... but an element that can readily solve both action and perception problems is a memory that can be indexed either by “output” or “input” (e.g. auto-associator, old idea here), and, most importantly -- to counter the curse of dimensionality and the aforementioned factorizable nature of the physical world -- the “output” addressing is factorized / disentangled. 
2.4.1 This in turn allows low-power learning algorithms (like RL) to have enough leverage to do things like fast-mapping (in dogs) and interaction with the physical world... and maybe even language / grammar?  
3. Very well, so we need a self-factorizing auto-associator.  I propose this is what the cortex is doing in mammals. 
3.1 And probably plenty of other things... but still. 
3.2 What is the cortex, then?  Laminar structure, with some columnar structure, notable input (layer 4) association (1, 2-3) output (5 & 6); each layer has ~80% excitatory neurons and 20% inhibitory neurons.  The inhibitory neurons have a great number of specializations, both morphologically, electrochemically, and gene-expression wise.  Three major classes: PV, SST, VIP.  Exitatory neurons in contrast are of fewer classes and firing properties (at least, to my limited knowledge).  The excitatory neurons project, the inhibitory neurons generally do not, though there are exceptions here. 
3.3  Strong motifs in the cortex: 
3.3.1  E-I balance.  Most neurons, of both polarities, are in a constant state of push-pull, where usually excitatory and inhibitory input balances
3.3.2  Hebbian or local-type plasticity: synaptic ‘weight’ or coupling changes happen based on almost-entirely local changes, with modulation from retrograde signals, neuromodulators, iternal signaling pathways, etc. 
3.3.2.1  Some of these pre & post changes are very quick, so called behavioral timescale plasticity.  
3.3.3 Dales law: neurons generally emit one primary neurotransmitter; glutamate or GABA.  This of course has exceptions & many release multiple neurotransmitters -- but is a useful starting heuristic. 
3.3.4 Highly nonlinear membranes, inclusive the dendrites.  Some (many?) dendrites have dendritic spikes / plateau potentials, which can change the way signals are transmitted to the soma / spike initiation zone.  
3.3.5 1k - 10k synapses per neuron, roughly.  Most synapses have about the same ‘weight’, and the distribution of these weights is not agreed upon but does not have the resolution of 32-bit or even 16-bit floating point numbers.  (Nvidia suggests 8-bit logarithmic..)
3.3.5.1 Yet synapses certainly have more state! 
3.3.6 (Most obvious) The same rough cytoarchitecture throughout, with specializations say in auditory cortex (faster firing rates / high resolution timing information?) or in striate cortex (axons, e.g. -- probably to represent long-distance spatial relationships?)
3.3.7 Speaking of timing, there is a preponderance of spike-timing dependent plasticity.  Though the degree to which this exists, and how it works in the overall function of the cortex, is not agreed on.  STDP likely depends on much more than just pre-post timing; pre-post-pre and post-pre-post triplets have their own changes. 
3.3.8 Prediction *seems* to be a common motif, particularly for sensory encoding. This has obvious utility for animals interacting with the world (or simulating it, in the case of cognition)
3.3.9 Most feedback is local and low-order (in the poles and zeros sense).  Homeostatic effects are ubiquitous in biology; it’s just a good way to build a system. 
(.....)
4. OK, how to synthesize all these disparate details?  
4.1 It’s a feedback control system that tries to compress and predict (aka represent) the “input data” (say vision). 
4.2 Compression is via factorization, per above, to fight the curse of dimensionality. 
4.3 Factorization allows for comprehension, interaction, and manipulation. 
4.4 This is learned in a wholly *unsupervised way*
4.4.1 This is yet an unsolved problem in machine learning!  
4.4.1.1 But is it partly solved?  Seems like there isn’t a scientific consensus.  High-dimensional vector representations definitely work (see Dall-E 2, PaLM; each of these are transformers and they do seem to compose the physical world (visual or linguistic) pretty well, with perhaps limitations due to their feed-forward structure hence what relationships that they can represent. 
4.4.2 Sure, these self-supervised transformers seem to get at it, albeit indirectly, and they are sample / data / energy inefficient (ish ... they work). 
4.3 Yet transformers have a key to the puzzle: they multiply activities, not just weights by activities.  Hinton pointed this out (as I’m sure other people have earlier).  The remarkably impressive Nvidia StyleGAN 1 & 2 also relies on activity-activity multiplication.  
4.4 Activity-activity or activity-weight-activity multiplication is possible if not easy in a biological neural network; indeed many of the nonlinear dendritic effects (spikes) and morphology (inhibitory synapses on the spine neck) directly support this. 
4.5 I propose that (this is not unique) dendrites act as both activity-weight products (linearly summing the dot product between incoming activities and synaptic weighs) and activity-weight-activity products (taking the morphology-aware product of activity*weight groups, and (possibly nonlinearly) summing that to the soma.  
4.5.1 Probably this offers a continuum: as the connections grow, their nonlinear summation / multiplication increases. 
4.5.2 The seemingly random encoding possibly works as a hashing function, which has been shown to be super effective in eg super-resolution and NERF training. (of course part of this effect is the adroit mapping to hardware.)
4.6  Put these things together: it should be possible to make a feedback control system (E-I balance) that uses only local learning (biologically plausible), *_a continuum between weight-activity and weight-activity-activity multiplication_*. 
4.6.1 Gradients not required!  Can use feedback-alignment trick: the forward and reverse (perhaps higher order) weights can be encouraged to mirror each other.  This effects a degree of weight transport or error propagation; fun things like visual illusions may well be the result of slow bottom-top-bottom feedback loops oscillating.  (This also suggests that feedback readily spans layers)
4.7 And this is what I’m working on now! The race is most definitely on; so many bright people working very hard in this field.

April 1 2022

Doing effectively regression testing. 
It seems -- very importantly -- that we need one 'bias' bit, to allow explaining DC activity, without the limitation of the PID activity controller.
Yeah, that didn't change anything; there were already always-on bits by virtue of the complement system. 
So, there has to be something else keeping the error from going all the way to zero? 
From inspection, it looks like the center pixel is oscillating between positive and negative changes... 
Hm. I guess we don't really need the DC term.  
The middle pixel is on 5/8 of the time, which is kinda a weird cycle. 
it's 1,2,3,4,7
Let me see if i can make the system more expressive .. ? 
Meh, going in circles a little bit here. 
Think I need to rewind & think concretely about the task / what sort of codes need to be extracted. 
It could be that I'm working on a far too difficult task, where everything is perfectly entangled & unblanaced to boot... 
We have the representations sliding around well, check.  But this seems to be an effect of noise, not directionality; the way to get the 4-symbol task to work is to anneal the noise with time, which is not a good sign! 
If you leave the noise variance constant, it's always bopping around (albeit with some positive convergence properties; the solutions / encoding are ''close'' just not perfect. 
:-/
(...)
Maybe need separate noise on the forward and reverse passes? 
..
At least I've improved the plotting. 

April 3 2022

Hey Luke, 

hope things are going well ...  How are the projects / how is google brain?  Any brilliant new ideas?  hah... I’m at the CSHL NAISys conference this week, will let you know what I learn.  

As mentioned, been attempting to get a tech demo together.  Would like that to happen before the VC liquidity starts to ... dry out?  Solidify?  Inflate away?  Hoping to make a deck in the next ~ 2 months.  The position at Loyal is temporary, anyway.   Where do you want to be in this -- want to be in from the beginning, or later assuming that money is raised?  What might it take to have you leave the near infinite resources of goog?  Open to suggestions ... we probably should do a zoom to figure these things out.  Wanted to (re-) broach the subject though. 

This past week spoke with a friend, Michael Mager, who is the CEO of Precision Neuro.  I’m a consultant and on the SAB for them, and Michael has been helpful in thinking about Springtail.  Very obviously yet very importantly he keeps emphasizing that we need to find a business use case.  The market can be small, preferably is under-served, and is without an excess of incumbents.  I like your idea of an automatic sysadmin bot, no idea if there are companies in that space... or really how hard it might be to make it.  Likewise for “NeRF the world” idea (though preferably it would be in a nanite representation.. ;-) Are there good underserved markets for inverse graphics?  My naive default is “we’re going to make models so powerful that it will be useful for a great many things” might not make money fly out of VC’s pockets, alas.  Good for academia, though. 

Also been wondering: we’re both researchers.  (among many other things).  How do you scale research?  Distribute it organize it, encourage it if not manage it?  It’s a very slippery subject that leaves me wondering if i should be joining Goog brain instead!  

Now, more technically: I’ve been trying to demonstrate a disentangling, factorizing network in a very small toy example (1 layer, 8 different overlapping symbols).  It is unsurprisingly hard to do with just feedback signals and local plasticity.  I think the problem is equivalent to finding an address space (as in computer addressing) over the input space, where here address bits are taken from nonlinear (quadratic and cubic) functions at synapses, e.g. nonlinear dendrites, consistent with biology.  Presently it works with annealing (decreasing the noise temperature), but that just suggests to me that the solution doesn’t have reliable convergence properties.  There is probably something going on with synapses at inhibitory interneurons, as well as credit-assignment via dendrtic spikes; or it’s likely that sensory-motor interaction is necessary to determine the hidden factors of variation.  The only thing I can think of is to keep grinding and experimenting.  

To reiterate a bit, I know we’ve talked about this -- the long-term vision is very much a mirroring and automation of exactly what we do above when we bootstrap and experiment: try a bunch of things out, learn both what works and doesn’t (as in RL) but more importantly learn input-output mappings and factors of invariance / equivariance / noise variance.  Use that to compress, explain, and make the world memory-invertible; then do means-ends reasoning, A* search, reprojection of goals etc to make new progress.  Of course, our evaluator is meta-circular, so the process is applied to the process itself to allow exit from repetitive search patterns.  (Ahem solve this and you solve consciousness; suspect that it requires a deliberate removal of the memory address-memory content dichotomy.  But these are muddy thoughts i had more clearly on a hike a month and a half ago...)

As usual, the more structure I add, the slower everything runs!  Really need to switch over to JAX, maybe I’ll do that this week based on the meta-learning experiments we’ve done earlier.  Would love to do some sort of code sprint on this.  Also more than happy to help out with your ideas; maybe the rewrite in jax can be good practice for reciprocation.  

Maybe catch up next week after the conference?  

Cheers, 
Tim

PS. LiAn designed a logo!  see springtail.ai  and novaly.ai (hedging bets here in case people have an aversion to springtails.  Novaly =~ Noe Valley :-))


April 13 2022

Back from CSHL NAISys -- mostly finished intake of references from all the talks.  Trying desperately to synthesize all the data presented; quite a wide variety of studies to build upon (and plenty that might be pure distractors..). 

Overshadowing all was the presentation by Blaise Aguera y Arcas which described, at a survey level, the very recent advancements made by Google’s PaTH and OpenAI’s Dall-E 2.  Both are foundation models, with huge numbers of parameters, and massive training sets.  Path was trained on 45TB of cleaned data, has 500B parameters, and was trained across 6144 TPUv4s.  Rather than offering key theoretical insights or methodological breakthroughs, the model seems to be succeeding purely due to scale and engineering effort.  A lot of people have put a lot of time into this, the silicon, JAX and XLA frameworks, compute resources, cleaning the data, and making all the details of the model stable and train well (e.g hyperparameters, batch sizes, network widths, parallelization.. deeply inspiring when you think of it all!)  (Aside: seems  daunting, but best to have a youthful attitude here and excess of confidence.)  Furthermore, Blaise suggested that these are yet again only the beginning, and even better models will allow for comprehension / modeling / generation of multi-sensory data, including video and self motion.  

These models are transformative, and some at the conference were quipping “it’s been solved! what do we do now?”, yet many questions remain: 
* Is there a more efficient way of training a model from this data?  PaTH cost something like $15M to train, while the brain appears to have much more in the way of priors hence data efficiency (and power efficiency -- though this is more a hardware
factor).  
** While it is hard to directly challenge backprop, as historically & mathematically there seems to be nothing better than following the gradient, provided that a problem can be formulated as an optimization.  Had a long discussion with Brian Cheung about this at one of the lunches at CSHL; I argued that breaking the global problem into a hierarchical optimization or “data organization is at least more representative of what the brain is doing, and should increase parallelization & data locality (but does not avoid the coordination problem).  With SGD, data needs to flow forward while recording activations, then backwards; the two can be interleaved but it requires some shoehorning.  The brain does not have this form of locking, so far as we know.  
*** There is the example of brain rhythms, but conflicting evidence here: according to someone at the conference (I forget who), some people don’t have an alpha rhythm.  Yet other studies have shown that the phase of microstimulation with respect to the theta rhythm affects memory encoding. 
** Furthermore, the whole idea of using SGD in a self-supervised method -- which is 99% of what foundation models are doing -- also seems like shoehorning.  Certainly there is a more direct way of going about it, and arguably this is what the brain is doing. It doesn’t need gradients in the same way as it’s occupied with storing and generalizing data directly.  (Or rather: this happens as an emergent phenomena of what it does, just like it happens with SGD and MLPs as an emergent phenomena / due to the simplicity bias.  
* Is there a better way of representing statistical dependencies?  Transformers are brilliant in that they natively can encode two-factor conditional probabilities (and hence in many-layered networks, can represent conditional functions of arbitrary numbers of terms), but I think this is a general instance of activity*activity multiplication combined with weight*activity projection.  Both of these individual neurons, or even individual dendrites, are capable of representing both [], though this is just now being explored for deep learning.  
** Biological neurons seem to be able to extract invariances and equivariances, as opposed to ANNs where convolution determines weight sharing and                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               and invariance is approximated through MLP layers or max-pooling.  So far as I know, this remains an open problem, one that is related to but probably not to be confused with disentanglement, factorization, and compositionality.  
* The transformer architecture is feedforward, which makes inference and backprop simpler (recurrent networks are harder to run backprop on), but at the same time this limits their computational expressivity.  Psychophysics has shown that the brain solves some problems in a parallel fashion (e.g. search for outliers in an oriented field of lines), and some problems serially (search for a particular shape or object, usually).  That said, this limitation may be justified as the training demands are already excessive. 
* Data is fed during training in a random order, and the learning agent does not directly interact with it; it cannot perform experiments like a child.  These sorts of natural experiments are very likely to ‘’dramatically’’ simplify problems of disambiguation and disentanglement, but joint sensory-motor learning is, as mentioned, just beginning to be worked on. 

In aggregate, it ‘’should’’ still be possible to improve machine learning models with insights from biology and theory, and these improvements can go to the ‘’bottom’’ of the technology stack. Otherwise, most changes will be at the ‘top’ of the technology stack, eg scaling.  This circulates back to what to change, and how to change it.  

* Backprop.  Many many people have been working for a while on this and - lo! - it is possible.  There a large variety of alternatives now to backprop, like feedback alignment, brainprop[], error correction through emergent phenomena [], eligability traces / use of auxillary variables, RPE-gated plasticity etc, but none seem to approach the simplicity and generality of backprop + SGD.  Not a small factor in this is that the chainrule is built into several highly performant ML packages, and most problems are ordinal and can be formulated as an optimization (they are duals). 
* Network topology.  I actually think that, if you stay with backprop, there are incremental, moderate gains to be had here.  Definitely worth the engineering effort.  For example, Nvidia presented using 8-bit logarithmically-scaled weights to reduce weight storage / bandwidth.  (This was biologically motivated, too.).  
** If you forgo backprop, the sky is the limit!  But lack of constraint means that exploring the space and selecting starting points is hard.  Wolfgang Maass’s Probabalistic Skeleton work is inspiring here: sure it took a supercomputer, but evolutionary strategies was well capable of finding robust solutions to make an ant walk.  (With zero learning as well)
*** I think their cleverness was to make the encoding straightforward: a vector of developmental weights, which create the probabilities of connection, which creates the overall computational structure. 
* Training schedule.  Make it closed loop, as mentioned.  Very open and interesting field here.  Maass’s probabalistic skeletons 

The world is hierarchically structured, the brain is also hierarchicially structured and modular; want to be able to edit the data flow graph accordingly (which will perhaps take some cleverness). 

April 14 2022

PS2
	Fixed parameters
		num neurons
		num neuron types
		num dendritic branches
		Topological dimensions available
		num synaptic learning rules (blends?)
	Variable parameters
		Instance locations (N-d space... ? )
		Instance connectivity (very sparse)
		Instance synaptic location / coupling (also sparsre)
		Instance types

Steps in evaluating a given PS
	Childhood, N steps
		Episode, K steps
			Set the input neurons to image
			Propagate signal around
			Update synaptic weights
	Compression test
		Inner, J
			Set input to image
			Record input, output, true encoding
		Linear regression to see how well the compression works.
			(might need a slightly better disentanglement metric here)
	Prediction test
		Inner, L
			Set input to corrupted image (known image + noise)
			Record error / novelty neurons
		Straight MSE over all examples


Thinking about representing the whole recurrent network as a sparse matrix multiply, which of course simplifies forward propagation by a lot!
Then the development simply entails unpacking the code (which presumably will be simple MLPs over input space) to fill the matrix.
This should set the default synaptic weight / presence thereof.

But then how do we represent weight changes / synaptic plasticity?
At present it's a factor of pre and post activity, exclusively.
In practice it's a function of pre, post, neuromodulator, Ca+2 in dendrite / spine, eligability trace, ???.
	(biology also likes to gate presynapses.  Not here, not yet; can do with postsynapse)

Simulation Flow:
	presynaptic activity is broadcast to all axon terminals
	postsynaptic activity is computed by multiplying by weight
	postsynaptic activity is computed as a function of input and space (e.g. threshold function)
		I'm not sure about this .. segments of dendrites are supposed to implement the AND gate, and the overall tree implements OR, but ..
		(Needed boolean gates:
			AND, OR, NOT, PASSGATE.
			MUL, ADD, INV, IF.
				(FOR, WHILE, *PTR, TEMP, LET, LET REC, .. the standard substrates of computation.  Only we want this to be meta-computation.)
		Passgate = PV spine neck inhibition.
	Postsynaptic net activity is computed
	Synapses are updated based on rule..

Synapse update basically means a kernel which takes in the various factors and spits out a delta, same as what we did earlier in Mojave musings in JAX.  (could be faster..)
This means using kernels, rather than sparse matrices.
Then development is basically populating the 1-D array of synapses, aka the COO sparse matrix format.
Lots of scatter-gather operations.. meh, so long as it can be made embarassingly parallel, its ok!
	Seems like it's conceptually simpler to do gather operations, so will thing this way for now..
Dimension for iterating:
	1. synapse number along dendritic segment
	2. dendritic segment in neuron
	3. neuron in organism
	4. organism in population

So, a 4-d tensor of synapses (which can be reprojected into 1-D, of course..)
Simulation Flow, again:
	+ Update activity
		- Over all synapses, perform GET operation for the presynaptic activity from the vector (matrix, population) of activity.
		- Over all segments, perform nonlinear sum (multiplication). Emit Ca+2
		- Over all neurons, compute output, and write to vector (pop matrix)
	+ Update synapses
		- Over all synapses, perform GET operation for pre, post (dendrite, soma).  Emit weight update. Emit state update (as per jax implentation).

I really have no idea how well this all will work, or if it will work; need to scale gradually, starting from very toylike models. (glider task!)
Suppose it makes sense to do it in jax, for sure.
	Will have to turn off the gradient accumulation.

April 30 2022

Have the JAX code up and working!  Though it does not seem to be evolving complex behavior...
I think one problem is that connection probability is necessarily normalized -- neurons 'have to' be connected to other neurons, instead of defaulting to no connection.
Defaulting to no connection / making the probabilities absolute should fix this, but we'll need a 'null' connection & need to think of how to implement it.

Also, it's not clear why the learning rules & biases are all clustered around zero.  These are effectively under brownian motion, so should not matter at all; they should only be constrained by the limits, yet they do not seem to bounce into them at all .

May 2 2022

OK need to present Springtail! 

The situation has been evolving (pun intended?), and it’s worth going back and reflecting on the path. 

Last year: 
-- Current ML methods adopt a grossly simplified version of what synapses & dendrites are.  There are three areas rife for disruption: 
--- If we were to add behavioral timescale plasticity, it should serve well as ephermenal ‘binding variables’, much like the ‘attentional’ variables in the now-prevalent Transformer architecture.  
---- Notably, this is very much at odds with the current ML approaches, which adopt high-dimensional vector representations.  (Are these really so bad? If we have only a few examples, say one positive, one negative, it’s easy to measure the difference between the two: it’s a vector.  And, if the representations are truly disentangled, then there won’t be much per-class off-axis variance.  That is, the vector will be invariant and true. Hmm.  So there is not really such a problem with vectoral representations, which is much more consistent with modern ML approaches.  Provided, of course, that they are disentangled. 
--- This also solves the (infamous) binding problem: if neurons are feature detectors, and their activity represents the presence (or absence) of an object, how do you represent the linkages between parts to form a whole? 
--- Dendrites are inherently nonlinear, and have plateau potentials & forward / back-propagating spikes.  This can flexibly serve to allow for activity-activity multiplication, which again is an essential component of Transformers.  It should also support some basic logical operations: if the soma (summation node) is ‘or’, then the dendrites can be ‘and’.  (Add in ‘not’ and you can express any boolean logic.)
--- The nervous system is notoriously closed-loop; the balance of excitation and inhibition is just one example of this. In comparison, most machine learning models are sequential, with one feedback loop: that of error propagation through SGD.  Finer granularity in feedback permits much higher level of parallelization, and supports perceptual-infill (e.g. visual illusions; the creation of an approximate scene graph in the first place.  


-------
Random thoughts: 
* Why do this at all?  why not just allow for the current pace of ML to continue?  It’s hyper competitive, which is super bad.  Lots of plenty bright, ambitious people working in the are.  And, as the CSHL NAIsys conference showed, it’s all different bits of optimization and search; one of the reasons that academic projects don’t get so far is that they simply don’t apply enough effort. 

Generally the idea is that biology supports higher levels of parallelzation and sample efficiency.  E.g. it seems like a bit of a shoehorn to use SGD with self-supervised methods.  But then again, this works!  Also, the degree of invariance compression / equivariance extraction in ANNs is still somewhat weakly defined.  For the 

In comparison, the brain basically 

May 3 2022

Well I seem to have stopped on that last part and got back to work on skeleton.py .. ahhah
All tests with population of 3k.

train_jit:
	1.906 sec / iter.
train no jit but jit on sim_step_update and no scan:
	1.16 sec / iter. (faster!!!)
train no jit and no jit on sim_step_update:
	1.16 sec/iter (interesting -- does jax jit everything internally?)
train with jit, no jit on sim_step_update:
	1.88 sec (??!)
train with no jit, no jit on sim_step_update, but scan on:
	1.18 sec or 1.17 sec
		*** this is the winner.
		( slightly slower if sim_step_update is jitted )
(double check) train no jit, no jit on sim_step_update, no scan:
	26.46 sec (!!!)
(double check) train no jit, sim_step_update_jit:
	1.91 sec

eval_gar_jit:
	3.37 sec
eval_gar no jit:
	1.42 sec

POP is 7680
float32:
	train 2.412 make_pop 0.003 eval_gar 4.596
float16:
	train 0.795 make_pop 0.0006 eval_gar 2.574

Ok so ... we need to figure out how to make float16 work!
Much much faster and or can run larger models / larger populations!
	(might well be limited by memory bandwidth here..)

Note note:
Needed to add the following lines to the kernel boot flags to allow the (nearly) full memory of the 3090 to be used:
> sudo nano /etc/default/grub
GRUB_CMDLINE_LINUX="pci=assign-busses,realloc,hpmemsize=128M,hpbussize=0x33,hpmmiosize=128M,hpmmioprefsize=16G"
> sudo update-grub
On the 11Gb 2080Ti, I can get a 3k population size before running out of memory;
On the 3090 w/ above, it's possible to get 6k, with the current neuron type configuration.
(No changes were made to bios).
To get nvidia-settings to display something,
sudo nano /etc/X11/xorg.conf.d/80-igpu-primary-egpu-offload.conf
:
Section "Device"
    Identifier "Device0"
    Driver     "modesetting"
EndSection

Section "Device"
    Identifier "Device1"
    Driver     "nvidia"
    BusID      "PCI:05:0:0"                 # Edit according to lspci, translate from hex to decimal.
    Option     "AllowExternalGpus" "True"    # Required for proprietary NVIDIA driver.
EndSection
:
That all seems to work and is better than looking at nvidia-smi.
Success!
Now need to encode an objective function for the 'output' neurons.
(& this one needs to be rotation invariant)


May 5 2022

Tried out pure simulated annealing overnight, best solution seems to be:
correct 42.475266 err 40.931347 out 26.354053  /  109.76066
Let's compare this to search with recombination:
seems to oscillate a whole bunch
correct 87.72359 err 58.33085 out 31.88125  /  177.93568
take that solution and switch back to simulated annealing
Doesn't seem to be working any better!
Annealing is also oscillating around 170-180 total loss.
correct 87.258545 err 59.643234 out 31.868845  /  178.77063

May 6 2022
After reading a bunch of Kenneth Stanley's papers, I've concluded that we need some form of novelty search -- to push the networks out of local minima.
Propose doing this by recording activity on the evaluation phase, storing it in a large matrix, and sorting this matrix based on self similarity over time..
Problem is that we *probably* need speciation or so ... novelty will encourage diversity, but crossover and recombination will collapse this through in-mixing (??)
Will still need to address the problem of recombination: how to keep effective cabals / coordinated genes around. hmm.

May 9 2022
Can we get rid of the only-positive acitivities limitation?
That's how ANNs work .. but of course it's not how BNNs function.
It would simplify and streamline things considerably, of course: the rules could allocate however many excitatory and inhibitory neurons as needed.

The limitation is seeing if the learning rules can be stable / or how to formulate them so that they are stable.

It likely makes sense to revisit some of the old experiments of hand-crafted 'cortical' networks to see what and how the subtraction is happening.

(This thinking is.. )

May 11 2022
cortex_andor3.py seems to be (finally) working ok.  Critical advancement was to add both nonlinear term expansion as well as compression, using nonlinear least squares.
It solves the checker-glider problem when the output /address space is 4 bits, but sometimes gets stuck when the output space is only 3 bits (which is the minumum required to describe the stimuli..).
Let me try just running the thing for longer, maybe it will get unstuck due to symmetry-breaking & nonlinear hebbian magic? hhm.

How do these networks work anyway?
Input l1e is expanded to l1o
l1o is projected to l2x via w_f
l2x is converted to l2c using nonlinear least squares
l2c is expanded to l2r
l2r is projected to l1x via w_b
l1c is converted to l1c via nonlinear least squares

Update:
error is l1o - l1x
	aka actual expansion minus estimated expansion.
Considering w_b,
	change w_b based on the nonlinear outer product of
	l2r and error
		where l2r is the nonlinear expansion of l2c
Considering w_f,
	change w_f based on the nonlinear outer product of
	error and l2r

IF error is positive & output is positive, increase corresponding weights in both w_f and w_b
likewise, if error is negative, decrease both corresponding weights.
For the reverse weights, this is obvious
for the forward weights, less so: whatever led to the current output configuration, decrease that!
This has the obvious effect of changing the output representation to be less similar to whatever it was ...
which pushes it to some other representation (via weights)

Yeah, the system works reasonably well with 4 output bits.
Still some residual errors, but that's part of nonlinear hebb.
With three, it always can't represent one of the input patterns (suggesting that if there were 7, it would represent all?
answer:  No, still seems to get stuck on one symbol.
Probably need to boost / maybe PID to quash those errors.

Also, fwiw, seems to use a vectoral addressing scheme for the symbols (as opposed to a digital address.. hm. )


May 17 2022

Much improved understanding from Anthropic's investigations on Transformers (they work via routing information -- typically keys -- forward in time, making the softmax operation a soft 'search' or 'match' operation.  All this arises through very opportunistic use of local information, which is perhaps why SGD is such an effective learning algorithm.)
+ the FAIR paper on solving recusive sequence tasks using neurosymbolic computing, again with transformers.  Wow, it performs better than Mathematica! And probably could be better.  (see the m8ta blog entry for more.. probably could be expanded to programming, with effort.

Anyway, this proves it all seems to come down to
(1) Mechanism.  You need to provide sufficient representational structure to solve the problem, which is a form of meta-problem and requires some thought.
(2) The transformer architectures are able to solve these recursive sequence problems much faster than evolutionary techniques; they have much better 'inductive bias' and *memory* of everything they've been exposed to.
	Hell, this means that the old boostrapping dream is solved!
(3) As mentioned, SGD is opportunistic but needs to be poised where opportunities occur semi-spontaneously.
(4) Probably routing / gating is as important as multiplication; I was wrong by over-emphasizing the relevance of MUL above PASS
(5) Transformers can represent not just statistics, but more relevantly and importantly, basic algorithms e.g. conditional copy or analogy-making.  That these are evolved and not baked in is truly amazing!
(6) Transformers are a remarkably linear system: almost everything ouside of the attention matrix is linear (or ReLU), and when the attentional matrix is fixed, its all basically linear.

The reading experience gave me far more respect for the emergent ingenuity of SGD and the remarkable generality of Transformers, and lead to significant questioning of what I'm doing here (again.. hah)

Intuition suggests that trying to avoid SGD might well be a purely academic exercise, useful by virtue of getting notoriety, but one must be cautious and thoughtful about what needs to be done.

I think the original motivation was right: you just need too many training samples with standard deep learning to scale well to large, diverse, real-world datasets.  (This with the caveat that many passes through a dataset might well be required to infer the structure ; fewer passes would require more prior information.)
Or, put another way, need a method to learn representations & routing control protocols (aka algorithms) in a way that solves the
(a) coordination problem (= determine representations per layer)
(b) alignment problem (= reduce the error of reconstruction).
(c) Noisy inference problem (= make sense of an ambiguous world based on accrued experience)


May 23 2022
Need to record thoughts from Friday (Boom in the forest) and this weekend (Yosemite with Tom and Will).

Boom in the Forest:
Jason Yosinski: ML researcher working on wind turbines using networks of pressure sensors.
He's worked in machine learning for decades, and evolutionary algorithms for at least this long too.
Suggests that in any sufficiently high-dimensional space, half the gradient directions will be better than the other half (but parhsp not better than the current solution), so improving a network or genome requires only finite bits of luck -- only a few bitch of coordination need to come together at the same time.
He also reiterated that many solutions, like eyes, had intermediate forms that were selectively beneficial.
Led me to think (again) that predator-prey dynamics, which may have been essential to the Cambrian explosion, is useful for iteratively improving the 'loss landscape' ala GANs, allowing effective exploration without needing to have large quantities of luck at any given point (or makes getting stuck in local minima less likely).
After digesting this for a while -- and looking at
A -- How current machine learning models seem to work (they exploit latent structure in a network)
B -- The small successes of the maiden-ogre experiments
I think it's work revisiting the skeleton experiments, just with a different eye to the objective function (must be dynamic) and a possibly different conceptualization of the meta-objective (make it incrementally possible; no leaps are required..)

Yosemite / Hetch Hetchy:
I'm remembering what Chongxi said about his son, that he could recite a Chinese poem long before he learned the meaning of the words.
This also came up while thinking of long-term episodic memory & its incredible *depth*.  How many roads have i travelled, how many places and events are actually well recorded in my memory, if only they can be queued / accessed!
If we turn the task of the neural auto-encoder on its head, and say that memory (remembering) occurs before generalization, then the task is quite a bit different: take a database of all examples, learn what to throw away and learn an auto-associative index on them.
This is hard, of course; you need to know an ordinality/ordering to make decision about what to throw away and how to assess similarity.
	(The difficulty in assessing similarity is one reason why both skeleton.py and cortex_andor4.py have a hard time: they have to search over many possible encodings / addressing schemes (in addition to there being plenty of bugs haha)
Which led to the next thought: the way to infer an ordering / indexing on highly nonlinear compressed yet low dimensional data is to
(1) *experiment* with the natural world, eg. be able to move about it.
(2) Assume that most causal factors are low-d, piecewise constant or piecewise linear.

Plenty of oldish literature to suggest the latter, but I think that the former is perhaps more interesting and relevant, esp. considering the sample efficiency of the human brain
	Ish .. Yan LeCunn just tweeted that the average five year old has on the order of 5 * 365 * 12 * 60 * 60 * 20 = 1.5 billion frames of video from which to infer the structure of the world.
		* Some things, like the emergent algorithms of transformers, may be emergent only at large scale..
		* Still, you really need to start with the right substrate.

I'm sorta imagining something like recursive LSH, where there is enough structure in the indexers / predictors to allow low-gradient selection of working addressing schemes.
The task of inverse graphics remains a great testbed, but needs to be broken down in significant ways before scaling.



Hmm, seems transformers can already do something interesting:
Geometry-Free View Synthesis: Transformers and no 3D Priors
https://arxiv.org/abs/2104.07652
Ah, transformers.  But need to see what the VQGAN is doing in there.

May 24 2022
Critical point of the mammalian cortex as compared to current machine learning models:
--> The preponderance of feedback to disambiguate sensory experience!

Meta thoughts:
Algebra and theoretical models are an essential "crutch" or tool for problem solving.
I may be running in circles because I've not engaged mathematics / algebra nearly well enough.
And perhaps the reason deep learning is working so well is that it does engage algebra and calculus (at least the chain rule of differentiation) *explicitly*
Which implies that trying to avoid derivatives / gradient free computation is unecessarily cripling the approaches.  As I've intuited for a while.
Therefore, it's a good idea to use gradients for the forseeable future, where practical (e.g. not analytical gradients in evolution).  Yes!

Alright, gradients work really well and are super easy ..
as we well expected.
Can easily do nonlinear compression with the outerupt functions.
Only need a little bit of tweaking of the representation to get this working..
Results were with a roi of 5 on the 28 x 28 image.
fi1, fi2 = outerupt_indx2_flat(siz, roi, True)

Try with ROI of 1 -- this will be much closer to PCA (at least for the forward pass)
should run faster.  fewer terms in the expansion.  (doesn't seem to be running much faster..)
Really should add a batch dimesion to improve speed..
End loss is 11
w_f (32, 784)
w_b (784, 528)
HUH seems you ** don't need the extra forward terms? **
	Also note that there are digital-ish errors in the reconstruction.

Back to roi = 5
end loss is 10 (not much better eh?)
w_f (32, 7696)
w_b (784, 528)
What about straight PCA?
oops didn't do that.

What about only nearest-neighbor cross-terms?
end loss ~15
w_f (32, 784)
w_b (784, 63)
	(some pixel errors here too, fyi)

OK, now real (nonlinear) PCA / one hidden-layer network:
end loss ~ 16
w_f (32, 784)
w_b (784, 32)
(Why this obsession with DBN?  This seems to work just fine TYVM)

If we run it for 2 epochs,
end loss ~ 13
(Still getting the pixel errors..)

Turn back on the nonlinear feedback. Q = 528
w_f (32, 784)
w_b (784, 528)
Also run for 2 epochs.
end loss ~ 9
(not sure if it could go down further?)

What happens if we reinstate roi, only even bigger, 7?
	Really need to quantify based on the test set not train set!!
	If we increase the matrices enough, eventually it can store the whole dataset..
w_f (32, 14772)
w_b (784, 528)
end loss ~9 again.

Question: Can this compression be used to predict class labels?

May 25 2022
YEs, it can! 
However, if you use the full 60k training set, it's still better to just do linear regression to select the label (as assessed by MSE..)
If you cut down the linear regression size to 100, 
then the compressed representation is slightly better. 
	compressed mean label prediction error 5.7132864
	naive mean label prediction error 6.7690516
That's with a ROI of 7. 
Try with ROI=1
	mean train loss 9.757882
	mean test loss 9.6124525
	compressed mean label prediction error 5.7888865
	naive mean label prediction error 6.9488473
Ok, so ... about the same. 
What if we increase the representation size? (48)
	mean train loss 7.382174
	mean test loss 7.2273593
	compressed mean label prediction error 6.494618
	naive mean label prediction error 6.2625747
What if we decrease the representation size? (20)
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 14.872782
	mean test loss 14.594089
	compressed mean label prediction error 5.20966
	naive mean label prediction error 6.1839123
Good!  So, it does better.. 
Adding some translations: 
	torchvision.transforms.RandomAffine(10.0),
gives: 
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 15.875707
	mean test loss 14.274618
	compressed mean label prediction error 5.3931966
	naive mean label prediction error 7.572067
Looks promising!! 
Try again with 20deg rotations: 
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 16.331211
	mean test loss 13.986706
	compressed mean label prediction error 4.5654993
	naive mean label prediction error 6.855
Oh wow!!?? Even lower error. 
Let's throw the ROI=5 back in there. 
		{ Aside: really need to enable batch mode here.. speed up training}
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 15.717631
	mean test loss 13.437547
	compressed mean label prediction error 5.3180842
	naive mean label prediction error 7.040411

Wnat to run more experiments but really need to eat something! 
ROI = 5, rotation = 40,
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 16.888586
	mean test loss 14.305665
	compressed mean label prediction error 5.9888306
	naive mean label prediction error 8.747594

Added in a batch dimension.. speeds up computation, but slows down convergence.
Interesting.

With a batch size of 8:
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 17.341217
	mean test loss 14.789629
	compressed mean label prediction error 5.616935
	naive mean label prediction error 8.502002
With a batch size of 32:
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 30.184021
	mean test loss 28.65005
	compressed mean label prediction error 5.768119
	naive mean label prediction error 8.599775
BS 32, ROI 3:
	w_f (20, 3700)
	w_b (784, 210)
	mean train loss 38.49564
	mean test loss 37.350113
	compressed mean label prediction error 6.2187138
	naive mean label prediction error 6.6117

Umm.  I think I might be doing this wrong.
The label ought to be one-hot!  Categorical!
Of course it won't work super well with just the numbers 0,1,2..
Yeah, duh.
Below, one epoch, roi=3, batch=32
	w_f (20, 3700)
	w_b (784, 210)
	mean train loss 38.642357
	mean test loss 36.807293
	compressed mean label prediction error 0.072603054
	naive mean label prediction error 0.095275916
Below, two epochs, roi=1, batch=8
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 18.2238
	mean test loss 15.588934
	compressed mean label prediction error 0.07381655
	naive mean label prediction error 0.10375466
Below, two epochs, roi=1, batch=64
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 38.745132
	mean test loss 37.064823
	compressed mean label prediction error 0.06895668
	naive mean label prediction error 0.09322322
Run again (since it's fast..):
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 37.436176
	mean test loss 36.331757
	compressed mean label prediction error 0.08552185
	naive mean label prediction error 0.106565624
Increase TN, the number of examples used in the linear regression, to 200.
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 41.640827
	mean test loss 40.023956
	compressed mean label prediction error 0.06269046
	naive mean label prediction error 0.11225252
Humm..
Bump the hidden dimension to 32.
	w_f (32, 784)
	w_b (784, 528)
	mean train loss 20.714134
	mean test loss 17.512472
	compressed mean label prediction error 0.061895754
	naive mean label prediction error 0.11251253
Without rotation:
	w_f (32, 784)
	w_b (784, 528)
	mean train loss 17.407146
	mean test loss 17.232634
	compressed mean label prediction error 0.055899657
	naive mean label prediction error 0.10356578
Without rotation, hidden dim 48, roi 3:
	w_f (48, 3700)
	w_b (784, 1176)
	mean train loss 17.438473
	mean test loss 17.392864
	compressed mean label prediction error 0.089243285
	naive mean label prediction error 0.11814148

Note:
tried changing the output / final nonlinearity.
It's quite sensitive to this!
Changing to
	l1i = jnp.clip(w_b @ l2r, -0.2, 1.2)
Causes the model to not converge (!!!)
It makes it so the outout has only a few bits on.. why?

Sigmoid output on l1i gives nice results, but the codes are saturated and bad.
Am thinking that we're missing some bias terms here .. ?
So that the slopes can be inverted?
Maybe helps, maybe not.
Seems to help with lost pixels at least.
	w_f (32, 3700)
	w_b (784, 528)
	mean train loss 10.11068
	mean test loss 8.062884
	compressed mean label prediction error 0.057571307
	naive mean label prediction error 0.11125314
Again:
	w_f (32, 3700)
	w_b (784, 528)
	mean train loss 10.0058565
	mean test loss 7.905799
	compressed mean label prediction error 0.056958407
	naive mean label prediction error 0.10198992


Seems this is about as good as we might get --
roi = 5 batch = 16, P = 32, epochs = 10
	w_f (32, 7696)
	w_b (784, 528)
	mean train loss 9.989642
	mean test loss 8.041153
	compressed mean label prediction error 0.05843653
	compressed one-hot label prediction error 0.053519998
	naive mean label prediction error 0.10049593
	naive one-hot label prediction error 0.079059996
Lacking any further information, seems like ~ 5% error is as good as possible without using supervised data (probably?)
roi = 5, batch = 16, P = 20, epochs = 10
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 15.351827
	mean test loss 13.229023
	compressed mean label prediction error 0.056146413
	compressed one-hot label prediction error 0.04976  <-- slightly better
	naive mean label prediction error 0.09369152
	naive one-hot label prediction error 0.08028

roi = 5, batch = 16, P = 20, epochs = 100
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 16.233717
	mean test loss 13.735222
	compressed mean label prediction error 0.060591716
	compressed one-hot label prediction error 0.057
	naive mean label prediction error 0.105436824
	naive one-hot label prediction error 0.095359996

Ok!  I think this is as optimized as I might well get at this point ...
Sure I could fiddle with it more but .. eh?
A days+ experiment seems effective, time for a

May 26 2022
Looks like my fancy nonlinear system works ...
exactly the same as good old PCA in terms of dimensionality reduction. 
With 16 epochs: 
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 16.316217
	mean test loss 13.730327
	compressed one-hot label prediction error 0.061139997
	naive one-hot label prediction error 0.08794
	PCA one-hot label prediction error 0.06172
	
OK, turn off all the goofy multiplication: 
epochs = 5, roi = 1, no nonlinear expansion of hidden state, 
This and the previous used 40deg affine transforms
	w_f (20, 784)
	w_b (784, 20)
	mean train loss 21.786966
	mean test loss 19.35434
	compressed one-hot label prediction error 0.05706
	naive one-hot label prediction error 0.09804
	PCA one-hot label prediction error 0.06388
YEah .. just nonlniear does very slightly better. 
	w_f (20, 784)
	w_b (784, 20)
	mean train loss 20.900702
	mean test loss 18.523268
	compressed one-hot label prediction error 0.06888
	naive one-hot label prediction error 0.09566
	PCA one-hot label prediction error 0.0729
{ of note: some of the examples even I can't recognize! } 
Let's try only nonlinear input expansion !? 
( should be almost exactly the same)
Yes no change. 
	compressed one-hot label prediction error 0.067719996
	naive one-hot label prediction error 0.097219996
	PCA one-hot label prediction error 0.070379995


Friday May 27 2022
Seems like it's time for a bit of review!
Let's go through the source code (and maybe some papers) and review the lessons learned.
* cortex_nn.py
	Used to iteratively estimate a 8x4 matrix (4 real dimensions, 8 apparent)
	This with either one-hot activations or random combinations of the 4 vectors.
	Started the l1 l2 nomenclature...
	Not really sure if this worked so well.

* cortex_nn2.py
	Iterative forward-backward trail, with three layers.
	Raised awareness to the fact that that hidden layer is hard to correct -- how to allocate responsibility / credit to either the top layer or the bottom layer weights?  How do the upper layers contribute to prediction in the lower layers, if the hidden layer depends on both?
	Fun expermients, but inconclusive.
	Still don't really know the answer to responsibility / credit issue .. I think, when the stimulus is ambiguous, there is no way to really represent hidden layer activity without upper causes (hm?)

* cortex_2axis.py
	This one actually works.
	The task is to take a 3-dim input (6 with complements) and produce a 2-bit address / index using feedback alignment.
	Yes, the nonlinear hebbian rules can do this in the tiny toy example.
	Well, better than not working!

	Gave some confidence that the Hebbian learning + E-I balance + feedback alignment is not totally riddiculous..

* schmid_test.py
	This is after Schmidhuber 1999
	VAE is a 3-layer, one hidden MLP
	Probably I got this from the web somewhere, particuarly given the print statements.. using save_miage as opposed to plt.imshow, fwiw.
	It's a beta-VAE, where the beta term grows from 0.25 to 0.75 (ish)
	The latent representations seem not so disentangled,
	consistent with the architecture..
		[1]S. Hochreiter and J. Schmidhuber, “Feature Extraction Through LOCOCODE,” Neural Computation, vol. 11, no. 3, pp. 679–714, Apr. 1999, doi: 10.1162/089976699300016629.

* cortex_bumps.py
	This was written in the Mojave (partly) .. idea is to predict a series of moving bumps, rather than predicting binary patterns, using the same framework as above.
	That is, the next frame was predicted from the current frame + the current hidden activation.
	Spent quite some time trying to figure out how to represent something like 'pass gate' topology.
	This is really another, perhaps overy complicated, version of activity-activity multiplication, that required the use of a weight *tensor* not matrix, which is quite the excessive memory representation.
	(Don't think we should generally use weight tensors in the future; there has to be a better way to represent or factorize .. that said, the Anthropic work makes quite successful use of tensor products in understanding how transformers work.. hmm but that's for understanding, transformers are still mstrix-matrix mults.
	I don't think it ever really worked, partly because what information was kept around was not clear.

* cortex_bumps_sgd.py
	One of the several times I check to see if the topology actually works by using SGD and autodiff.
	* but still with the pass-gates
	It does, pretty sure.  But quite slow; this is not a great way to represent the problem, per thoughts above.
	Unsurprisingly.
	{Still not a bad torch implementation.  Building the calouses and foundations.. }
	Looking at the plotted output, unclear if it's really disentangling anything... the reconstructions are fairly noisy around zero (might well just need appropriate clamps / nonlinearities.. )

* cortex_mnist.py
	This was the first 'complete' script written while at Loyal, and it is actually pretty successful!
	Is able to quickly and semi-reliably reconstruct MNIST digits in the two-layer autoencoder topology per cortex_2axis above.
	The nonlinear Hebbian learning works well, even if it never fully cleans up the residual errors.
	Most importantly, 'conjunctive' code allocation + explain-away sparsification does seem to work well, and it all converges rapidly, ~ 3000 steps or so.
	Feedback alignment (in the local meaning of the term, not lit) also works correctly.  Asymmetric learning rates may contribute to this.

	Problem is that conjunctive features only go so far for representing the digits ... need conditional features, at least.

* cortex_andor.py
	This version had nonlinear dendrites, was written in torch, and is trained to 'compress' or index the glider-checker task.
	As of today, it sort-of works..
	This features the product-of-activity outer product, nonlinear hebbian correlative weight update, noise bootstrapping (which i like), and a supervised option.
	It was designed as the simplest toy case to think about & test conditional / activity * activity features, in a quasi-biological way (nonlinear dendrites).

	Lessons: learning arbitrary indexes/compressions is quite hard, system is prone to instabilities.
	A highly compressed representation is fragile: if you change one address line or bit, then all the other ones need to be changed as well!
		Of course this is not true of the real world ...
		it is fractionally spaced, not full for a given set of axes.
		E.g. not all combinations of all things; there are categories of axes, axes of axes so-to-speak.

cortex_andor2_be27787.py
	So this solves the glider-checker problem!  Almost perfectly!
	Proving above wrong, or at least incomplete, I guess.
	Yet it's a tricky system that required a fair bit of work to function,
	and is also unstable.
		(One part of the solution is simulated annealing, though even without this it seems to converge quickly..)
	I think in general something in this realm could more generally work when, per above, the full address space is not used, or the address space itself is factorized / structured.

cortex_andor3.py
	This was after skeleton.py (below) but in the same series; I switched from pytorch to JAX here.
	Got rid of dendrites, kept the activity-activity outer product mostly intact
	When encoding eight symbols with 4 bits, it always seems to miss one.
		Seems that it gets stuck in a local minima, where updates on one symbol exactly counteract updates on another, and the feedback can't push it to a new representation... and noise just can't push it out of the hole.
	Increasing the latent dimension fixes this, i think.
	Model still works with a latent of three dim, i think.

	Remains toy, but still pretty OK.

cortex_andor4.py
	Took andor3 above and switched to MNIST.
	No SGD, still E-I balanced feedback.
	But but, did use non-linear least squares to convert from the expanded latent dimension to the compressed latent.
	Doesn't work that great -- turning down synapses to and from the uncompressed latent is very tricky!
	Spent some time on this one, but the approximations / reconstructions never were that good..
	Some aspect of this might be tweaking the representations and feedback, some aspect might just be the overall sensitivity of the model / instability of the encoding.
	As mentioned, if one address / index changes many other encodings also need to change, which makes the problem more tangled hence harder to solve.
		-- Seems like a fundamental issue!!
	What if you simply don't change an encoding once it is set?
	(Other than if other encodings completely take over predicting for one mode .. then you need some learning rule or something to make sure the weights decay.)
	* Model is surprisingly sensitive to learning rate!
	Needs more negative feedback loops for sure ...
	but that said, at this point I was getting itchy that the network struture itself was limiting, and that I didn't understand what the problem was / how a 'good' way of solving it might be.
	As in: are activity*activity maps a reasonable representational basis?

	All this was prefaced, as indicated by the names of the scripts, that current MLPs are pretty good at linear 'OR' operations. Input A or input B (times weight) gets summed to produce output (unless negative weight..)
	What you seem to get in dendrites is more of an 'AND' operation.. you need both conditions for there to be output (~= multiply).
	Hinton definitely mentioned this.
	Many people have worked on it, but of course the reresentations become entangled then ... which is part of their power.
	It can also be thought of as an 'if' operation.

	IDK, looking at the output again now, I think there is a bug somewhere.

* cortex_andor5.py
	Finally gave up and went with SGD!
	Are these representations even reasonable?
	Yes, they are.
	Note: did non use non-linear least squares here, just propagated the gradient through the whole model & had asymmetric w_f and w_b matrices.
		e.g. w_f went expanded l1 to l2; w_b went expanded l2 to l1
	Yep, it works, and offers reasonable reconstructions.
	Can be used for few-shot learning of classes with an accuracy very slightly better than boring-old PCA
		PCA on whole training set,
		Bio-autoencoder (BAE) on whole training set,
		then supervised learning on 200 samples of the training set
		and testing on the whole test set for both conditions.
	Plenty of fiddling showed that it's mostly the nonlinearity, not the AxA, that lends efficiency in semi-supervised learning.
	A result in and of itself!

* skeleton.py
	Much work went into this & I think much work remains.
	From Tim Saliman's work, I think that virtual (or not) batch norm might be required to get better per-neuron behavior hence overall behavior-solution diversity [1]
		(notes that this was essential to get some behavior out of convnets, where intial noise does not produce meaningful actions.)
	He also offers some good insights on parameter fuzzing vs policy fuzzing [2]

		[1]T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, “Improved Techniques for Training GANs.” arXiv, Jun. 10, 2016. Accessed: May 27, 2022. [Online]. Available: http://arxiv.org/abs/1606.03498
		[2]T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever, “Evolution Strategies as a Scalable Alternative to Reinforcement Learning.” arXiv, Sep. 07, 2017. Accessed: May 27, 2022. [Online]. Available: http://arxiv.org/abs/1703.03864

	Big open question of how to set the objective function properly to get good encodings?
	Also, we need to incorporate environmental interactions!
	(Spoke with Tarin about this yesterday; see alt-ai.org)
	Will have to write more later.

June 2 2022
Since the last entry, have had dinner with Luke & learned a bunch of things.

(1) A number of theoretical physicists have entered the deep learning field, I think at GB or DM, and they have done very well.  According to Luke, this is because they have been *careful* -- I'd assume, do the associated math..

(2) Very large image and language models are getting to be both compute ($$) and data-limited; it's hard to curate more of the web to improve the results further.  Very interesting!

(3) Luke mirrored my questioning: So what exactly do we do, now that Transformers, or at least objects with attention and resnet units (and probably convolution) seem to be cleaning up?
	-- emphasis on "seem to be".  See the caveats above, and questions about the data / compute efficiency of this class of methods.

(4) Mentioned Adept.ai, which is a new spin-out of Google Brain, working on using large language models + program synthesis stuff to automate common tasks on the computer.
	It's a good idea, think it could definitely work!  Strong team! (Though allegedly Augustus Odena is a character.  Are we not all..)
	-- Aside: some aspects of the Transformer may seem obvious in retrospect; isn't that true of many things though.  I suppose Vaswani and Parmar needed a lot of experience to gain insight that attention is indeed all you need.

(5) Luke has been working on a meta-learning 100+ task corpra, and has found that whatever algorithm he has works for most, but it can never work for all / and it can't work on itself.  Have to see what the tasks are to better understand this.
	This led to a short discussion of conciuosness and symbols / meta-symbols and the possible need for recurrence / a recursive syumbol manipulation system for "fully" closing the loop.
	Talked a good bit about this on my "swan hike" with Judy, Arthur, and Alex..
	Hard to recreate that magic, but I do think it all comes down to computed-addressing / "pointer arithmetic" and amortized search.

(6) Which is really what transformers are doing, I think.
	-- They compose / create both algorithms (as in the Anthropic paper, where cetain heads are shown to copy data based on key-query matching; presumably more complicated algorithms can arise with this sort of match-search-forward algorithmic substrate?) and do good function mapping (via the many layers of MLP and resnets.)
	-- Can do pointer math, maybe even!
	-- Certainly can do conditional execution & express higher-level probabilities.  Probably do this in a way that is better (?) than just activity-activity multiplication because of the added (minimal) structure (e.g. search)
	-- My rough estimate is that for these sorts of models to work, you want to use the absolute simplest representation possible (e.g. MLP + ReLU / swish seems like just barely enough nonlinearity to represent anything; query-key-value is probably just the right quantity of complexity to allow algorithm composition
	-- Keeping everything as linear as possible, in high-dimensional spaces .. just  seems to work with SGD.  The two are compatible.  SGD is a very effective way of tuning weights in high-d spaces. (where everything a saddle and vectors tend to be orthogonal..)
		-- I think that the "secret language" of Dall-E is an example of this.  Vector representation of what is roughly discrete or factorial representations... you get aliasing, but also generalization.

(7) skeleton.py:
We're asking it to do roughly a NP hard problem.
It can't reliably do this in the alotted time; cortex_andor* has a lot more time to anneal down to factorized variables.
If we want it to work, need a sensory-motor loop to allow experimentation to cut the ambiguity & allow for quick factorization.
There is really no other way, other than brute force exploration, and then you get into the problem of entanglement of addressing lines / factors.
	(This might be a problem in the task too?)
Yeah anyway, time to not be lazy ... and probably increase the complexity and capacity of the network (at the expense of a smaller population, but so what)
Definitely worth a shot
	(though I'm leaning more toward the 'artisinal' now ;-)

June 8 2022
Spent the last week-ish binging on papers. I think I have a better lay of the land, at least as it stands at present; now need to organize my thoughts (and also re-read all my old notes...)

1. Predictive coding and equilibrium propagation are fundamentally the same thing -- an iterative means of propagating error derivatives backwards through a computational graph.  Provided you have error units, predictive coding even reduces to Hebbian learning (which is what I've implemented).
	Notable that this can be derived from Friston's free-energy principle.  While I've shied away from this in the past, I think that, as Donald Knuth says, you need to lean into mathematics -- it may be a crutch, but it's also a tool for navigating difficult conceptual areas.
	In light of this, I should be writzing more calculus!
	And need to read the free-energy tutorial.
2. Predictive coding / equilibrium propagation *do* solve the forwards-backwards coordination problem, but but some elements of coordination may be avoidable, vs-a-vis Sindy Lowe Putting an end to end-to-end.  You may be able to learn OK representations locally & greedily.
	Caveat: in all Hinton's DBN work, the layers are trained greedily, but then you need a period of backprop fine-tuning to "get the elements to work together" or so.
	Greedy training gets the parameters approximately in the right starting point. (although he offers no proof of this; it seems rather to be an intuition.)
	One assumpion in the Lowe work is that the underlying latent / generating factors do not change on a rapid timescale, hence end to end-to-end uses a local contrastive loss, InfoNCE, (Oord 2018).
		This seems to be sound & has much precedence e.g. Linsker
	Another caveat: both predictive coding and equilibrium propagation require many passes to move their errors around the hierarchies.
		Thought: depends on the markov blanket / complexity of the computational elements you are trying to select for!
3. Contrastive methods, as used in wave2vec, can be used with backprop to very effectively build a unsupervised decoder.
	The way that this system works is that they train contrastively:
	They mask off certain portions of the sampled, compressed latents, input that into a transformer, then use the transformer to predict the class membership of a quantized version of the masked latent.
		Quantization is through a differentiable Gumbell softmax (which includes a sampling step!)
		Quantization was important to improve the latents, and is consistent with the underlying discrete nature of speech (though they do not analyze this..)
	That is, they don't predict the actual quantized latent ... just try to maximize the difference between that and other codewords in the same utterance (humm?)
		This is a common refrain -- you do not want to predict the whole input sequence,
		but you want to be able to discriminate that sequence from everything else.
		This is different from the 'pure' version of an auto-encoder;
		having to re-represent all of the features as required by e.g. L2 loss, puts a heavy requirement on the encoder half.
	Other contrastive systems work on the principle of predicting the next sampled symbol (which is predictive coding in another form)
	There is a transformer in there too, which is trained via the contrastive loss...
		Note that the fill-in codeword is trained as well.  Probably some tricky clever engineering in there!

	These contrastive losses on semi-auto-encoder models seems like a much more data efficient way of training compared to GANs.

4. Search!  Distributed search (hierarchically and spatially) is vital to breaking inference problems down when perfect / algorithmic inferences cannot be found.

5. Spent some time on hashing / locality sensitive hashing / spectral hashing etc.
	Hashing can of course be very fast,
	but LSH does not seem to be per se a powerful way of organizing or generalizing over data.
	E.g. the way to make LSH work better is to train a DBN (greedy + fine-tune) to do the hashing,
	Or to look at spectral methods (e.g. find the PCA of the data, break the space up using non-outer-product eigenvectors = hash functions)
	This is not directly performing computation on the dataset in the same way as SGD on epochs of data is doing it..
	You could imagine extensions of the process that refactor, contrastively perhaps, the hash functions (= adresses) of the data based on input occurrence.
	This would be consistent with phenomenology.  You don't need an auto-encoder of the data if you simply ... remember it!

	As for addressing, can do some sort of online incremental version, ala NEAT etc, where once some old index no longer is needed, it can be repurposed...

	Problems with hashing:
	1 -- It's not clear how to make it hierarchical.
		Do you hash addresses, recurrently?
		Or concatenate the addresses to bits of the data?
	2 -- Not obvious how to deal with inference of / use of latent variables.
		Is this like a Hopfield network, auto-associative fill-in?
		Latent variables = position of objects, identity of objects, binding of attributes into whole, etc.
	3 -- Not obvious how to use straight memory to do 'computation'.
		Computation is necessary for generalization / compression.
		Computation is (surprisingly) effectively done well by transformers, which do some sort of variable binding and forwarding magic & can iteratively explore this space via SGD.
		Networks like ResNets, CNNs do less computation than function approximation.
			Hedon: can you have meta-symbols, functors sorta, that work on higher-level symbols?
			That would allow for even better generalization performance.
			Maybe this is possible by making the system properly re-entrant and reflective (per consciousness discussion above)?

Ok, start with a database. MNIST, SHFN, all youtube videos etc.
This already has an indexing variable
	imperfect but very useful: time and space
Find other indexing variables that
	-- Allow you to rapidly search over the recorded data, ala LSH
		* ANNs are definitely a form of fast readout: you just propagate activations through a network to read out the memory (function approx)
	-- Allow interpolation
		* ANNs are also great at this, by interpolating in the latent space & again reading out the input (reverse propagation).
		StyleGAN and Transformers are perhaps the best ...
			because they do the best at approximating data with pseudo-algorithms.
	-- Correspond to real factors of variation in the world
		* This is similar to the disentangling hypothesis / set of ideas.
		You want to make the latent space roughly correspond to the structure of the world.
		This obviously makes indexing much faster / easier,
		and it makes the database required much smaller: you can combine values (as in key-value store) compositionally to predict the world.
	-- Perform processing gain on the stimulus
		* Similar to above.  procesing gain = compression.
		There seems to be no way around the need for extensive computation to find modules that evince processing gain. You need a lot of compute!
		Also, it would seem that making an advancement on SGD is very hard if not impossible.
		That said: SGD is harder to parallelize than biological update
		yet it's not obvious how biology solves coordination problems (finding coalitions of good settings across levels of a hierarchy)
		and SGD is ideally good for this.
	-- Utilize motor output / reafferent / experimentation to disentangle the world
		In this sense, there are XXX ways of pulling out independent factors of variation in data:
		1. PCA.  linear methods. SVD.  Generally efficient. Not so powerful (just a rotation and scale.)
		2. Linear projection + token nonlinearity (ReLU),
			trained via SGD, or *prop.
			Even rectification offers enough processing gain to support completely general function approximation; modules learned in the process of supervised learning have proven to be useful for other tasks and/or tend to be general factors of variation.
		3. Nonlinear PCA / ICA
			Use higher moments of the distribution of data to extract separable components (blind separation)
			E.g. data is never perfectly white & tends to follow categorical or discrete distributions while noise may follow gaussian distributions.
				The original Sejonowski and Bell paper (1995) focuses on linear mixtures of non-gaussian sources based on the InfoMax principle.
				What about nonlinear mixtures of non-gaussian sources?
		4. Kernel trick, then PCA or ICA.
			Take a nonlinear problem and make it linear by dimensionality expansion.
			Suffers from the curse of dimensionality, alas -- and still needs a metric for selecting the (linear combination of) output dimensions.
		5. Slow variance of latent factors in the real world.
			Wiskott and Sejnowski 2002
			For example, when hallucinating a video of mountain biking, the only thing changing is position (and even that is conditional on velocity, conditional the terrain elevation etc. )
			The world is not changing (except the clouds wind etc)
			We have reduced things to maximally static!
			Seems like a good goal.
			A perhaps more realistic model is piecewise linear:
			There are abrupt transitions in the underlying latent variables.
			Like speech!
				Note note: "performance degrades if the model is forced to learn multiple invariances simultaneously" hmm.
		6. Experimentation.  'Natural scientist'
			Some papers on using self-motion signals as a supervisory signal (predict the motor signal from the visual field, e.g.)
			More can definitely be done here.
		7. Brute force.
			This is equivalent to partitioning a graph, which allgedly is NP-hard.
			Ran into this problem directly in the glider-checker task in that maximally compressing address variables are inherently entangled with each other.
			If we were to address sequentially, might work?
			Or might just find bad encodings.
			Or perhaps if we allow much larger encoding /addressing spaces, then subsequently weight-decay pare down it should work?
			Is this sufficient to evince computation hence compression?
			Also: how to do this in a small-enough way (e.g. ReLU is just barely enough of a nonlinearity) that it's explorable.


Thoughts:
	Using backprop makes it much easier to use only feedforward architectures, but it does not preclude you from using feedback (e.g LSTMs, BPTT)
	cortex_andor5:
		Has about the same efficiency as PCA for image classification
		Which is saying a bit .. PCA is easy to compute and (present day) obvious.
		Predicts (is an auto encoder) in pixel space, MSE in pixel space ... one way to regularize this is to add a contrastive term.
			Might be worth experimenting with.
		Far better is to seed disentangling with motor reafferent.

Hedon:
	One reason for the highly asymmetrical morphology of cortical excitatory neurons is the need for address / value computing.
	Some things, such as removing translation or zoom variance natuurally factorizes as an addressing operation: you add the eye position to the 2D address, and automatically re-index to head coordinates.
	Likewise for head to body, body to world..
	Indeed, indexing (gather) or even scatter opeartions are very general, and can easily be used to generate algorithms, which is really the (only) way to permit low-data generalization.

Yes but:
	It makes sense, but how to actually implement it?
	Addressing signals are back-propagating to L1/L2?
	Value signals are axons that ramify to L4? From L5/6?
	There are definitely more layers here than two, and many more neuronal subtypes to deal with.


July 4 2022
	Happy independence day!
	Going on a trip to NM in a few days, so it's good to get some work done today.
	Finally got key-query mapping working for circles; need to rejigger the weight / key update rule. 
	Rather obvious in retrospect. 
	Might not be biologically plausible in the current instantiation / present meta-parameters, but meh. 
	
	Todo: 
	-- One-hot obviously will not cut it for more complicated or interesting input data. 
	As before, we want something closer to a factorial encoding or 'dense' encoding where addresses (here 100..) are maximally (or fairly) uncorrelated, and can contribute to different parts of recognizing and recreating the sensory space. 
	
	I suspcect that a second layer of tuning is required here; cortex_mnist *does* decompose the space into conjunctive features, which sum to reproduce the digits.
	(And with SGD, cortex_mnist5 does offer slight processing gain..)
	Presently the key matches on the entire input space, which doesn't make sense; there needs to be a 'don't care' or 'don't connect' vector. 
	Alternately, we combine the two topologies so that 'don't care' is equivalent to zero forward or backward weights; it hence does not contribute to the reconstruction. 
	Or, as I proposed with Laura Demming: key-query indexes a value, which subsequently sums.

	There are two problems here:
	1 -- Re-creating the stimulus, ala an auto-encoder.
		Work so far has focused on E-I balance, which is plausible, though
		- there does not seem to be a ton of evidence for backward inhibitory projections, and
		- it's not clear how to scale the system to multiple layers.
			Oldhausen also ran into the problem ... you get a conjuction of linear features, but need backprop to make them work together (coordination problem)
	2 -- Deliberately segregating the query/address space to allow invariance: 'don't care'.

	re 2, Probably the most obvious solution is the best?
	Add a second set of coefficients which acts as a weight on the K-Q match?
	But then, how to update the gate?
	Maybe a momentum term ... if the input data consistently matches the key, then increase to saturation.
	If it's unreliable, then decay to zero.
	But try to make the gate vector as large as possible.
	re 1, perhaps the solution is some sort of reprocity?

	Getting a weird bistable behavior here ...
	that of course needs to be fixed (probably wrong learning rule?)
	Also probably need a dataset with a bit more statistical structure
	Or hidden layers that are smaller.

July 5 2022
	Looking over what I did last night:
	-- The 'gate' weight seems redundant, even though it does support the desired operation: allowing for partial key matches (keys 'focus' on maybe 1/4 of particular circles..)
	-- The redundancy encourages a moving dynamic of keys and gate, which maybe makes sense, but is not really desireable..
	-- I wonder if the right solution is to incorporate E-I feedback control here.
		The current network is not trying to minimize any error.
		So it's challenging to say "how well it's doing".
	-- System is super sensitive to hyperparameter (weight decay) change.
		0.999978 works, 0.999878 does not work.
		Not a good sign.
		Probably if we increase WD further (slow weight decay), the gate variable will move around even more.
		Nope.  Slowing weight decay makes it more stable.
		Decided to get rid of weight decay and replace it with total weight normalization ..
		Divisive normalization doesn't work,
		but substractive normalization works just fine & properly encourages the keys and gates to focus on smaller regions of the stimulus space.
		(see 20220705_circles2)

	Question is: beyond address-space segmentation, what really do we want to do here?
	Ultimate goal: via a series of layers, convert this circles tasks into a compressed, disentangled representation: cx, cy, radius.
	* With, of course, the help of these supervised variables!
	* Without, of course, any SGD or actual supervision.
		This may require some heuristics to solve coordination problems.
	* In such a way that, with additional time, it could *conceivably* also disentangle in a wholly unsupervised way.
	* In a way that it generalizes to problems other than circles, eg MNIST, images, video, stereo video...
	* Incorporates something like E-M or iterative inference of address/query latents in the case they are not supplied. (network that is able to do both? )

	Alright, it seems that I can finagle the network to mostly do what I want.
	"Artisinal"
	I think the next priority is to add in auto-encoding feedback?

	Notes on circles2:
	PCA can extract most of the variance of this dumb simple dataset.
	The top-3 PCAs can predict w/
		r^2 of 0.938 for cx and cy, and 0.737 for radius.
	Bumping to top-10,
		r^2 of 0.967 for cx and cy, 0.770 for radius.
	(Corresponding weight matrix looks like a scale/rotation, good).
	** Suggests that a lot of structure might well be just reasonable PCA..
		which many dot-product architectures should be able to extract.
		(Pelehan et al.. )
			Hmm, this is good to know!
	Yet .. recreating the circles from those PCA vectors. Hmm.

July 7 2022
	Have a version of the closed-ish loop inverse-graphics version of circles started. 
	It doesn't work yet, as the expanded variable doesn't vary too much; 
	Think I need to whiten and center it. 
		(This seems to be a common feature of all sorts of statistical processing algos)
	OK, 'ex' is whitened, but ... still not clear that it's finding a good mapping from the expanded supervised cx,cy,r to the l2 variables. 
	Let's remove the 'gate' variable, which should make it more interpretable. 
	Yeah, now need to see if it actually inverts the data -- e.g. can do approximate forward graphics, with no error and limited feedback .. ?? 

July 11 2022
	With the gate variable removed, not clear that it's really finding a good address from 'ex' variable ... need to do a reconstruction?

July 13 2022
	OK, have the reconstruction implemented.. it's a useful feedback mechanism.  
	But I seem to have broken the key-query matching. 
	(... stream-of-work writing here ... )
	It all seems to be working again, though circles3 does not match the larger circles well. 
	Circles2 is back to matching segments of circles; this seems to be dependent on the *form* of computation of "gate".
	Here gate is just an outer product of activations between L1 and L2, decayed.
	(Hence is of Hebbian form). 
	
	Regarding what exactly the keys are aka how the space should be segmented, this is a fundamental problem!
		- Clearly, with circles3, we can do both forward and inverse graphics, with some (untuned) efficiency. 
		- Yet still making a LUT for 100 circles sure seems like cheating. 
		- Need something that factorizes the space, implicitly (in an unsupervised manner) first, then in a quasi-supervised manner later. 
		
	Outstanding problems: 
		- Need an encoding that compresses and simplifies the stimuli, while ignoring invariances / extracting equivariances. 
			So, early in the pipeline, we worry only about local statistics; as we ascend the hierarchy, worry about larger and larger receptive fields. 
				Currently don't have this.  Units need to have a topology, axons / dendrites spatial limits (certainly dendrites)
		- Said encoding might start out as one-hot but gradually entangle itself to be factorial (?)
			I like this intuition & it's consistent with 'aha' moments. 
			From the run:
			Distributed, parcelized search based on the fractal structure of the world is bound to be better than SGD (?), and should be able to find an efficient "address space"
		- The system ought to do inference in a iterative E-M manner, consistent with visual illusions and the timing of visual perception 
		- Slack variables: 
			Inference probably involves use of ephermenal variables; could be BTSP. 
			We don't really have any data, other than in the hippocampus, that BTSP is important,
			but from a logical perspective, it would seem so! Natural place to store the information. 
				The alternative is to store everythnig in vectors of activations. 
				Which, to be honest, is not *totally* unreasonable.
				See transformers, which work well with 1024-2048 dim latents. 
				(I also wonder if BTSP might be a bit of a distraction..)
		- Time:
			We don't have any elments of time or timing in the model.  The CNS seems to put quite a lot of energy into timing though.  It might be needed for stability, and it might be needed for causality or slow-feature analysis.  (All of the above?)
		- Feedback:
			Allocation of neurons & their keys ought to be subject to being able to predict the stimulus / lower layer.
			That is, if some part of the stimulus is unexplained, keys should move annd/or unactive neurons should take up that space.
			The previous series of experiments worked well with this, and used the 'explanining away' effect to some advantage. 
				Still, I think that the K-Q model 'feels better' than E-I feedback balance (though that too may be a part of the system, perhaps through the thalamus?)
				// Yeah well the overall architecture of the brain generally (exception: the basal ganglia) does not employ inhibitory long-distance projections.  
	
July 14 2022
	// Last night read an interesting thread on hacker news describing how perception is not a problem with Tesla's autopilot, it's commonsense reasoning.
	// The perceptual system can notice things like trash cans, stoplights etc, but it just never learns the local geography / does not have more than 15-30 seconds of memory for what the world is.
	// This is an important bit of insight: am I solving the wrong problem?

	Reading back over things. Might want to loop back to the overarching goals / remind why motor-informed disentangled representations are important.
	1 -- Boostrapped knowledge.  We want to be able to explore an unknown problem domain purely based on experimentation.
		Networks like EfficientZero RL algorithms might be getting pretty close (review that?)
			Right, three improvements:
			1. Vision model (project to latent space) + Action model trained to match next observation + state ala Siamese networks.  DUH
			2. LSTM to predict value based on series of states to prevent state-reward aliasing
			3. Off-policy reajustment when updating value prefix
				(this i understand less well)
		To properly make this work, need:
	2 -- Disentangled / factorized representations
		So that the agent 'knows' how to manipulate the world & does not suffer from the curse of dimensionality.
		** Need to critically inspect this assumption **
	3 -- Computational gain on the perceptual module
		This is a correlary to 2.  Factorization requires computation; in the current conception, this is done with some sort of address arithmetic which supports abstraction beyond just PCA.

	----

	To make the 'circles' set of algorithms work, I do think that these are needed
	1 -- Iteration / feedback
	2 -- Nonlinearity
		(otherwise, it's just PCA --
		of course the activation is highly nonlinear, but presently I suspect that we're still doing a degree of PCA..)
	3 -- Active allocation of K-Q units / elimination of redundant units

	4 (?) Random projection is a cool idea, but it's not exactly invertible.

	----
	Plugged in the mnisty data, and it appears that among many other things, we need to improve allocation (even more obvious now!!)
	and to add in slack variables to make reconstruction with just the supervised signal work.

	Thought that needs to be clarified:
	Without added interactions and added assumptions (temporal consistency, e.g.), everything will revert to PCA, or some other form of "conjunctive feature" analysis.
		All we have, at the end of the day, are the statistics in the data.
	Still, even given this, it should be possible to iteratively-anneal representation space.
		MNIST + circles should be a good simple testbed for this.

	----
	With two different values of NL2, 100, 500 and 768, we don't get a uniform representation of all the digits. 
	Instead, it all seems to be 1, 9, and a few 8's. 
	If we change the RandomAffine to 5 (from 40), with NL2 at 256, it looks better -- though 1 is definitely over-represented. 
	Adding translate, scale and shear deformations (so easy with PIL/torch!) makes the problem even worse! 
	I think that the way key matching is being done is wrong; we need a different statistical model, or a different form of the gate variable. 
	Switching back to the gate calulation scheme (decayed outer product) from circles2, doesn't fix the problem. 
	Get a bunch more 9's replacing the 1's, fwiw.. 
	System really should be segmenting the tiny visual scene into line segments... 
		Say, shouldn't there be some calculation on the address lines or something? 
		Even if it's not invertible, make something like a nonlinear expansion, cross-terms yadda yadda?? 
		Let's think more about this in the morning! 
	After, of course, fixing the model to get segmentation working as well as the nonlinear hebbian models
	(Could just replace the outer product K-Q update with a cube or something... 
		yeah that looks like crap.)

July 15 2022
	Some thoughts from last night:
	1 -- Transformers very much has activity-activity modulation:
		The key, query, and value are all generated at the same step with one continuous weight matrix.
			Github that illustrates this:
			https://github.com/openai/glide-text2im/blob/69b530740eb6cef69442d6180579ef5ba9ef063e/glide_text2im/xf.py
			Note: Gaussian ReLU, or GELU = swish
	2 -- The 'slack variables' or 'ephermental variables' that I was talking about can serve the same purpose: they can 'route information around' based on activity.
		Ideally, they have concrete meaning, eg location, pose etc.
		These should be inferrable through slow-feature analysis + active experimentation -- that's what we do!
	3 -- Such variables need to interact with the other address variables, probably in linear and non-linear ways, as PSPs do in dendrites (neverming NMDA spikes, calcium waves, up/down states... oy)
		Transformers mix things up via their input encoding and decoding matrices.

	Just read:
	https://www.newyorker.com/tech/annals-of-technology/the-pastry-ai-that-learned-to-fight-cancer
	Proves .. yet again .. that at the end of the day, you want to search for *algorithms* to describe & perceive the data (not just feature detectors)

	Need to get the statistics of the first layer working reasonably well with MNIST first.  Then later more complexity.
		I think we can let go of principles for a bit,
		Try out various heuristics.
	First of all:
		We are matching only on ones.
		Need to match more on *all* the digits!
		Removing the population mean did not improve much.
		then removing the gating also did not improve much.
		Need to have some sort of normalization that encourages ... forces total activation to be constant per key.

		-->> Why does it appear that many of the units have exactly the same keys?
		If a query matches one key, it should inhibit other keys. Balance.
		But not WTA: the units need to be decorrelated, something I've worked on a bunch.
		Explaning-away works well for this.
		Think about other feedback first? Hard to fit explaining-away into a multi-level hierarchy.
		What about just weight decay? or LTD?
			Hard because the E and I eelments of the K-Q calculation are not separated.
			In the brain obviously PV neurons are busily doing their own thing..
		Also, in terms of activations, why are '0' and '1' very active, but '9' shows up more??

July 18 2022
	More time passing.. well.
	Decided to just straight implement feedback control, since need to experiment and alignment of encodings to data is not obvious otherwise.

July 26 2022
	Effectively re-implemented Hebbian learning, though with key normalization (normalize by incoming activity), and it works ... okay.
	The cricles aren't perfect, and some of the weights need to be negative for it to work properly, and it's still just linear projection.
	But, as before, it does tend to vectorally auto-encode the circles.
	It does /not/ automatically form a good non-negative basis, as with cortex_mnist.
	And it takes longer to arrive at the basis -- cortex_mnist forms the bases almost immediately, without GPU acceleration.
	What is this missing, again?
	Weak nonlinear interactions?

	Problem!
	cortex_mnist does not work with circles stimuli (?!)
	But it does work fine with MNIST input.
	Why?
	Seems that l2e is too low; even though the forward weights are strongly positive, the reverse weights are even more positive; it;s become imbalanced.
	Turning off boosting drdamatically slowed convergence!
	But the circles system does not work with boost = 0.33.. ??
	Nowo seems ok.
	Still plenty of residual errors, but does find a reasonably good basis for the circles.
	Not quite as good as circles3.. ?

	Eh, anyway, I'm not addressing the real problem here ... addressing!

August 1 2022
	bumps.py
	The key averaging ideaa works perfectly well, progressively tiles the space with low-d functions.
	e.g. topology of space is learnable with temporal coorellations, and nothing else.
	Now need to add feedback / allocation & use that to update the keys.

August 2 2022
	Still not clear on the feedback allocation of addresses.
	Varaince normalization helped, though!
	& This network (bumps.py) can continually represent the data, despite the adresses changing & becoming more simple
	Also it appears that the network completely fails at the cricles task.
	Issue:
	-- keys forward and keys reverse default to large DC static values.
	-- Removing the random key seed & replacing it with a noise term does not markedly change things.
	-- Adding in mean subtraction, in both time and P, improves things slightly..
		But, the network no longer immediately encodes the stimulus, as before.
	-- Alas, the learning rule does not seem quite right.. forward keys arise, then fade- ?
		Stimulus is not amenable to this averaging!
		(If the pixel values are just the x,y coordinates, then the center of the circle is easy / immediate to fall out
		But the radius is not obvious, and cannot be calculated by averaging; it needs a distance calculation r = mean( sqrt( (x-cx)^2 + (y-cy)^2 ) where cx and cy are the mean / center of the circle.
		Of course any pair of 'on' pixels can be used to calculate the distance ..
		and also of course we want an algorithm that generalizes beyond just circles!
			Like squares and squiggles and such!
		Try going back to K-Q matching??
	-- The other aspect is that, without knowledge from other pixels,
		you a priori *don't know* whether that activation is part of any particular circle.
		+ One solution to this is to add in local cross terms to the forward key calculation.
		  (Have explored this idea before.. it's obvious)
		+ Another solution is to make both forward and reverse into K-Q maps.
			Will this work?
			Say we decompose the input into an ideal 3D representation.
			Each pixel now has a 3D address ...
			Determining if the pixel is "on" is then a distance calculation:
				(px - cx)^2 + (py - cy)^2 - (pr - cr)^2 ~= 0
					Where pr = 0 (pixels should have no knowledge of radius)
			Actual distance calculation:
				sqrt( (px - cx)^2 + (py - cy)^2 + (pr - cr)^2) < thresh
			Seems we'd need at least one extra term there -- a a per-axis 'gain', which would be allowed to go negative (!!)
			(Which we already have :)
			(And also go to zero ... how to calculate or update this??
				So:
			sqrt( gx*(px - cx)^2 + gy*(py - cy)^2 + gr*(pr - cr)^2 ) < thresh
				(thresh can be constant, absorbed into gain vector).
	-- Assuming that we *know* (e.g. supervised learing) what cx,cy,cr are
		Is it possible to infer the gains on each of those address lines?
			(In addition to the c* varaibles)
		Assume that it's a central pixel @ 0.5, 0.5
		physical space is [0..1]
		any circles where dist(cx, cy ; 0.5, 0.5) = cr will activate this pixel.
		aka dist(...) - 1.0*cr = 0
		example pattern:
		0.25, 0.50, 0.25
		0.75, 0.50, 0.25
		0.50, 0.25, 0.25
		0.50, 0.75, 0.25
		0.2, 0.2, 0.424
		0.8, 0.8, 0.424
		etc.
		again, invariance is that dist(cx, cy ; 0.5, 0.5) - cr = 0
		can we somehow gradient-descent into this?
		or otherwise detect this invariance -- which is of course a parametric circle!
			Yes .. detection of invariance!!
	-- The most obvious way of detecting invariance is to straight search for it.
		Randomly combine, perhaps heuristically, different operators on the data.
		Ala "Distilling free-form laws from natural data" 2009.
			This used the heuristic of derivatives:
			Conservation equations should be both conserved (duh) and
			predict the relations between derivatives of state variables over time.
				This is probably too big of an ask in s'okae,
				but that should not detract ffrom the search for invariances in other ways.
	-- Let's see if gradient descent can solve the problem.
		Why not be lazy.
		As usual, implementing it has revealed a lacunae of thinking:
		In the current architecture, each pixel has a key and gain.
		key -> should be 2-d location
		gain -> width of activation
		realistically, you don't need independent keys / gains for all of these.
		keys are basically axis-aligned linear ramps.
		gain is just three variables used by all keys
			which is equivalent to pre-multiplying those addresses / queries.
		Assuming that we are given the 2D linear ramp keys, can the gains be learned?

cirlces_grad.py:
	Yes, SGD can estimate the gains based on MSE from all the photos + supervised 3d coordinates ...
	But, if you seed it improperly, it doesn't work.
	eg. if the radius gain is positive, it does not find the 'good' solution of
	[ 1.5 1.5, -1.2] (ish)
	Soo.. in this case, seems you need to pick the sign.

	Recap:
	To fully analyticaly do forward-graphics from cx, cy, cr using the K-Q distance measurement framework,
	In one layer,
	You need to do per-address line gain, with one of those addresses negaitve.
		(e.g. subtract the radius)
	Then, *this* needs to be passed through another distance function -- or at least square prior exponential. ('match')
	Thus, really need two layers --
		which is of course exactly what circles3.py does!
			(if imperfectly)
			& it uses one-hot encoding to do this.. which seems slightly like a cheat?

	Invarainces, covariances, descriptors, how to allocate iteratively, how to allocate properly..
		the truth is still obscured.
	1 -- Patterns which co-occur  *can* help to describe a topology of the space.
	2 -- K-Q distance calculation *can* be used (in layers) to do forward graphics.
	3 -- K-Q distance calculation *can* also be used for inverse graphics,
			But atm, this seems only for one-hot representations.
	4 -- NL Hebbian learning is good at NNMF.
	5 -- Cross-product terms between pixels can readily compress MNIST to a low-d vectoral space (20 dimensions), but this compression does not yield any classification improvements relative to straight PCA.
		-- This is probably possible with the NL Hebbian work,
			But that is limited in that allocation & self-org is driven by feedback,
			which in turn doesn't work well when the forward or reverse transforms are significantly nonlinear.
		-- Notably, with K-Q, the forward and reverse transforms are very nonlinear!
	6 -- In the circles task, supervised activation is determined as a result of an invariance relation.
		-- A priori, it's not clear how to find these.
		-- But, one hope is that restricting the building blocks + allowing implicit search (matching), invariances will be easier to find.
	7 -- Intra-layer normalization schemes (lateral + homeostasis) are effective hacks for whitening / centering.

As I was telling LiAn last night,
	1 -- All patterns of retinal ganlion cell firing is not equally likely, despite the > 100:1 compression of the retina itself (at least for luminance)
	There are common patterns, on multiple scales, and these tend to be semi-algorithmically related to underlying factors.
	Such underlying factors of causality or variation are cognitively available, and lead to a compositional nature of the world.
	Such forms of compositionality have been seen in both StyleGAN and Transformer based text to image models (w/ pre-trained language models)
	Transformers, and to a lesser extent, StlyeGANS, have a computational structure which allows pointer-arithmetic like algorithm composition.
	Said pointer arithmetic is conditional MLP-layers, which are easy to train with SGD.
	Transformers also evince an implicit searching through Softmax attention (though other topologies also work).
	MLPs, in turn, can theoretically do the same functions -- the projection allows for invariance (can ignore non-projecting dimensions), but it takes potentially many layers to combine vectors to allow something as useful as 'pointer arithmetic' (e.g. distance function must be created with two opposite-signed nonlinearities in a lower layer)
	This motivates the creation of 'address spaces' as a fundamental operation in bio-infused artisinal neural networks.
	Addressing, of course, implies an ordering of the stimuli, which either needs to be inferred through spatial-temporal statistics (correlations), or though active experimentation.
	Addressing neatly solves the memory / function approximation dichotomy, too: you can use all your data efficiently by only updating the addresses, but keeping the contents relatively constant.
	This, of course, is complicated by the potential presence of hierarchical or layered address calculation, where intermediate layers need to change both their output and input keys.
	Still, there is a way.

	It may be commented that human appreciation of beauty -- which frequently reflects some degree of self-similarity, fractial or integer-dimensioned, seems to be reflective of some fundamental operation and or assumption of the brain.  It assumes some quasi-algorithmic compressability of sensory space, and when that does happen, it can be pleasureable.

August 3 2022
	Thinking while running:
	Probably need to decompose the problem into something simpler, particuarly given the math / logic above.
	What about lines, which is also 3D (inersect and angle / slope) but can also reasonable be parameterized as 4D (two points on line).
	How would the K-Q net deal with this?
	Even simpler parameterization: just angle. line always goes through the center.
	This is very easy to parameterize, pixels have an address based on angle (slope)
	Assuming key is 1D angle
	On = distance(angle - address) < threshold
	OK, now add 1 DOF: x-intersect.
	Add in an additional key variable, 'x', which is a linear ramp
	On = distance(angle - address, k_x - a_x) < threshold
		(a_x shifts the line horizontally)
	Thus it's easy to add an additional variable, y-intersect:
	On = distance(angle-address, k_x - a_x, k_y - a_y) < threshold
		I *think* this should work, but better make sure...
	No no, the angle argument needs to be modulated, or the angle look-up needs to be translated.
	Modulation: (hmm ... please go to the notebook!)


August 5 2022
	A. Coates and A. Y. Ng, “Learning Feature Representations with K-Means,” in Neural Networks: Tricks of the Trade, vol. 7700, G. Montavon, G. B. Orr, and K.-R. Müller, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2012, pp. 561–580. doi: 10.1007/978-3-642-35289-8_30.

	Uses power-correlation between keys to organize / group K-means filters into smaller-dimensional groups for subsequent K-means (e.g. hierarchical!)
	Interesting / smart.

	Also just discovered UMAP!  Beautiful explanation at
	https://umap-learn.readthedocs.io/en/latest/how_umap_works.html
-------
	Had lunch with Jascha, some salient feedback:
	1 -- Do you really need RBF representation?
		Divisive normalization might well be good enough.
		DN constrains the vectors to be on the surface of a sphere (if you divide by the l2 norm), or some other geometric form with a different norm.
		This is sorta less interesting in high-d spaces, where most of the mass is at the periphery anyway .. ???
	2 -- He thinks that the current transformer architecture can't possibly be perfect -- how did the original authors back in 2017 get many of the details correct?
		I think that there very well maybe an engineering plateau (which is what I said), but more relevantly, if transformers enable algorithms like pointer arithmetic, then while there may be better ways of training them, there really aren't more fundamental ways of representing pointer (or vector) arithmetic.
		Or, perhaps, doing matching other than cosine distance.
	3 -- JSD thinks that there is much opportunity for networks that learn to ignore or take advantage of invariances.
		This, of course, can decrease the representation dimension, which makes learning easier (perhaps consistent with his theoretical work?  I should read it..)
		Says that current networks can have one or two transition points corresponding to dimensionality reduction, but there is much room for more, particuarly in the algorithm design.
		This relates intimately to above, and ideed to what I've been fiddling with.
	4 -- Some discussion of ethics of AI research, what human jobs will be automated first, and moralistic ambiguity of working it (you are accelerating it's arrival, but also have some leverage over its use?)
		This segued to the current polarization problems, and how imperfect algorithms are already really good at gaming human minds.
		His point that patches can be applied to software, but not to humans (!) ... we're really going to need some collective action here / laws to regulate advertizing and mind-manipulation.
		Case in point: even mediocre algorithms are quite good at propagating misinformation and deeply messing with society.

	Re 1 & 2: True, softmax is divisive normalization, where alignment is a dot-product, not distance measurement.
	(see link July 15 2022)
	 weight = th.einsum(
            "bthc,bshc->bhts", q * scale, k * scale
        )
	here q and k are activity vectors from the lower MLP.
	This is probably better for gradient-flow; the normalization distributes the gradient to other members of the same vector.
	I've been thinking mostly about l2 distance while trying to do forward graphics on the circles and lines etc.
	Absolutely worth trying to do the same with cosine distance or divisevely normalized dot-product (as in softmax).

	Would love to have some intuition on what exactly it's doing with space; perhaps my addressing ideas are wrong?
		Well, in computer RAM, perfectly entangled addressing can be achieved not only by distance calculation (which has the unfortunate effect of sign change of the derivative, hence optimization is less straghtforward)
		but also by dot-product & threshold, if you include the complement on the address bits (as with the XOR task, way back when), and set the threshold to match on *all* bits.
		Provided the complement is perfect, this will work
			if it's not, then can cheat by setting everything to 1 and everything is selected.
		Anyway, if the input is not normalized, then it does succumb to attack -- dot-product if the vector is eg [5 0 0 0 0] selects 16 things .. not one of 32.
			(distance : would select nothing)
		But if its normalized, then yes basically have a map which is the surface of a N-sphere, rather than a N-dimensional cartesian space, and cosine distance is probably just as good as cartesian distance.
		So, equivalence?
	I think there is probably something deeper here.
	UMAP e.g. is already decent at classification, and that uses a very simple euclidean distance in high-d (k nearest neighbors) to map down to low-dimensions (using a binary cross-entropy loss).
	In supervised learning / classification, we know the approximate distances, at least the distances for same-class objects (low), which is non-euclidean; you then tune the parameters of the network to minimize this 'distance'.
		Distance is another way of representing multi-dimensional ordinality,
		and SGD is a way of converting between the two,
		Where location is represented in high-dimensional, possibly hierarchical activation space.
		Yet if we have actual location / ordinality, should use that.

	Why not just have some sort of datastructure that represents the distances / ordinality directly??
++ Use distances
	(including various internal metrics, such as L2 norm, linear correlation, power-law correlation (L4?), explain-away feedback)
	when self-organization is required.
++ Use supervised information
	(eye movement, head movement, any other form of ineraction - manipulation)
	when possible, + assumption of slowly-changing world
		Yes!

	A current hypothesis is that the brain uses sequential one-hot and vector representations; for example, saccades can move an object between hemi-fields, which at the level of V1 will completely change which subsets of neurons are active (kinda like Laplacian pyramids / mipmaps / common graphics pipelines..)

	With L2 addressing & variable width (might be a better way - ed.), then selecting different subsets of V1 neurons based on location is trivial.
	Extracting that information is a bit harder; might need some sort of iterative voting scheme?
	At each level you need to infer some address bits (vectors)
	With cosine / softmax addressing, instead of moving linearly through the space (e.g. position is encoded in rate (mgiht be hard for neurons))
	You instead can rotate through the space.
	But one rotation is still really just parameterized by two vectors: x and y, which are orthogonal (easy, most are) and concevably -x (? maybe not)
	Or could be two sets of vectors X and Y ... Probably this doesn't matter so much, its still a 1D change, what really matters is propagating this to lower levels.
	So, if now we have two hemi-fields, the next vectoral representation needs to convert the full sweep of that vecor (in either scheme) into either nothing (not in hemi-field) or a linear ramp, e.g multiply by 2, add offset, threshold.
	And so on, down the hierarchy.
	Obviously this is not so hard to implement with local or distributed normalization, but how ever could this be learned?

	Distance and keys seems to make this simpler, because you just tile any space as necessary, and learning is obviously hebbian / local.
	But, hierarchy!
	Need to go back to the circles / squares test / toy problems.  Must be a way to adjust keys and queries to auto-organize.

August 8 2022
	(edited above as well)
	To really think about it, the visuo-sensory world is not just 2+D (~2M axons from the eyes), but dependencies extend in time as well, which you can imagine as sampling the world ~60 times / second.
	The task of a memory is to store as much of this information as reasonable (throwing away ecologically appropriate details), and make it as accessible (indexed and auto-encoding) as possible.

	Seems to require a degree of local-greedy algorithms + iterative, up-down reorganization... there necessarily will be multiple heuristics.

----
	Getting back to the circles task, job now is to organize those feature vectors so they can be indexed by local small translations, local orientation changes.
	Maybe an down-up-down algorithm would work here?
	E.g. use the alignment of forward and backward keys to make the maps place similar activations physically close to each other,
	And also address-wise close to each other
		But this is merely a consequence of forward-backward symmetry?
	Bottom-up: integrate information to both disentangle and form coherent whole objects.
	Top-down: organize tuning & responses to make the aggregate feature detection - selection work.
	Also, probably, perform perceptual fill-in to the correct and auto-adjusted level.  (no hallucinations, visual ('optical') illusions OK)
		"Push things back onto the manifold" .. but this has to be coupled with learning the manifold itself!
			I wonder if you lived in a world where the assumptions that visual illusions violate were absent ... would they still be illusions?
		How can a top-down signal work for both organization and fill-in?
	Anyway, currently the allocation of units in all the networks is essentially random, which is unphysiological.
		Even higher in cortex I assume there is a local topology?

	Experiments with UMAP
	conclusion: UMAP can recover the latent 3D structure directly, reasonably robustly, and quickly. (with default parameters!)
	you do not need any hierarchy or whatnot,
	but you do need more samples than the number of dimensions.
	e.g. 2000 works, but 500 does not)
	from the map it is somewhat trivial to invert (map supervised signal to embedded dimensions, do interpolation of the examples in embedded dim)
	but not totally clear that this is an 'algorithm' per se.
	instead, it's a detailed bidirectional map, which *could* concevably be used to make an 'algorithm'
	better would be to use the umap algorithm to discover the compression / decompression algorithm (??)
	make the map then compress it.
		Aside: need to look up how semi-supervised umap works.

August 10 2022
Hi Jascha,


    The nice thing about DP in a normalized space is also that every input will be represented by some set of active features. For RBF kernels in a high dimensional space, most units will be off for most inputs. It's hard to tile a high dimensional space with Gaussian bumps ... you need an exponentially large (in the space's dimensionality) number of Gaussian bumps to do it. (another option would be to use RBF activations, but with something like divisive normalization, so that a few are active for any input. This seems likely more brittle though.)


Yes, that makes good sense, thanks!

I'm trying to reconcile two ideas:

One is that algorithms like umap and t-sne are sample efficient and can (sometimes) learn useful compressions/indexes on the data.  They work on distances, like RBF, and are invertible.  These mappings offer implicit allocation of nonlinearities, I think.

The second is that normalized activations are much more amenable to high-d spaces, like you say, and they can be readily stacked into a hierarchy.
This hierarchy is necessary because each layer offers only one explicit hyperplane nonlinearity per output dim.

Can you somehow combine both?

1: For example, t-sne mappings of the digit '7' organizes into a 2D map of attributes like angle, length of legs, and presence or absence of the continental cross.  These are variances with meaning, with kinda complicated forward nonlinearities -- and very complicated inverse nonlinearities (via manifold approximation... it becomes like a lookup table)

Right, a deep MLP aims to approximate a function via a series of summed, rotated, scaled hinges / nonlinearities (example: fitting a bump, see test_sinc_mlp.py).
	And it sometimes has problems with this, since it can't really multiply terms.
T-sne and umap aim to approximate the manifold of the data directly, with the primary constraint that it needs to map down to a handful of dimensions (usually 2 or 3..)
	Speaking of which: this is exactly what James DiCarlo was talking about!
		J. J. DiCarlo, D. Zoccolan, and N. C. Rust, “How Does the Brain Solve Visual Object Recognition?,” Neuron, vol. 73, no. 3, pp. 415–434, Feb. 2012, doi: 10.1016/j.neuron.2012.01.010.

In the test_sinc_mlp example, there isn't even much rotation, actually!  Nor do weights frequently (if ever?) cross zero.
Furthermore, the way that it approximates the function is sorta non-obvious: take a positive slope (with bias, i assume) and subtract off negative hinges.
Again, I say: "Just barely enough of a nonlinearity"

A RBF approximates the same sinc function using .. one central bump or 'key'.

2. A second big disadvantage of RBF or UMAP is that the resulting transform is much, much slower than if you implement it as a clean fast MLP or even transformer.
This is because I think you need to do a bunch of comparisons with the existing data (??)  Should look ups.
	Yes, umap relies on PyKNNdescent library for finding the N-nearest neighbors of a datdapoint.
	I think for transform, though, it does not use parallel threads (?)
	PyNNdescent is multithreaded.

Here is an example run of circles_umap.py:
umap fit 28 64000 (200704, 25)
UMAP(n_components=3, verbose=True)
Thu Aug 11 12:22:48 2022 Construct fuzzy simplicial set
Thu Aug 11 12:22:48 2022 Finding Nearest Neighbors
Thu Aug 11 12:22:48 2022 Building RP forest with 18 trees
Thu Aug 11 12:22:48 2022 NN descent for 16 iterations
         1  /  16
         2  /  16
         3  /  16
         4  /  16
        Stopping threshold met -- exiting after 4 iterations
Thu Aug 11 12:22:56 2022 Finished Nearest Neighbor Search
Thu Aug 11 12:22:56 2022 Construct embedding
Epochs completed: 100%| ███████████████████████████████████ 200/200 [00:26]
Thu Aug 11 12:23:32 2022 Finished embedding
umap transform
Thu Aug 11 12:23:33 2022 Worst tree score: 0.13764062
Thu Aug 11 12:23:33 2022 Mean tree score: 0.28430208
Thu Aug 11 12:23:33 2022 Best tree score: 0.85714062
Thu Aug 11 12:23:35 2022 Forward diversification reduced edges from 960000 to 714586
Thu Aug 11 12:23:37 2022 Reverse diversification reduced edges from 714586 to 714586
Thu Aug 11 12:23:39 2022 Degree pruning reduced edges from 799768 to 799762
Thu Aug 11 12:23:39 2022 Resorting data and graph based on tree order
Thu Aug 11 12:23:39 2022 Building and compiling search function
Epochs completed: 100%| █████████████████████████████████████ 30/30 [00:09]
Thu Aug 11 12:25:19 2022

Yes so ... "Building and compiling the search function" is an expensive operation (the most expensive?)
	It is not multithreaded.
	And is needed for "transform" function.
	Seems to take about a minute and a half.
	It's in /home/tlh24/.local/lib/python3.10/site-packages/pynndescent/pynndescent_.py", line 1635, in query
Next up is building an embedding, 26 sec.

So NNdescent does have GPU implementations, but they might not be as efficient as bruteforce for high-dimensional data, see
https://docs.dgl.ai/en/0.8.x/api/python/knn_benchmark.html
bruteforce = compute all N(N-1)/2 distances, sort and select the k smallest per row..
It also seems that KNN starts to fail when data dimensionality gets above ~ 50 ... weird, because umap (which uses NNdescent) works fine on MNIST and semi OK on FashionMNIST, and these are 784-dim.. perhaps the paper is too pessimistic on recall, or perhaps recall is not that important to umap?
Seems that umap uses a RP forest for NN descent.
... which is kinda funny as RP forests have much lower recall than pynndescent.  Guess it's useful as a first pass at least.
	ref: https://github.com/erikbern/ann-benchmarks
+ RP forest:
	It works by building a forest of N binary random projection trees.

	In each tree, the set of training points is recursively partitioned into smaller and smaller subsets until a leaf node of at most M points is reached. Each parition is based on the cosine of the angle the points make with a randomly drawn hyperplane: points whose angle is smaller than the median angle fall in the left partition, and the remaining points fall in the right partition.

	The resulting tree has predictable leaf size (no larger than M) and is approximately balanced because of median splits, leading to consistent tree traversal times.

	Querying the model is accomplished by traversing each tree to the query point's leaf node to retrieve ANN candidates from that tree, then merging them and sorting by distance to the query point.

		Note though: pyNNdescent uses built-in rp_trees.py, not an external library.

Well still .. I really like the concept of the NNdescent algorithm.  Start with a random collection of k nearest neighbors, then join over their neirest neighbors, add in reverse neighbors, and select from this larger set neighbors that are closer.
The problem with any NN metric is that proximity becomes less and less meaningful with larger dimensions.
	MNIST probably has locally low dimensionality, which is why it works..

Experiment: Adding transformations to MNIST unsurprisingly makes clustering via UMAP harder.
See screenshots/20220812_umap_mnist_native.png and screenshots/20220812_umap_mnist_transformations.png
	End-goal would be to have something that can approximately extract transformations..

Tested umap on cortex_andor5.py, which does nonlinear compression of the vector space.  It does worse that untransformed data (the digits are smushed into each other), even though one-hot prediction was better than PCA prediction (and presumably PCA is just as clusterable, if not better, by umap).
	Well, this system was not per se engineered to represent invariances;
	instead, it was engineered to represent bilinear probabilities (pairs of pixels that are on at the same time..)
	The pairs of pixels seem like a decent compression, but apparently do not provide processing gain to improve digit recognition.

August 12 2022

Likewise, translation, skew, scale & whatnot do not hinder human perception noticeably, but they strongly effect umap and t-sne.
	Interestingly, I think that we do a version of internal mental rotation (latent variable inference?) when perceiving characters that are highly skewed.
	Much more common, stereotyped letters are basically instantly recognized.
		("amorized inference"... but how to add a de-translate module?
		I'm assuming that it involves a degree of content-agnostic addressing, still)

Read over
H. Tang and K. Ellis, “From Perception to Programs: Regularize, Overparameterize, and Amortize.” arXiv, Jun. 13, 2022. Accessed: Aug. 12, 2022. [Online]. Available: http://arxiv.org/abs/2206.05922
And i think the general scheme, of using reparameterizing, overparameterizing, then regularizing makes sense!
To unpack:
	Make much longer programs than necessary.
	Parameterize them variationally using the gumbell-softmax trick
	Regularize after 10 epochs to encourage parsimony.
However, I think amortizing this inference via MLPs is ... no good. Need something more powerful, at least.

Also notable is https://dselsam.github.io/the-terpret-problem/
"The terpret problem" -- shows how, fundamentally, SGD is liable to getting stuck in local minima.  With a toy problem.
	What if you break ties better??  Might be better solved by changing the representation / substrate, rather than SGD.

Did end up sending an email to Jascha.  His email + draft is above; for completeness, this was sent:
	I'm trying to reconcile two ideas:

	One is that algorithms like umap and t-sne are sample efficient and can (sometimes) learn useful compressions/indexes on the data.  They work on distances, like RBF.  These mappings offer implicit allocation of nonlinearities but are slow to interpolate and slow to invert. I think.

	The second is that normalized activations are much more amenable to high-d spaces, like you say, and they can be readily stacked into a hierarchy.
	This hierarchy is necessary because each layer offers only one explicit hyperplane nonlinearity per output dim.

	Wondering if it's possible to have the advantages of both...

I do think there is some meat there!!!
Transformers have an advantage in that each layer + head has explicit search!
& each of the K, Q, V have both activity and bias components (hence allowing static keys, as needed..)
Wondering outloud, maybe a good approach is to re-engineer transformers to work better or more scalably with image input data.  Since they are so good.
	Use the same tricks as above.

Other thought that I've been kicking around, since I've been digging into UMAP
	(which utilizes pyNNdescent for making a K-NN graph of the data
		(which in turn  based on RP trees)),
is that perhaps adding appropriate levels of algorithmic complexity (e.g. search, fast tree-based datastructures, iterative organization of maps, iterative regularization / program-extraction) might be the right way to improve representation-learning.
Really really keen on iterative regularization!  But for that, you absolutely need a substrate that permits the expression of algorithms.
Which entails, at minimum:
1. Constants (e.g. weights)
2. Arithmetic (+,-,*,/)
		clamp or relu
		maybe log, exp, sin?
3. Variables
		In a MLP, the variables are always bound to a channel = activity.
		In a transformer the variables are implicit / unnamed & exist in the latent space.  Which is a fixed dimension but so what.
			Amazing that SGD can pull useful structure out, given that it doesn't commonly change sign in MLPs...
				And probably never does with ReLus, let's try leaky ReLu.
			(Suggestive that it's just finding existing workable structure from the random initialization)
		In a hypothetical K-Q network, again the variables aren't named; instead, they are addressed. Which leads to..
4. Computed addressing
		aka pointers.
		If you can address variables, you can variably bind them, do scatter / gather, etc
		Maps -- and especially maps with vector representation, so that you can do vector math ( = pointer arithmetic) seem like a good substrate for this)
5. Conditional execution / If-then-else
		This can be accomplished via nonlinearity and then computed addressing, probably as happens in Transformers.
6. Loops?
		Can be constructed via recursion and conditional execution, which is elegant but ...
		These are used in programming languages for reductions (sum), which is a built-in in all neural networks.
		They are also used for projections, another built-in.
		Projections can in turn be used for expansions and outer products.  All built-in.
7. Search?
8. Nice datastructures, like trees, maps, dictionaries etc?
		These are of course both built in to high-level languages such as Python.
		And took a good bit for humans to invent them.
			There are undoubtably more datastructures to be invented
		Seems useful to have them available.
		See also: https://news.ycombinator.com/item?id=32186203
		Union-find?

Seems like the sort of problems that we need to solve, namely make an auto-associative, self-organizing, invariance-finding, compositional-disentangling memory or artificial cortex
	Might be solvable with the right (set of) datastructures?

Most of the datastructures / algorithms in that link above approximate space fractionally, e.g. with a tree.  Sometimes stochastically, sometimes using the curse of dimensionality (eg hash tables), always with some end-use in mind.
Union trees are nice in that they amortize parent finding during the search itself!

The real visual world is of course much more entangled than any of these datastructures can support; they are all more-or-less bijective, whereas in pixel space, everything is super highly multijective.
One initial concept would be to have built-in 'transforms', and have a system that can estimate the parameters to those transforms based on search and later shortcut connections (and if these fail, some sort of complexifying map).
E.g. a map which transforms, controlled by a map that iteratively complexifies.
For this to work, I think, we do need 'computed addressing' ... problem is, that RBF type stuff does not work that well in high-D
	Solution would be to
	-- use dot-products,
	-- stick with lower dimentsionality,
	-- do smarter normalization as is likely happening in the cortex.
Looks sorta like capsule networks
	What ever happened to these?  Why have they not taken off?
	Looks like people implemented them a while ago, and the repos haven't been touched / updated.
	Ah, seems as from here:
	https://analyticsindiamag.com/what-happened-with-capsule-neural-networks/
	that transformers offer a less clunky and more generalized routing algorithm.

August 16 2022

From https://www.nature.com/articles/d41586-022-00018-5
	Janelia separates its large-scale projects, which are professionally staffed and managed, from its academic research projects. The latter are staffed by students and postdocs in small labs led by principal investigators. But such institutes are few and expensive, and are difficult to create. Furthermore, their nimbleness is hard to sustain. Janelia’s first director, Gerald Rubin, wrote in 2019 that “without an opposing force provided by management, there is a slow, steady drift toward a more conventional environment increasingly focused on maintaining successful programs and documenting individual achievement at the expense of risk-taking and collaborative, interdisciplinary work”4.

August 17 2022

Changing direction: rather than trying to figure out a new network architecture / new datastructure / new way of training both,
I'm going to merge these three:
+ Kevin Ellis, DreamCoder
+ d'Ascoli, Deep symbolic regression
+ Ye, Efficient Zero

Namely, to change the MLP in DreamCoder to a Transformer,
	then incorporate RL to guide the program synthesis search.

Todo: make more sense of the program, probably by tracing it out via PDB.
What are the grammar neural nets?  How do they work?  What data is passed between the different modules?
	See: https://github.com/ellisk42/ec/blob/master/docs/software-architecture.md
	from: https://github.com/ellisk42/ec

August 18 2022

Ben S: Nextflow seems really cool!  Works well for the methylseq information.
The reads are 90 million characters long --hence need to run the analyses massively parallelized.
Use AWS batch for this, via the Netxflow / Nextflow tower / Docker image with the requisite software.  Smart!

August 19 2022

Ok, so I'm a big dummy, and removed the Jane Street capital 'core' library and replaced it with the standard library .. .duh.
Turns out the type signatures for the two are quite different, and at this point probably best to stick with the presumably mature JS lib!
Well, at least I have a better understanding of the code -- and have refreshed my Ocaml knonwledge.


August 23 2022

Running into ocaml configuration issues recompiling DreamCoder ... very annoying.
trying a fresh install of ocaml via opam.
run
eval $(opam env --switch=4.14.0)
To select the non-debian-supplied version.
to update an environment after installing things:
eval $(opam env)

September 4 2022
Tuxedo NY
Have the dreamcoder solver directory fully compiling,
and the list / text tasks seem to be running fine --
but the logo task is not working.  It bombs while reading in a huge huge block of json -- something too large for kate to work with.  Way too many parentheses!

Been reading a lot of program synthesis literature; dreamcoder is good (need to revisit the E-graph or equivalence-graph enumeration ideas + the version space algebra (which is?) plus the fragment grammars
	All these seem like essential and clever means of managing the exponential search space of programs.
	Which otherwise is insurmountable.
My instinct is that we need an datastructure + algorithm that is some intermediate between symbolic logic (discrete), programmimng languages (discrete / terse, good generalization) and super easy to fit (liner regression / MLPs etc.
Transformers of course exist in this intermediate space; they can evince algorithms, but can be trained via gradient descent.
Program synthesis of course uses much much fewer examples, compresses greatly, and there are efficient means of search (SAT solvers), but does not have the fluidity and robustness that comes from opearting on features in high-dimensional spaces.
	Of course, part of the effectiveness is synthesized programs have relatively few parameters;
	The complexity is in their *structure*
		Again this is exponential to search.

I keep wondering if there is a way to have a continuous relaxation of "maps" to "algorithms" to "linear classification and projection"
	(to try to tie in some of my older work on unsupervised / compressional learing..)
	Are there ways to hierarchically tag and label datapoints?
	Turn a map or u-map into an algorithm?
	Make functions that operate on high-d spaces?
		Then again, not perfectly clear how essential recurrence is for important things like vision.. ?

Anyway, a more important proximal thing to do is to merge DreamCoder with transformers!!
See https://veo.io/index.pl?pid=1571
Need a demo!!
Then also need to pull in Github codes, ala Copilot.
	And think about using transformers on other axes of the task..

September 10 2022
	Back at it -- logo task is not working, need to debug it.

Note:
– General to specific learning. Start from all true and refine
with the maximal (consistent) generalization.
– Specific to general learning. Start from all false and refine
with the most restrictive specialization.
– Version space learning. Keep all consistent hypothesis
around – the combination of the above two cases.

"Fringe" used in DreamCoder refers to the version space algorithm.
(reference is in Dropbox and Zotero, search for "CS 2750". 

September 13 2022
Ok, so the logo task is working!  
At least, it's stably producing drawings ..
Does not seem to be evolving in any respectable way toward a functional library.  That is, the drawings are getting no more complex, and it's solving none of the 160-odd tasks. 
Still, I'm using the ocaml compressor, and not the pypy compressor as orginally applied. 
Also, when I tried the rust compressor, it errored out -- perhaps there is something wrong with the ocaml compressor / library generator?? 
Suppose I should re-try this with pypy, again ~overnight. 
	(Wonder why they need to use the huge memory instances on google cloud -- maybe the pypy compressor uses a ton of memory? )
	(Also, maybe I should just wait longer?  It's been 10 epochs out of 20, so about consistent with the reported wall time (one day with 64 cpus), but a resonable expectation would be that in this time, it will make progress toward the goal: at least some of the tasks (e.g. drawing squares) will be completed, rather than bopping around with curves and such. 
	Hmm.  Will stop it after swimming.  
	Might be noted that the ocaml runtime is probably not that efficient..


September 15 2022
Ah!  Where does the time go?
Seems like my blocks of time keep getting broken up, and that work is always the last prority..
Anyway.  What I've been meaning to write for a while:
Why not *learn* both the equivalence graphs
	with, e.g. memorization of common semantic structure / dataflow graphs?
		This is more similar to the way that humans do it.  Looking for analogies / loose matches in the dataflow graph, and backing that to semantics and associated syntax.
	or, with a map from the details of the language under test (e.g. python)
	to an underlying e-graph datastructure, which can then be factorized.
	The factorized representation can subsequently be converted back to the written programming language.

Need a datastructure for mapping between domains (syntax, semantics, dataflow) and performing approximate key-query match (with segmentation) on the database.
	Said datastructure needs some degree of compression, ala E-graph (egg library) (redundancy reduction, very smart) or hash consing (with again the segmentation problem.)
		The PAC learning paradigm (upper fringe, lower fringe) offers a good means of learning equivalences based on experimentation.
	Ideally only want ~1 fundamental datastructure, approximately independent of language (modulo functional languages?), and most representative of underlying best-practice computation...
		Then can have multiple maps to and from it.
			I think that this is sorta what transformers are doing internally.

This datastructure or algorithm needs to allow for *abstraction* aka library (or motor primitive learning), as shown powerfully in DreamCoder.
You can only conquer the curse of infinite dimensionality by hierarchically breaking up the problem.

Sept 16 2022

Serious problem with DreamCoder: the dreams are insane!! Some of them are 100kb long!  There is basically no way in hell to learn input-output relations from such complicated and cleary chaotic behavior. 
A human would never go about things in this way.. 
I wonder if I've messed something up.  Well, I can fix it. 
	Right: doesn't the paper say that programs are enumerated, rather than sampled stochastically? 
	Seems like many of them are total run-ons, huge chunks of meaningless code or code that does not affect the output & should be culled. 
	Also it's clear that the syntax of the logo tasks are very loose, e.g. "FWRT" (forward, right turn, which should take a length and an angle) has all sorts of extra arguments, which may or may not be executed??
Idk, this is a bit of insanity, the search should definitely be tree based and breadth-first, with some sort of culling based on equivalence.  
Also, it takes much too long to execute the tasks.  
I will instrument this. 

Sept 19 2022

Trying to understand the implementation of lambda calculus. 
slanted square task. 
Input to parser: 
(
	(move 0d (/a 1a 3))
	(loop k 4 
		(move 1d (/a 1a 4))
	)
)
pretty smiple.  Rotate, then draw a square. 
move takes distance and angle arguments; aka logo_FWRT
Output of parser: 
(lambda 
	(logo_FWRT logo_ZL (logo_DIVA logo_UA 3) 
	(logo_forLoop 4 
		(lambda (lambda 
			(logo_FWRT logo_UL (logo_DIVA logo_UA 4) $0)
		)) $0
	)
	)
)
So, where does that $0 come from, and what does it mean? 
Is it a continuation or something? 
ANd why do we need two lambda's there? 

Try again. 
Input: 
(
	(move 0d (/a 1a 6))
	(loop i 7 
		(move (*l 1l i) (/a 1a 4))
	)
)
Output: 
(lambda 
	(logo_FWRT logo_ZL (logo_DIVA logo_UA 6) 
	(logo_forLoop 7 
		(lambda (lambda 
			(logo_FWRT (logo_MULL logo_UL $1) (logo_DIVA logo_UA 4) $0)
		)) 
	$0
)))
Right, the $0 must be part of the syntax .. it's the de Brujin index of the lambda expression or environment that is implicitly returned. 
Likewise, $1 is the index of the loop counter. 

Now, let's investigate the Grammar generator. 
Why does it produce runon labda expressions of zero meaning?? 
Seems that it's initialized to 'uniform' -- which is not (at all) the way that exploration should be proceeding. 
	(Think I will try a little bit longer to reform DreamCoder before switching to a reimplemenation... 
	Clearly there is a lot of cruft here that can be removed or refactored!)
Looks like ocaml 'solver' program never returns any functioning programs. 
hum. 
Also seems like it might not have been a great architectural choice to split between python and ocaml?  Forces repetition. Makes debugging harder.  Discourages use of pytorch. 
Alright, let's start from scratch, sigh. 
	(Of course need to take as much inspiration from DreamCoder as possible)

Sept 20 2022

I think I'll need to re-implement the interpreter ... the (typed) lambda calculus is too low-level and clunky. 
I deally we'd just use ocaml directly, but making an interpreter gives far more control over error messages and decompressing the execution graph etc. 
Which, I think, is essential. 
Should be pretty easy for just Logo tasks.  
	MAy eventually head back to the DreamCoder implemenation, but for now this will be fun! 
	Realized that last night: snice I'm not really being paid for this, it damn well better be fun! 

Sept 28 2022

Tim Hanson <sideskate@gmail.com>	Wed, Sep 28, 2022 at 11:44 AM
To: Timothy Gardner <timgardner314@gmail.com>

https://www.zdnet.com/article/metas-ai-guru-lecun-most-of-todays-ai-approaches-will-never-lead-to-true-intelligence/

    He has abandoned his prior faith in using generative networks in things such as predicting the next frame in a video. "It has been a complete failure," he says.


I think this is a bit extreme -- clearly we humans are doing some degree of predicting what's coming next in our sensory experience. (e.g. expectation mismatch, P300 etc)

    And the reason it's a complete failure is, again, because of the constraints of probabilistic models where it's relatively easy to predict discrete tokens like words because we can compute the probability distribution over all words in the dictionary. That's easy. But if we ask the system to produce the probability distribution over all possible video frames, we have no idea how to parameterize it, or we have some idea how to parameterize it, but we don't know how to normalize it. It hits an intractable mathematical problem that we don't know how to solve.

Sure seems like diffusion models offer a good solution !

    LeCun decries those he calls the "religious probabilists," who "think probability theory is the only framework that you can use to explain machine learning."

Agree.

    JEPA architecture actually tries to find a tradeoff, a compromise, between extracting representations that are maximally informative about the inputs but also predictable from each other with some level of accuracy or reliability

This is a great insight.

    much of the deep learning community seems fine going ahead with something that doesn't have common sense

Which is fine -- there are plenty of humans, and they generally have common sense.  Our moral imperative is to make systems that do things we can't ... right? (like AlphaFold).  We only follow human intelligence because it's one of the few working examples that we have.

    Those signals, they're called conjugate variables in Lagrangian mechanics, but in fact, they are gradients.

Very interesting, I hadn't realized it.  Many forms of optimization are mathematically equivalent ...
He makes a good point about online optimization of latent variables being an inference problem, and we're definitely doing something like that in our visual system.
Still, 'optimization' is isomorphic to having a cardinality of solutions (I'll argue), which is where you get the connection with data-structures and ordering.  Ordering offers a different intuition for guiding thinking.
The great benefit of ordering is that it's inherently multi-dimensional, vs. energy, which is unidimensional afaik.  For example, with a database of motor-action primitives, you can order sequences of activations over joint angles, then over observed endpoints, then even higher things like target of a thrown object; each ordering is non-exclusive and can be combined.

My working hypothesis of what the cortex is doing is something like an ordered map or dictionary, where part of the ordering is supervised from motor variables, and part is purely invented based on combinations of observed variables.  This is the same as the (distributed) latent variables that LeCun talks about.

I've been learning the game 2048 (it was on my computer during a long flight) and have been instrumenting myself as I get better.  It seems that something like a map of (outcome + action) indexed by (decomposed input configuration) is a key component.

    Borrowing language from statistical physics,  energy-based models posit that the energy between two variables rises if they're incompatible and falls the more they are in accord. This can remove the complexity that arises in "normalizing" a probability distribution.

Aha.

Have to think more about this. And check
https://www.zdnet.com/article/metas-ai-luminary-lecun-explores-deep-learnings-energy-frontier/

On Tue, Sep 27, 2022 at 5:26 PM Timothy Gardner <timgardner314@gmail.com> wrote:

    https://www.zdnet.com/article/metas-ai-guru-lecun-most-of-todays-ai-approaches-will-never-lead-to-true-intelligence/

-------

So!  Now have the parser -> png system working fine, apparently. 
Time for the convnet + transformer. 
So ... python.
Want this to be entirely bootstrapping: start with naive grammar generation, learn input-output mapping... 
Maybe it makes sense to think about this first in terms of databases, rather than transformers.. ? 
	Transformers might not have the sample efficiency for bootstrapping.. eh, still worth trying.  
		Scale might be the solution.

Question: Representing a program as a string is pretty inefficent and redundant. 
Far better to represent as a AST or so. 
Q is .. how to linearize the AST?  Transformers expect vectors of tokens, not trees of tokens.  
I guess you can linearize it with either breadth first or depth first. 
With, perhaps, strict tab rules like python. 

So, a program like: 
(
	(move 0 (ua / 6)) ; 
	(loop 0 7 
		(move (ul * v0) (ua / 4))
	)
)
is expanded to: 
(
	(move 
		0 
		(/
			ua 
			6)) ; 
	(loop 
		0 
		7 
		(move 
			(*
				ul 
				v0) 
			(/
				ua 4))
	)
)
encoded as: 
(AST pos) (Argument no) (Token)
0 0 Seq
1 0 Move
2 0 Int 0
2 1 Divide
3 0 Unit_angle
3 1 Int 6
1 1 Loop
2 0 Int 0
2 1 Int 7
2 2 Move
3 0 Mul
4 0 Unit length
4 1 Variable 0
3 1 Divide
4 0 Unit angle
4 1 Int 4

Since we have more than one channel, can encode the AST location, argument location, and token each separately.  
Argument No. is of course redundant, but I think this could be useful.  

Now, editing the AST ... hmm. 
Just adding one token won't really work; need to add multiple, perhaps variable, tokens at once.  
Or we can use a library, and copy-paste?  
If not, we need the transformer to sequentially output tokens like
Insert 1 1 Move (which moves Loop to 1 2)
Insert 2 ... wait, no. 
'address' in an AST is variable length. 
You specify which one of the leaves at each branch you want to access. 
This is kinda tricky with a transformer, and is probably easier with a datastructure of sorts... 
Then there is also graph neural networks, which can work on branching datastructures (right?)

October 4 2022

OK, have the python <--> ocaml interface working somewhat ok now.
Sure was a lot of plumbing to get this working ... I wonder if it would have been easier to just stay 100% in python (was listening to the Lex Fridman interview of John Carmack, and he emphasized that single-language projects are desireable..) 
Anyway, it's done, and this interface can now be used for other things as well. 
	(Clearly I need to temper my ability at both Python and Ocaml.)
	
Next up: emit programs. 
then, have to emit programs that are well-formed. 
then, have to emit programs that produce images. 
then, have to emit programs that produce (and conditional on) desired images

ok, need to generate random programs & see if they produce any output. 
seems like we just need a vocabulary. 

tokens = ["(", ")",";","+ ","- ","* ","/ ", 
			 "move ","loop ","v","ua ","ul ", 
			 "0 ","1 ","2 ","3 ","4 ","5 ","6 ","7 ","8 ","9 ", 
			 "eof"]

also seems that purely unstructured search is not likely to find anything .. ?
as in, start with ()
insert each of above.
the integers work, everything else doesn't
move plus one extra argument doesn't work
move plus two arguments works for integers and ua / ul
two integers doesn't work, but integer binop integer does
loop needs three arguments 
 	variable, count, body
but even that's complicated, body needs parenthesis
	yet it can be discovered by having a empty body. 
	count meanwhile needs an int or something that evaluates to an int
	likewise for variable. 
plus, need a semicolon between sequences (after pairs of parens)
need to think about this.  Enumerating hundreds of thousands of programs is not really a problem, if we have a sane way of going about it -- and can train a transformer / stat model accordingly. 

In the pool was thinking that we need a library -- templates, like an original genome, to modify from. 
Even if we start from nothing, still want a library of well-formed primitives for directing / constraining the search space / memory for incremental learning (as seen in DreamCoder)
	Of course, ideally want an efficient datastructure for this... maybe just boosted trees? 
The point is to make a model (or: datastructure + model) that "knows enough" about the DSL that most of the time it only emits well-formed programs. 
	Later, agument this to emit well-formed programs that solve a problem. 
Editing (insert, replace, delete) is a good bit more complicated than the typical transformer sequence-to-sequence learning, but is more inline with what humans do (tweaking; flexibility)
	I still think it requires an external library
	and some sort of probabalistic structure. 
	This is bound to be more data-efficient than a transformer. 
		Which is of course what I've been working on for a while .. 
		The crux (& well-known problem) is how to combine statistical / gradient approaches with pure symbolic datastructures. 
		Maybe do a form of distillation?
		Nothing really exceeds end-to-end SGD. 
			For end accuracy.  
			It's not compute efficient --
			But it does learn structure, 
			and it does leverage the lottery-ticket hypothesis, 
			and it's scalable / there is hardware support
	OK irregardless the data-representation, do we start with recursive sequence generation (easier)
	Or with (insert, replace, delete) continuation-based editing? 
	
Getting back to dreamcoder, I really like the idea of using upper and lower bounds for constraining probably approximately correct learning.  It seems like a more efficient way to learn a (conditional) manifold when samples are valuable, 
the space is unconstrained, yet locally there are low-dimensional constraints (e.g. well-form-edness). 

Anyway..  evaluating all possible sequences of 5 tokens is only 5M samples. Not so bad if it can be parallelized. 

October 5 2022

I was thinking in the shower: one reason there is friction is that we are interacting with effectively a AST in it's non-native, linear-string form. 
If we could interact with the AST directly, add / replace/remove nodes, then the program is much more likely to be well formed. 
What if in the library each code snippet has an associated hidden vector value (which can be updated via SGD?)
	This sort of knowledge base seems more interpretable. 
OTOH, working with strings is more general.
And easier to shoehorn a transformer into. 

Looking at https://github.com/huggingface/pytorch-openai-transformer-lm, 
it seems that for a classification head (e.g. class logits output), you just flatten the input over both tokens and embedding dimension, and output n_class. 
Pretty simple; I guess do the same? 
For the language model head, I think they just take the very last token (aka truncate), and predict from that. 
This also makes perfect sense, and might be a better way of going about it
But need to check actual function w/ pdb.

October 6 2022
Looking at the pytorch implementation of "Attention is all you need", 
The relevant bit of code is in Layers.py, DecoderLayer, 

   def forward(
            self, dec_input, enc_output,
            slf_attn_mask=None, dec_enc_attn_mask=None):
        dec_output, dec_slf_attn = self.slf_attn(
            dec_input, dec_input, dec_input, mask=slf_attn_mask)
        dec_output, dec_enc_attn = self.enc_attn(
            dec_output, enc_output, enc_output,  mask=dec_enc_attn_mask) 
        # query, key, value
        # query from the decoder output; key and value from the encoder.  we'll need to do the same.
        dec_output = self.pos_ffn(dec_output)
        return dec_output, dec_slf_attn, dec_enc_attn
        
Where pos_ffn is a position-wise feedforward network. 
	xf.py does the same thing, simply / implicitly. 
Hence: 
	-- query comes from the decoder (e.g. autoregressive), 
	-- keys and values come from the encoder layer. 
	-- mask is there to prevent attending to later tokens. 
	
I might be lazy, but I don't see why we can't do this with an encoder-only architecture, where the full program (perhaps with vector annotations?) serves as the input, and you only have to make edits. 

Question then is how to get the edit information?  
Judging from the pytorch-openai-transformer-lm, just flatten over tokens, and use a MLP output layer to map to encoding.. 
IDK, better to get started? 
	Probably will have to go back and fix later eh? 

Hmm now seems that I broke something... ocaml interpreter is being called twice.  (?)

Oct 7 2022
OK, it seems that the problem is that we're not handling the ocaml response properly - and, moreover, when the python program bonks, it leaves the ocaml program running, which then spits out an unending series of log. 

OK seems to be working fine!! 
Todo: add in option to not log, speed things up further.
Also: plug in the transformer :-)

October 17 2022

Transformer is plugged in!  
But it's not training -- probably because many sequences map to the same image, so there is no way for the transformer to determine which of the many sequences made the image. 
So, we need equivalence classes of some sort.. 
I guess do it now in the form of a datastructure?? 
Problem is that this is not probabalistic. (Whereas neural networks /are/)
Right, so this would suggest an 'equivalence' transformer as well as a translation transformer; or that the whole stack is not penalized until it's done, aka RL. 
	(As usual, I'll think about this for a bit before probably deciding that the simplest / most obvious solution is best..)
	
Well... the transformer really should average, right?  
Even if the printed loss is not zero, the actual loss ( in producing a program ) should be near zero.
	That is, it will learn equivalences implicitly ? 
	This is probably also why just GELU works better than softmax for ultimately reducing the loss. 
Thought: train contrastively?? 
	Yeah, that makes a lot of sense. 
	But first need to extend the database of images to something much much larger. 
	Probably this should be done in ocaml
		but then the issue is communicating between ocaml and python.. thousands of records might be slow? 
		Some sort of fast database, SQLlite?? 
			eh, one thing at a time. 

October 19 2022
	
OK, per notes -- need a better source of training images (at least to start .. want to train a transformer to convert images to segs, segs to programs. )
Definitely need some sort of 'library' ala dreamcoder to boostrap structure .. yes?  But then need an e-graph? 
How to even order them?? 
What would a library look like? 
Say, for drawing a polygon: 
def poly n (
	loop i, n, 
		( move (ul) (ua / n) ) )
and a circle: 
def poly n s (
	loop i, n, 
		(move (ul/n *s) (ua/n) )
def circle r (
	call poly inf r)

Right, so drawing a circle required a specialization to the poly drawing routine. 
Maybe I should implement procedures before getting any further ..
there is a lot riding on the type system in logo now! 

October 21 2022

Alright, procedures are implemented, and seem to be working well.  
	Along with comments, I might add -- needed because of the implicit stack-based (annoying) calling conventions. 
		Might need to change that in the future. 
		At least there are no stack collisions per se. 
	Also need to enforce recursion limits. 
	
Let's see, steps for 'evolving' a program. 
1.  Learn which constructs are syntatically / semantically permissible.  
	That is, learn the syntax of the programming language, typically from examples. 
	We can supply these examples; or, in the case the atoms are supplied directly, we search for syntatically permissible constructs / examples. 
		With, notably, analogistic generalization from other domains (e.g. human language)
	Remember those permissible constructs and -importantly- their effects. In basically a database. 
2.  Do heuristic line search on the constructs to try to improve them / fit them to the specification. 
	Typcially in human learning, there is a cirriculum of increasing complexity to mirror the staged complexity of the world and to allow progressive growth of the library + heuristics. 
	-- Interesting here would be to -bootstrap- this: is there enough guidance from either readily-available data to gain complexity
		Or, can you supply meta-heuristics for directing elaboration (e.g. beauty?)
		Maybe probably, that's how humans work.
	+ Also want to store this in a "cortical database of types" (or, in the meantime, transformer..)
3. Observe patterns (how?) in the execution graph, and refactor (e.g. replace, with specialization) long descriptions with shorter, more 'elegant' modes. 
	This heavily involves analogy-making. 
	Deep nets are seemingly good at analogies through vector algebra / arithmetic, 
		But no guarantees on data efficiency. 
		Things like SVD on sparse matricies of document word-occurrence etc also seem to work, so could be that there are many ways to uncover structures (hence axes / analogies) in datasets. 
		Something that I've been thinking about for a long time ... 
	For example, when converting a specification to a progam, one first step would be to just generate a whole list of line segments. 
	Take this program as an example, it plots 5 cricles. 
	
(d0 : ( // arguments: 0: n sides ; 1: size
	loop 2, v0, ( // 2 is loopvar (unused)
		move ((ul / v0) * v1), (ua / v0))) ;
move 0, (ua / 40) ; // center the circles
c0 20, 3; // these are empirical.. 
c0 20, 5;
c0 20, 8;
c0 20, 12;
c0 20, 17
)
	Those 5 circles are composed of 20 segments each, so we have 100 segments.  
	Each segment, if chained (which it can be) consists of 2 floats, 4 bytes each. 
	So the whole list of segments is 800 bytes. 
	The minimum resolution required to see this image is about 64 x 64, and even then segments of the smaller circles are not legible. 
	64x64 is 4k bytes, so the 100 segments is a decent approximation. 
	This same program is 137 bytes without comments (which again is not currently accurate, those comments are needed -- but if we had a better interpreter / runtime, they could be elided)
		Also, of course, some of the smaller circles are best approximated by lower-count polys. 
	
-------
Alright, given an image, can we output a series of segments? 
Then later, given a series of segments, can we output a program? 
Seems like, to bootstrap, you need a large list of segments to predict or something.  
I guess we can just run the stochastic evaluator and make some heuristics on 'good' images, then train up a transformer .. ? 
This does not avoid the invariance problem (segment order is invariant to starting position ; programs may output the same image with differing starting positions, depending on centration.)
	(Convnets permit a degree of translation invariance of course, but the ViT starts out withthout a whole-field convolution.)
	
Maybe there are clues?  
The mypaint test seemed to figure out stroke directionality OK, somehow.

October 27 2022

From the reading yesterday: invariance across different (task) domains implies causation.  
Not sure how general it is, but the example is a duck: it's form is invariant (up to perspective and lighting) across any number of contexts.  Can be a pond or the air or whatnot. 
Interesting idea. 

Think I just need to evolve the programs to produce something 'useful' -- and pay attention to how I do it, so that we can learn this E2E in the future. 

And really need to figure out the stack overflow. 

October 31 2022

Halloween!  
OK, made the edit-ast (as opposed to generate wholly new one); now need to integrate with the python code, to allow python to order the database of images -- 
images for training.. 

Nov 2 2022
Ok, that mostly works ... debugging, ever debugging. 
Time to itch the half-formed idea (hedon) that has been gradually forming.  

1. Humans default to data-efficient solution of problems, not highly statistical data-intensive random-explorers.  I think this is a fact to lean on.  
2. Humans use examples (templates), and typically modify them incrementally (templates are functions that are approximately linear in their parameters).  These examples are typically memorized; some approximation of the Jacobian is stored, too. 
3. **Examples themselves are little bits of code (functions)** -- that is, you can ‘execute’ or trigger memories to modify workspace representations & create intermediate representations.
	In the programming world, these mappings can be learned via experimentation. 
	This probably should hold for the rest of the world too?  Small-network power-law causality? 
		I don’t know about the visual world.. causes are much broader and more entangled.  Probably better to not think about that for now. 
		Also: many snippets of code cannot be readily inverted; they must be run to see what they do. 
			We’re more interested in the computationally invertible snippets.
4. Intermediate representations are often essential to the process of compression. 
5. Humans cache all the representations, especially the IR, to aid in compression.
6. Iterative search proceeds across different levels of representation. 
7. Slow reasoning & search is gradually converted to long-term memory. Statistical regularities in aggregate experience are abstracted away / compressed (how?!). 
8. The system is meta-aware: statistical regularities in its own processes are also converted to long-term memory for shortcut reasoning. 
9. Aesthetics is absolutely essential for open-ended, unguided search. 
	Though this may or may not work so well for biology... where beauty arises spontaneously, by necessity - rather than as a heuristic to speed creation.   
	Aesthetics would have to be controlled via a meta-parameter. 

How a human programmer would solve the logo task
1. Iteratively convert an image to a set of line segments or ‘move’ commands
2. Sort / order those line segments into continuous curves and shapes
	In the case of commands, might need to alternately append / prepend / insert as needed? 
	This is a little bit more complicated
3. Reproject sequences of segments into length and angle space (per logo protocol)
	This is perhaps the most interesting and critical thing to learn from experimentation -- the causal disentangled space is not coordinates, but angle and distance. 
	Seemingly can learn this by forcing computation in the program space, rather than segment intermediate space (the parameters are length and angle)
4. Iteratively search, perhaps based on analogy or something like DNA-alignment algorithms, for encountered programs that generate parts of the sequence.  
	Segmentation (clustering) can be based on region of space, or similarity in coordinate / length / angle spaces.  
5. Memorize virtually everything, and use that for jump-off points for iterative compressive search.

I’m fairly confident that I can get all this working (except 5) with some tweaking on all of the logo tasks described in DreamCoder. 
But it’s still me doing the reasoning, forward-backward passes through the simulation that constitutes planning. 
Ideally we don’t want to do that, we want to make something that is very open-ended and can be used for arbitrary problems. 
e.g. inverse graphics -- if we can do that, then virtually anything is possible! (And is a substantial step toward AGI, or at least narrow information pumps.) 

Human #5 above is the hardest, but also the most critical -- it’s how humans can take on infinite-dimensional problems, and tackle them iteratively 
	Not that this is easy -- much discussion over the past few days has been the vast challenges of ‘automating’ or speeding up scientific research.
		And one must remember that, even with data-efficient humans, you need 10,000 hours of practice at something to become an “expert”. 
How do you store these memories? 
DreamCoder takes the sensible approach of storing knowledge in both libraries (commonly used and useful code) and neural networks (low-N prototypical networks, could be better).  
Humans of course do both, we remember nice little snippets of code, motifs and templates, and can deploy them by modifying in-place to fit the need at hand. 
Libraries are a bit higher bar for abstraction, and usually serve based on code-and data-flow analysis to see common patterns. 
	They supplement human memory capacity.  
		Even AI will probably need such multi-level memory (with of course (vectoral?) comments or something to make is searchable)
		Yet I think it’s not required for initial POC. 
	This can be trivial or can potentially take large quantities of compute, raw experimentation; as mentioned, not every forward computable function has a computable inverse. (Most don’t , probably!)
	Learning things like polar coordinates requires both experimentation and aesthetics ... which atm is really hard to quantify. 

Also of importance is setting up proximal goals: when fitting the segments, you can have a proximal goal of matching the angle, then a goal of matching the length. 
	Seems to require a bunch of machinery, but that’s the way things are. 

Human programmer input:
Atoms -- these are the things you can say.  
	Pretty sure this has to be provided; otherwise it’s unknowable.
Syntax -- this is how you can say it. 
	Can be learned from examples and from experimentation
Examples -- these are well-formed expressions in the language. 
Specifications -- these are well-formed outputs from executing the language.
Breakpoints, debug etc -- yield intermediate representations of what the program is doing.
	Seems to require “reasoning”!
Similarity metric -- some way of knowing if you’re getting closer (even if it’s dumb..)
	And ways of making local similarity, eg angle

I think, with atoms, syntax/examples, specifications, and similarity, it should be possible to bootstrap & solve all logo tasks. 
This is an RL task?  Output a series of actions to solve a given task... only the problem formalism is messier than what’s assumed in RL (MDP task)
	Hence, need to refine the formalism.  
		Pure supervised doesn’t work well because of the invariances / equivariances. 
		Model-based RL might work, ala AlphaZero / MuZero / EfficientZero?  With BPPT, i think. 
		You ignore the complexity of the program to segments space by simply exploring only a small part of it; 
		When using random enumeration, transformers or whatnot might become confused because the function they are trying to map is degenerate (many programs can generate the same image). 


Ok, formalism: 
Given: 
	A - a current program fragment (possibly none)
	B - a current set of example programs (not none)
	C - a set of atoms (small, finite)
	D - a specification (in this case, an image)
create a program that generates an image that better matches the specification. 
and
do this in such a way that search is amortized. (bootstrapping)
then
operate on the program to maximally compress it. 

Analogies to MuZero: 
	A - state of the game board
	B - examples would need to be turned into action sequences
	C - action vocabulary is predefined (if much larger than games -- includes the option of ‘moving’; see below)
	D - to win against an opponent (or not, in the case of Atari). This is more complicated than a fixed specification, from a game-theoretic standpoint. 

The example programs and atoms serve as ‘actions’ to modify the current program. (through fuzzy difference-taking?)
The role of the network/memory is to form a policy: given the specification and prog fragment, select an action, consisting of tuple: { atom / example + ast location + add/replace/remove }

Later, an email to Richard: 
Should you care to read, I'll try to un-muddy my explanation today of what I'm working on: 

1. Humans can learn through memorizing cause-effect pairs.   (In the realm of program synthesis, this is through experimentation)
2. This memory hence serves as causal or temporal inversion: given an effect, infer causes. 
3. Causes can be converted into actions (e.g. more experimentation) which serves to bootstrap (1)
4. Cause-effect pairs and their inverses can be stacked hierarchically, inline with the compositional (computational) structure of the world (computers)
5. Effects that can't be inverted via memory can be inverted via search (which solves all problems, just slowly).  Search then emits more examples for (1). 
6. Making the system re-entrant allows for larger patterns to be found and compressed. (eg. refactoring large codebases) 

There is a lot more icing to put on that already many-layered cake, but if there were a data structure that supports 'fluid' association of cause-effect chains, then it seems likely you'd be able to implement many of the layers.  

I'm proposing that the human cortex does something like this, vis the consistently polarized nature of pyramidal neurons (basal dendrites = causes; apical = effects / addresses) & it does so in a way (default-memorization) that supports data efficiency.  Representations would be vectors, same as in deep learning, though it's somewhat unsatisfying.  (The world is much more discrete than a bunch of vectors in an isotropic high-d space ... yet if it works it works right?)

Nov 3 2022
Why can’t we use something like a LLM for solving this problem? GPT-3 has some truly remarkable properties! 
----
I think we can’t readily use it because 
(1) We want to bootstrap; the initial dataset is too small. 
(2) Because we’re bootstrapping, the dataset is also non-stationary and non-ergodic
(3) This is not how humans learn; they do experiments. 
(4) Humans learn an expanding pallette of examples to draw from. 
(5) Aesthetics matter significantly 
(6) Transformers don’t have a default way of dealing with equivariance / invariance / degeneracy; contrastive training is inefficient.

Responses: 
(1) Why not just hammer-train with small datasets, memorize it, see what happens?  
The vector hypothesis is really very strong & makes a lot of sense & seems to work super well! 
However, in current networks, the general idea is that the dataset is static, ergodic, and the network is a static size too. 
Why not expand the size of the network as need arises?  
	Well, because that’s another thing to account for; static allocation is simpler... 

Back to the basics; what exactly is a transformer doing? 
It’s executing a message-passing and ‘search’ (through query-key) routing algorithm for calculating next-tokens. 
& transformers are typically autoregressive. 
As Andrej Karpathy accurately points out, it means that they are effectively being ‘programmed’ through examples.  
The FAIR number-sequence paper shows that, given enough examples & enough capacity, a transformer can learn approximate-inverse heuristic algorithms to invert even hard autoregressive numeric algorithms. 
	Very very surprising & something to lean upon! 
Maybe one way of dealing with (1) & (2) is to successively train new transformers of differing sizes as the distribution changes?? 

(3) We can incorporate experiments by using search to iteratively broaden the training corpus. 
Still, this is not what humans do -- they do linear approximations, and beam search or Newton’s method based on locally linear approximations.
	I suppose this can just be hard-coded. 
	Getting something that makes good experiments will require some experiments! 

(4) I think the default -- labeling the examples with random-parameter vectors (which can be changed through SGD - maybe? have to experiment) is one way of making it work.. 

(5) I don’t know how to enforce this, other than putting a strong prior on simplicity of code, e.g. store only the shortest examples that produce certain output.  Again, experimentation! 
	Pretty much did this, it’s still slightly unsatisfying? 

(6) This seems like as large a problem as (2) and (4).  The way that it evinced earlier was that the fully-trained transformer would output “move move ul, ul ..” because there could be a leading space.  
Of course, that’s not actually in the dataset; the transformer is improperly doing interpolation. 
	Maybe just train it for longer? 
	Or likely I have a bug in the code somewhere?  
		Aside: I love ocaml, too bad most of the libraries are in python (though certainly I can learn to use datastructures better in python?)

November 10 2022

Some discoveries!  
Upside-down RL, invented about three years ago -- seems to have a bit of traction in the form of using a Transformer to model. 
Idea is that, you have a bunch of trajectories (state, action, reward) in your replay buffer; rather than taking those and producing a policy of Q(s,a), you instead indicate the desired return and the present state, and use the model to produce an action -- the policy is implicit. 
They have showed that, in some cases (SeaQuest), you can even get better performance than what’s in the training buffer!  E.g. the transformer generalizes out-of-distribution.  (In most cases, it does not generalize).  
The transformer paper does not explicitly tune the replay buffer.  In the Schmidhuber paper, there is truncation selection in that the model is trained only on the to N past trajectories, as organized by total reward.  
Even then, they show the system can remember old rewards and produce appropriate trajectories when appropriately queried. 

Another discovery: https://ogma.ai/2019/08/acting-without-rewards/
This mirrors the UDRL idea, sorta. 
The idea is that you learn tuples: 
(state1, action, state2) 
& from this derive a change in state d_state so 
(state1, state2, action, dstate12).  
Now, say you are in state1 again, and want to get to state2 (maybe approximately)
Simply look it up in your memory store!  And viola, you have an action.  
	Actions generate a change in state, hence can be indexed & memory-inverted by setting a goal state and delta state. 
Add in reward and something like truncation selection, with interpolation, and you can do some reinforcement learning & presumably bootstrapping.  

This gets back to a core feature: most things a neural network can do can be accomplished by a database & suitable search-ordering system.  
Neural networks are generally fast access & afford interpolation, though. 
Transformers are unique in that they exist partway between database / look-up table and algorithm. 
As argued above, the cortex probably does the same; memories as bits of code etc. 
	Additionally, imho the cortex is a ‘more effective’ form of self-organizing database & has systems for fast, parallelized search (which maybe takes advantage of entropy). 
	The cortex implements forms of algorithmic compression maybe by exploiting this innate search + self organization to yield ‘causes’ or ‘addresses’. 
Also to note: even if this general database conception is correct, it still does not obviate the need for great skill in constructing the database & indexing mechanisms
	Or in great skill in constructing the neural network & training mechanisms, as is clear from history 
		(And is emphasized in the UDRL paper, which says that the algorithm works OK even despite not having a decade of research put into it.)

Ogma AI seems to have solved the inter-layer coordination problem, perhaps ala Sidney Lowe (just ignore it?), which is otherwise solved by backprop. 
	Not really a lot of details of his implementation, and indeed no math, which is troubling.. the code is a bit meh tbh.  But he does seem to be a clear thinker at least. 

OK, say we were going to solve this problem exclusively in ocaml. 
can easily make a database of: (state1, state2, action, dstate12)
	Even though I’ve been lax with testing it.. 
There are a LOT of actions that simply do nothing, and hence should not be added to the database.  
Say for solving a task with only 2 segments, this is not hard: you just do a look-up table.  
If you’re approximating the function using a transformer or vision transformer, real problem in that the mapping is not smooth (I think?). 
	And as mentioned above, it’s not how humans do it. 
	Also, a lot of these Atari tasks are not smooth, and yet people have been chipping away at them, quite productively! 
I think instead you want a flexible database, where by default you search the whole thing & later you do some sort of faster associative lookup. 
OK, start with (move ua, ua) -> one horizontal line & the associated segment. 
options are to change move (to a binop?  does nothing)
or change the arguments (to other constants, to binops, or to other variables; the latter does nothing..)
or introduce a loop (which will likely be very confusing to the system); this is an action to refactor the code, really. 

Now, we have the problem that the data looks pretty heterogeneous: images, variable length lists, tree structures, libraries of tree structures...
	e.g. we’d need to do 
	reprojection via Inception for the images;
		Or some BYOL approach.  
	BLAST for the variable length list; 
	some sort of graph matching for the tree structures; 
		maybe later; transformers for now? 
	iterative matching for the libraries. 
Searching and matching on any of these seems highly non-trivial; how is the brain so good at it? 
One obvious thing it does is to serialize otherwise difficult searches (spread the problem out in time) via attention. 
	Maybe attention is just holding one target in memory & seeing what auto-associates with it. 
	I really like the analogy to protein docking in solution.  Takes advantage of entropy! 


November 11 2022

Right, change_ast does seem to do something; ‘appears’ to work properly, though likely have to constrain this a good bit more.  
I like the idea of keeping it encoded as strings -- this is much more manageable than a tree (for the time being). 
Pretty easy to build up a library of these minimum-edits, i guess.  
	In the case of needing to do multiple edit operations before the DUT program works again,
	think that the UDRL + transformer approach might work fine? 
Next task is to allow genetic-algorithm like copying of one segment into another. 
	This needs to be from a bunch of templates.. 
	How to determine the edges of the edits? 
	Think we need a good string library (or regexp library??)
Then, the task is to search over the templates.
	Looked at the Smith-waterman algorithm, idk, almost want to initially take a brute force approach? 

I had better run before it gets dark.  Also need to do some upper-body workout..

November 12 2022

Alright, painted the laptop (Creator 17) for the second time!  This time it will look good! Hopefully.  Have to let it dry very completely (>3 days) if i need to repaint it.  

So, regarding yesterday, I was thinking: 
	1 -- The edges of the edits.  This takes a long time for even humans to learn!  It can be clear based on pauses in speech, but the phonemes  take time to disentangle.  
		Might be as simple as pattern matching (blind signal separation or sparsification) on the input stream, 
		Which is of course something that many people have been working on for a long time; neural networks might well be OK at this.  
		Other sequence alignment spaces, eg temporal convolution used in matching protein motifs, could work well - ? 
		Sequence alignment itself is an intensely studied problem. 
	2 -- If you have small enough convolutional filters, you could do something like the BLAST algorithm (which uses i think a fixed vocabulary?)
	3 -- Should investigate some of these tree RL and tree transformer algorithms.  

November 14 2022

UDRL + transformer approach to solving the problem.  
Can presently easily generate a database of small, well-formed and productive logo programs. 
Take this database, select a subset of it (biased toward smaller samples), and find the closest different program. 
Call them progA and progB. 
Given this, input to the transformer the context (imageA, imageB, deltaAB, progA, progB)
	Add in segments too?  hmm.  seems like too much data. 
Add in location coding for both the images and the strings. 
Add in a cursor location so the model can remember what it was doing
Supervise the model to output delta progA progB <end-of-edit>.  
Pray that it generalizes from the experiences. 
Allow some bootstrapping -- generate random programs, hence imageB, and ask it to suggest edits to recreate the image. 
Keep it around if it happens to run and produce ‘interesting’ output. 

Do this, if it works somewhat OK, then worry about adding in compressional edits. 

Thoughts: 
	Seems like it’s pretty easy to make a system that doesn’t work at the beginning. 
	For example, in the self-predictive representations RL paper (basically BYOL), 
	They find that only using the L2 loss causes representation loss, 
	whereas using cosine loss (normalized L2) seems to work fine & does not collapse. 
	Little things like this differentiate an experienced practicitoner vs .. me haha. 

November 16 2022

Working between Ocaml and Python is a real PITA ... spent the better part of the afternoon debugging a communication problem. 
Anyway, the goal now is pretty clear: 
given the context 
	Images A, B, and their difference -- all black and white
	Program A and program B (very different positional labels + location labels. )
output an 
	Edit command (insert, delete, sub, done -- one hot / softmax output)
	Location (in the same coordinates as the positional labels of course)
	Character (in one hot coordinates, same as the input)

Keep the edits pretty small, 1-5 .. for the time being .. then we can work on dramatically larger edits (aka refactoring!). 
	Make it work first! 

December 1 2022

December already!  
Tried running the full model on lambda cloud -- and it seems slower than my laptop. 
Normally, this wouldn’t be a problem, but it’s cloudy today so am not getting as much solar power. 

Anyway, todo: now need to bootstrap, eg. have the Transformer continually output programs, which then enter into the corpus of training data. 
This requires a second level of communication: 
-- python needs to apply the edits (which it already does), 
-- then it needs to transmit the encoding to ocaml
-- which needs to decode it and convert to a prog, e.g. AST
-- which can then be run by ocaml to produce a (progra, encoding, image) tuple
-- which is then transmitted, interleaved, into the databases of both python and ocaml.  

Sure would be much easier to have another way of distributing this processing, especially given that cloud instances seem to have lots of CPUs and GPUs, and it would be great to take advantage of this.  
This would also eliminate the complexity (read: stupidity) of having two copies of the data in two different places. 
	Such a bug has already bitten me once ... 
	That said, protocol-buffers are probably faster than a database?  
		Is there not a reasonably native database? 
		DAta schemas are easier to change when we don’t have that external dependency.. 
			Maybe punt on this one until we have more resources.  
			In the meantime, just have to be careful. 

Tomorrow: todo: make sure the transformer can memorize nothing -> line segments.  

Dec 2 2022

Made the program database larger -- 10k -- and immediately the initial loss was lower! (50 vs 62; batch_size = 24)
As it was compiling the database of programs, was obvious that many more compliated programs were being replaced by simpler ones.  Good!
Also tripled the number of iterations for training, since it seems we have enough solar / battery for the time being (middle of the day, though overcast)

After 228k iterations, loss is hovering around 42 (batch size 24); It does seem to be gradually decreasing, just not very quickly.
Really need to get the bootstrapping code working; worth cranking on this.
	Note: after 300K iterations, the loss did not decrease anymore.
	Still at 42; assume that it won't go down further?
	Errors seem pretty broadly distributed across type, location, and character.

I think, for this, we make a normal batch w/ unkown program b (sampled from the database) & ask the network to produce edits until it otputs a "fin" token.
Then, send this to ocaml, as above, and ask it to execute.
If it fits, per the same criteria as the ocaml stochastic AST generator, then add it to the database.
I guess do this every so often?

Also very important: things are getting to the point that starting from a blank slate isn't acceptable anymore.
Need to start from a pre-trained transformer & pre-filled database of images.
Redis integration time?

December 5 2022

Testing to see if the transformer can completely memorize the first 100 image -< program pairs, doing only insertion and no deletion or substitution. 
This is with the 10k database, as I tested in the desert. 
Hmm .. something seems off, 'insert' seems to be always the same, when it all should be asynchronous and hence varying on the sample clock (at least, with the sometimes longer programs) 
Got a bug in there! 

Redis doesn't look to hard.  Storing more complicated datastructures can perhaps be done through the JSON library; or by using multiple hash tables (which are an builtin structure)

OK, there are more bugs:  the transformer cannot memorize the simplest 100 program-image pairs: 
	after 300k iterations, slowloss is still 47 (batch 24)
	Todo: plot the loss
Hmm.  Need to test things more. 

December 6 2022

The loss seems to be gradually *increasing* over time ... its spiky though.  
Why would it be increasing?  Something in the model is ill-conditioned?  I need to look at the training data more before poking around with the model. 

I think i need to keep a log of the errors -- where the model goes wrong, and why, to better understand why it's not converging.  
Seems like it's stuck in some sort of equlibirium, where some of the batch members can't be accurately decoded; or, some members are too hard to decode because the space is too convoluted. 

December 7 2022

Seems to be training better... 
Todo: in cases of + and *, can reorg so that the largest operand is first. 
This removes some invariance. 
Check check. 

December 8 2022

Uff, even with a higher learning rate, the system does not converge.  
E.g. it can't even output 'E' at the beginning of the string.  
What's broken?  
Can it do classification?? 
Can it output a fixed string? (e.g. positional dependent production, all "ins". 
	This is easy to check -- train on one example.
		Always seems that I have to touch nearly everything before it starts to work.  Figures. 
Nope, sure doesn't seem to be learning. 
Ah, bug in the insert method ... fixed. 
	Proves that you have to instrument nearly everything!
Plateaus at a loss of 25. 
With occasional blips.. 
-- It always gets the editing position correct, good
-- It's perfect at the 'fin' token
-- When there is no context, at the start of the edits, it gets both the operand and character wrong (loss= 48)
-- At the end of the string, decode is correct, but loss is still ~30. 
-- Other two errors are for putting the operand as 'fin' when it should be 'ins'. 

Seems that the activations are blowing up.  Hmm.  how / why? 
Seems that they blow up really fast at the beginning... very unstable model, not sure why. 
Bad initialization? 
prt_to_edit seems especially wrong! 

WHEW.  Adding appropriate Softmax (don't be lazy, tim) and scaling the learning rate + adding gradient clipping seems to have fixed things, it converges with one example now :-)
Let's try 10 examples.

Yep, all works, even with 1k examples, and gets ~ 50% correct (11 out of 24) when trained for 100k timesteps.  !!Super excited!!
Next todo: take the emitted programs, and run them. 
If they run, see if they have 'about the same' cost and image result as a program in the database. 
If it does, then replace the items in the database. 
If they don't collide, add to the database.  
Then, see if this works... 
	Will definitely need to start writing to disk pretty soon.
	
Also need to add in substitutions and deletions to the list of actions.. 

OK, so this commit (Dec 8 2022, link below) has very good performance -- probably it's memorizing the dataset but so what -- only gets 5/24 errors on the test batch.  2x better with the learning rate schedule! 
https://github.com/tlh24/cortex/commit/d2ab09ba7012c17205eb9b9d574557ccda5b0d34

Dec 9 2022

Just installed torch for ocaml .. seems to work great!  YAYAYAYAYAYAYAYAYAY
All that i think is required is to link against the gpu libtorch (currently in dropbox) when installing.
Add:
export LIBTORCH=/home/tlh24/Dropbox/var/libtorch/
to ~/.bashrc
and everything should be smooth?
	(I have installed from git on the laptop)
	Also note that it requires a downgrade in a number of libraries; see torch.opam in the git repo.
Yep, seems that installing from opam and not from git works just fine, thank you very much! 
Now, need to move the image-space comparison from python to ocaml. 
Simplifies things greatly, think. 

Dec 11 2022
ocaml: 
	512 done; 644375 sampled; 395 replacements
why so many sampled?? 
python: 
	done with 10240 unique image-program pairs
	there were 406026 rejections due to image space collisions
	of these, 2057 were simplifications
(the python script does take longer, at least..)

OK, seems that distance calculation was being done differently in ocaml and python -- ocaml normalized the pixel values to be between 0 and 1, python did not. 
Changed the distance to 5 now (pixels)
	done with 2048 unique image-program pairs
	there were 110411 rejections due to image space collisions
	of these, 752 were simplifications
	this took 30.835718607999297 seconds
Ocaml: 
	2048 done; 106404 sampled; 725 replacements
	Execution time: 73.408486s
	2048 done; 104946 sampled; 728 replacements
So ... this took twice as long!?  why
Seems like they are just about the same computation. 
Interestingly .. GPU utilization as reported by nvidia-smi is about the same.  
Ah well, it was a fun experiment, I guess. 

Tried replacing pow with einsum -- no difference. 
Might as well use pow, it's easier to understand. 

Tried changing the bigarray conversion.. 
Made it slightly slower haha. (maybe)
Changing from square to absolute value speeded things up slgihty, but probably only because fewer programs were sampled.

Hmm, the program seems to run out of memory.  
Might be a memory leak -- ? 
with a db size of 2048, python uses 776Mb & takes 29 seconds. 
in ocaml, the same program needs 10360Mb, 13.35x more! 
Yeah, with only dbf, allocated, consumes 780mb. 
2048 * 30 * 30 * 4 = 7.3 mB ???
Ah, seems that torch by default allocates a fraction of the gpu memory for scratch space.  
The actual variables don't seem to change this. 

OK, everything seems mostly performant.
100k iterations, 5*2048 DB size, with minimal program, 24 sec. 
with full program (heavier GC), random images, 24.5 sec
with generate_random_logo, 27.32 sec
	delta = 28.2 us per program. 
with bigarray_img2tensor, 30.58 sec
	delta = 32.6 us per conversion (slow!)
Note it takes about that long to generate the random image, too. 

If we switch to the 2080Ti, 31.8 sec fake (gen prog but no conversion)
32.14 real (gen prog and convert to Tensor)
So, the conversion code is not so bad, considering there is probably fixed overhead for creating a tensor on the GPU. 
& the eGPU has a slower interface, but faster compute. 

Note: doing the same thing, with random images, in python (100k iterations, 5*2048 db size) takes 39.3 seconds. 
So, ocaml is still faster, even when also generating the program + image pairs

December 15 2022
Seems like there is some corruption in bimg.  Hmm. 
Igored the stride, whoops. 

December 19 2022
It's training to convergence with the new architectcure!  Yay!
Observation: if you only ask the transformer to learn / remember small segments of space (for example, by limiting the total number of edits, or limiting the edit type to only be insert) then the model can learn the mapping very quickly, on the order of thousands of iterations for thousands of items to be memorized (albeit with a substantial batch size -- but still, this almost comes for free with GPUs!)
So, I don't think the overall efficiency of transformers is per se extremely low, might be for carefully curated data near the 1/batch_size; or you need to visit every datapoint 20-100 times. 
Not as good as humans, but not really that bad. 
(Visiting the datapoint is not the same as gathering the datapoint, of course.  Also, the diff op is effectively a data augmentation, though probably a confusing one. )

December 20 2022
It converges! See:  
screenshots/20221220_ViT-prt-diffsize8_dbsize8k_convergence.png
This was with a database of 8*1024 (of 12 * 1024), program A and program B selected randomly from therein, with edit distance < 8 and > 0, num segments <= 4 and program length <= 16 for both A and B.  
Other parameters: 
	let image_count = 6*2048 
	let image_res = 30
	let batch_size = 128
	let toklen = 30
	let poslen = 6
	let p_indim = toklen + 1 + poslen*2 (* 31 + 12 = 43 *)
	let e_indim = 5 + toklen + poslen*2
	let p_ctx = 64

	Just turned on torch DataParallel -- the lights in the room flicker a little bit, but training throughput went from 2400 samples/sec on the 3090 to ~ 4100 samp/sec. 
	This with a batch size of 3*384 (384 per GPU). 
	The log loss is decreasing both faster and smoother; seems that a large batch is indeed useful!  
		Re flickering lights, apparently if I run DistributedDataParallel, we can use more CPU and make the load more uniform.  Problem is that there is but one interface to ocaml mmap files. 
		(Ocaml itself can support multiple connections; just need different named mmap files, which should be easy; don't quite feel like implementing that now). 
		As always, having more GPUs would probably help here. 
		(Seems like the price of 4090's and 3090's is approximately equivalent to their compute capability.  Seems like I should wait until after the holiday). 

Todo: add in a # of replacements counter to the logs & hence plot.
Todo: batch size is a command line argument for both ocaml and python

December 21 2022
batch_size command line argument for both ocaml and python:check. 
number of replacements counter: check

December 22 2022
Tried out automatic mixed precision: the log loss flatlines around -3 & the std deviation of the activations seem quite quantized. 
Mgiht need to change the model a bit? 
Seems to run quickly -- on the 2x 2080Tis, getting 4700-4800 samples/sec throughput! 
(using all 3 GPUs, get 5500 samp/sec)
This with a batch size of 512 (same as 768 with 3 gpus)
This also saves about 300W of power.. 

OK, AMP doesn't work well.  Never gets to the very small errors, and doesn't seem too good at editing programs, either. 
Will stick with floats for now! 

Changed the batch generation code such that half the time, it generates an edit (fix the program, within a certain distance), and the other time the network has to genearte the program de novo. 
Seems to be converging just fine, I guess?  
Have to see about the edits. 

Next up: add in a database of equivalent programs; the list head is the shortest. 
(Issue: because matching is within an aperture, periodicalyl will need to go through all the stored programs to make sure that they are still sorted into the correct bin..)

Regarding equivalency -- I think that we should add the equivalent list **after** filling out the full database, e.g. don't necessarily need to regen the full db... 
Checking and sorting the equivalents slows down original gen by quite a bit. 
And the equivalents are already quite saturated; can prune a bit
	Yeah, maybe that;s the right way, cap rather than truncate! 
	(So it's a random selection of equivalents..)

OK, the equivalents code is up and running just spiffy, thank you very much -- but still, i feel that this is one part in the whole; could train up a transformer (the transformer?) to simplify the program; but is this going to e.g. solve the problem?  
Really need library growth (too), as has been noted as the key contribution here: 
https://news.ycombinator.com/item?id=30190954

Todo: figure out why the model gets stuck introducing long strings of the same charachter -- even when this never occurs in the training data! 

December 23 2022
Got the model running & installed (relatively) swiftly on Lambda cloud.
on one A100, runs at ~2700 samples / sec.
CUDA is enabled on both the python and ocaml side :-)
(still slower than my desktop)
Laptop is running at about 1500 samples /sec @ 80W, throttled -- not bad, really.
On 8x A100-SXM server, get ~8800 samples/sec with somewhat imperfect resource utilization @ bs=8192 -- pretty fast, but also somewhat expensive. (Note that ocaml is not saturating, even at these huge batch sizes, at least for training and not hallucination..)
(~1 batch/sec;
c.f. desktop, 4400samp/sec @ bs=768 -> 5.7 batches/sec
c.f. laptop, 1500samp/sec @ bs=256 -> 5.8 batches/sec
c.f. desktop, 3090 only, 2650samp/sec, @ bs=512 -> 5.2 batches/sec
c.f. desktop, 2080s only, 3550 samp/sec @ bs=512 -> 6.8 batches/sec
c.f. desktop, one 2080, 2000 samp/sec @ bs=512
The install script needs a little bit of hand-holding to work; in particular have to tell opam to not edit .bashrc
Would almost certainly be faster if i re-engineered the system to have multiple python threads & async update, rather than regular DataParallel.
OK, anyway, good experiments, now i know!

To extrapolate a bit: a 4090 is maybe 1.7x faster in training than a 3090. 
My 3090 is slowed down by being on thunderbolt 3.  Should be ~2.2x the speed of a 2080Ti according to 
https://www.exxactcorp.com/blog/Benchmarks/rtx-3090-benchmarks-for-deep-learning-nvidia-rtx-3090-vs-2080-ti-vs-titan-rtx-vs-rtx-6000-8000

Hence, on a fast computer with a fast PCIe bus, total performance should be ~ 1.7 x 2.2 = 3.7x faster. 
2x 2080Ti = 3550 -> 13200 samp/sec for some appropriate batch size. 
Well, not transformative but still pretty good - and quite a bit faster than virtualized 8x A100! 

January 4 2023

Tested whether driving a monitor affects torch speed.  
It does not seem to affect it at all; see below. 

# with the monitor at 4k120: 
# GPU # 0 (3090) at 26-40C: 12.4598 (34% faster / 2080 is 53% slower)
# GPU # 1 (2080) at 60-70C: 19.0269
# GPU # 2 (2080) at 48-65C: 19.5551 (2.5-2.7% slower)

# with the monitor at 4k24: 
# GPU # 1 (2080) 65-72c:  19.0552 (slightly slower, within margin of error)
# with the monitor at 2k30: 
# 18.979, 18.9932
# back to 4k120: 
# 19.0269, 19.1146
# 0.4% slower.  Probably meaningless!
# Conclusion: no sense in adding another GPU to the system.  Seems that driving a monitor does not consume many resources; variance between cards is greater. 

Seems relevant: 
https://www.mosaicml.com/blog/introducing-pubmed-gpt
(their cloud is in public beta)
and also: 
https://github.com/HazyResearch/flash-attention
(though don't need it yet ... many more low-level tasks remain before engaging in such optimization!)

January 5 2023
Notes: NeuroAI blog post: https://xcorr.net/2023/01/01/2022-in-review-neuroai-comes-of-age/ 
and : https://xcorr.net/2012/04/05/topography-conquers-all/

It would seem the next steps are: 
1 -- Extended context, extended actions. 
Add in extended episodes, where the program can try multiple times to attain the target. 
	This requires extended attention, 
	which means we need to add in the logic of Transformer-XL or some other long-range attention mechanism (??)
It’s not that much different than the current approach, where the transformer outputs edits -- here the transformer would output commands like ‘debug’, ‘print’, ‘call’, then run the program, observe the results, and potentially make new edits based on the whole episode of programming. 
Afterward, we train on instances drawn from a database of the most successful program interactions (including “printouts” or “debug traces”, so that the transformer bootstraps useful behavior & clones it & hopefully generalizes to new situations.  
	Note that the only feedback to the fact that adding debug symbols was useful is that of context: given a confusing situation, performing debug action in the past got you closer to the goal.
	There is no explicit reinforcement learning, other than truncation selection memory! 
Critical here is to replay episodes, to some finite degree, that the transformer is unable to complete on the first pass; 
	This requires more feedback logic, of course. 
	And the aforementioned longer context.  (= short-term memory)

2 --  We need to add library generation here. Options: 
	* Version space algebras / algorithms
		? will it work with declarative languages? 
	* Equivalence graph analysis
		? this may or may not work through delcarative / partly functional languages
	* A separate transformer network
		Where I’m leaning now.  
		Seems perfectly general. 
		But very likely will be less performant and data-efficient than bespoke refactoring algorithms. 
		
3 -- What about A-star & other search mechanisms? 
	Seems like you'd want to add this -- or at least, support it?  
Perhaps via behavioral cloning.

Decision: should look at the EGG library more to understand it better. 
Internet seems to agree that the library generation is one of the key innovations of DreamCoder. 

January 6 2023
Looked over the Egraph documentation online, and I sort-of get it: 
the core of the algorithm is the union-find datastructure, which stores sets and can perform unions and finds on them, e.g. to determine if two elements are in the same set.  
I think, rather than storing point-like elements, in an Egraph, graphs themselves are stored. 
These graphs are generated by applying transformations (rewrites) to the AST, converting the tree-type graph to a DAG (maybe not acyclic?). 
The important thing is that the rewrites are not destructive; you keep around the original version of the graph, as you add to it. 
Nodes themselves are re-used in the rewrite rules, saving space. 
New nodes (operands and variables, eg.) can be added. 

That said, I don't see how the Union-find datastructure relates to the Egraph. The DAGs are stored in a union-find?? Seems like you just want to store the graphs themselves, and have some way of searching them? 
But then what's the union operation / rebuild in the EGG library?  

let me look a little bit more at the EGG library applications. 
	(interlude to fix broken libqt4 .. which is EOL back in 2019 .. not sure how it got installed??!)
Read the paper "Equality saturation for tensor graph superoptimization" -- all makes sense.  The explanation of e-graphs is high-level yet useful.  They do need to limit the number of possible re-writes as there may be a combinatorial explosion.. 
In the case of transformer-based rewrites, 
** would be helpful to have a datastructure like Egraphs to support arbitrary transformations. **
Of course, if you don't select the order of rewrites at the beginning, you have to eventually select the order later, when extracting, which is why they need to use integer logic programming to solve the ordering (I skipped over this segment). 
Authors indicate that ILP phase can take the bulk of the compute.  
Also like the idea of equality or output saturation as a means of getting around tight local optima 
	-- you might need a long chain of rewrites to discover something much better.  
	-- e.g. much higher search depth, which is supported by order-agnostic Egraphs. 
	
Next, need to review how DreamCoder forms the library.  Which is of critical importance: can't have all the memory in the transformer; it just doesn't scale as well as external code! 
Also on the docket, figure out how to get much more context. 

I'm really not sure about version space - equivalence graph hybrid used in DreamCoder supplement -- not only is the lambda calculus substrate unfamiliar, but the mechanism of rewriting / storage is very poorly explained.  Instead, it's a proof that it's congruent (doesn't change the semantics of the code) and complete (samples all possible (??) rewrites). 

I do think keeping around a large library of equivalents makes sense, and perhaps keeping these in a datastructure smarter than a series of strings *makes sense* 
E-graphs do this, in that it stores many permutations of equivalent representations, and you have the freedom at any level of the hierarchy to select 1 of N.  This, i think, is where the exponential compression comes in. 
	But does not abrogate the need to ultimately decide which series of rewrites are best. 
	In the context of the transformer, I guess again (for the time being), store a population of equivalents. 
In the context of making a library, we still have the task of mapping equivalent results from experience. 
Probably want something other than a transformer to generate the library and then refactor the programs in the database. 
The version space algorithm seems to work well in this respect in that, since it can store many equivalents of the progams and rapidly search for matches, you can propose many new library functions, try them out on the list of derived programs, and see which new functions 'win' in terms of rewrites. 
Importantly, want some of these rewrites to replace existing library functions, perhaps across multiple abstraction boundaries.  
For this, I think you need some sort of invariance-permitting BLAST matching algorithm, that can operate on actual dataflow observed in the programming language emulator.  
	Seems maybe inefficient. 
	(Might need a computer with a lot lot more ram ?!)
	
Hedon from run: Obviously, need extended batches of interaction ... w maybe frozen weights? 

January 14 2023
Well, I've spent a while fiddling with CUDA versions, waiting for multi-gigabyte run files to download, installing and re-linking different versions, and have to say... I can't get either DistributedDataParallel or DataParallel working.  

My comments are here: https://discuss.pytorch.org/t/ddp-training-on-rtx-4090-ada-cu118/168366

Roughly, https://github.com/The-AI-Summer/pytorch-ddp
DP runs, but gives NaN loss. 
DDP works, but is slower (with lower GPU utilization) than training on 1 GPU.  
Bumping up the batch size to 512 increases the GPU utilization, but does not increase the fidelity of the network.  After 4 epochs, loss is higher on the test set of 10k images vs. training on 1 GPU.  
This is with `export NCCL_P2P_DISABLE=1`
without that flag, everything stalls / but is killable.  

Regarding ec32: it runs just fine on either one of the GPUs. But I've never gotten it to work on both.  
Think I need to wait until the stable version of pytorch supports CUDA 12.0 & more testing has been done on RTX 4090s. 
(e.g. two 2080ti and 1 3090 seemed to work just spiffy.. perhaps because these have been tested a lot more?)

Finally, torch.compile doesn't work because the required shader model is either not supported by nvcc (sm_89), or b/c torch is not linked against cuda 12, it doesn't communicate properly.. 
	Again, waiting this one out is probably the best.  
	
Now, training on one 4090 gets us ~ 4480 samples / sec, (batch size = 512) or 4600 samples / sec (batch size = 256) which is better than done before -- and about half the performance of 8x A100 SXM using DP on Lambda Cloud!  hah.  
And it uses < 300W.. and moves along nice and quick @ bs = 256! 
	Helping this is that the fans remain constantly spinning, so when hallucinating, it keeps the GPU cool, 46C at 2.8GHz! 

OK, now let's move on to some real work.  
As I was falling asleep last night, it all seemed so simple (as it does, when you mind is free and loose..)
You start with complicated, entangled stimulus 
	(as in: the real world).  
Then you move to a more simplified, discrete representation
	here: a series of lines
Then you need to organize those line segments, which can be done with a sorting algorithm. 
	A sorting algorithm that's supported via an attention mechanism. 
	Or by computed-addressing active dendrites etc. 
Once sorted, then you use them as a key approximate-analogy memory to search over past experiences that generated those lines
	This is iterative, and involves a degree of trial and error. 
		Those trials get added to the database. 
Now you have a slightly more simplified, discrete representation of the stimulus & need to do another iterative, interactive pass to figure out how to compress it further. 
	In this case: loops or calls or something.  
Eventually, you get pretty good at inverting stimulus, and now want to factorize (compress) not just the stimulus, but the list of solutions generated. 
	This is an equivalent problem, though to some degree it's more complicated: the list of solutions can be very long, and so the interactive loop might be slow. 
None-the-less, the interaction is the same: peer through the long list of stimulus, and look for commonalities which can be used to factorize the list.  
	E.g. use the approximate-analogy memory to search over past experiences that generated these traces;
		Sum/average these over the large population of programs
			Might need backtracking though. 
Try it out interactively, and verify -- 
	in this case, you want exact or nearly exact replacements.  
E-graphs are great here, as it's easy to search them for equivalent factorizations -- but, I think, you can get something similar by looking in less-compressed space aka behavioral space and matching on that.  
	Or, by having a algorithm (or transformer) (or database) that can compress & remove invariances. 

Using transformers to serve as bidirectional memory (or rather, inverse memory) works pretty well here, as they *do* have a degree of algorithmic ability: they can do sorting / organization (as mentioned above).  
Yet, I remain itchy: can we not do something much more sample-efficient with direct algorithms, like fuzzy dictionaries and ScaNN/faiss for matching?  
It will also have to support algorithmic organization.. 
	To be more explicit, an important part of compression is moving from a space with significant invariances / equivariances (such as the order of line segments) 
	Into a space where they are cannonicalized based on say stroke or continuity 
		-- or, more importantly, what can actually be produced by slightly-more-compressed data representation (curved line segments). 
A transformer *should* be ok at this, but there might be something more direct -- perhaps some of the algorithms like Smith-Waterman for assembling DNA? 
	That is, you can look at the problem of sequencing segments as a problem of de-permutation (~= assembling DNA segments)
		Which is not Smith-Waterman: instead, you use cintig extension, with search for extensions speed up by keeping all the short reads in a prefix tree (or ‘trie’)
		See https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2045152/  
			Written in Perl! 
	Another way of looking at it is you're binning the stimuli into causes, where causes can spread laterally, too (eg edges, continuous segments.) 
		This gets closer to what the brain might well be doing.. 
		Not gradient descent at all!! 
		But certainly conditional computation / addressing. 
Such algorithms (or neural structures) will more explicitly represent an inverse map, and hence hopefully be more sample-efficient. 
	but you might need more of them, for multiple hierarchical states. 
	& to make them equivalent to the multi-level computation of a many layer transformer. 
Anyway, keep that on the back-burner!! 
	Fertile area, but let's stick to transformers for the time being, sample efficiency be damned.  
		And crap like ;333333333333333;EG@F=FB4>E4=G< lolsob. 
			(Really need to figure out this failure mode!)

Also on the back-burner: rather than using torch DDP or DP, both which don't work (yet), should augment both the ocaml server and the python client to simply run two versions of the same network in parallel, and have them both add to the database of equivalents etc.. in the meantime, meta employees can fix compatability with multiple RTX 4090s.  

Other things to generally think about: 
*  Debugging.  
	If everything is perfectly observable at every level, it's not needed. 
	But that's asking a lot & is not what humans do! 
	How do you build in debugging?  Something about the failure mode of inversion / need more information for the inversion code?? 
		A “frustration” module?
* Many levels of hierarchy. 
	Should we have one model (transformer) for working with different contexts, or different models for different contexts? 
* Library generation. 
	I'm thinking now that this needs to be via a "hard" algorithm, not a "soft" neural net.  
		Though our squishy brains seem to work fine.. 
	Think we need to get the current model working on MNIST first, outputting series of 'move' commands, based on longer interaction episodes.  
	
So, tasks: 
1. Longer interactions via larger sequence attention.  
	(Transformer XL + Memorizing Transformers)
	This entails some cleverness re episode interaction. 
	Also requires that 'hallucinate' is effectively called every time. 
		Which might be slow. 
		And how do we get rid of the stuttering?? Uff. It's clearly a feedback loop of some sort; maybe need to propagate some negative gradients on it.. ? 
	Yeah, prolonged sessions / repeated actions need to be sorted and contingent on actually solving the problem. 
	Getting rid of the stutter might require training loss on them, too.  Positive reinforcement on what you should do, some negative reinforcement on what you should not. 
		Hmm. Would be good to have someone to bounce ideas off of. 
2. Try to solve MNIST digits
	Always MNIST. 
3. If that's good and reliable, then start working on library generation for the solutions generated. 
	Remember: make the entropic steps as small as possible, so inversion is easy! 
	This means selecting the right sets of intermediate represenations. 
	ERNIE trains a transformer to do a sorting task as pre-training; can hypothetically do (something like) this to compress away the null space. 
	
January 17 2023

Compiled pytorch from source, linked against CUDA 12.0 -- still has the same bugs in torch.compile and DP / DDP.  
Learned today that many models are limited by memory bandwidth -- which behooves me to ultimately get both 4090s working in parallel, e.g. DDP. 
Maybe I should file a bug. or wait. 

Also compiled faiss from source; this looks like a very useful library!  Fast k-nearest-neighbors, with good example code via memorizing-transformers-pytorch.  
This alone should be sufficient to have the extended interaction. 

Spent some other time this evening looking at accelerated Smith-Waterman algorithms for alignment, e.g. for factorizing the library of examples: 
the target sequence would be the hypothetical library function, 
and the "genome" would be all the programs (and their traces, intermediate representations) that are to be refactored.  

January 18 2023

Fixed the CUDA install on the laptop -- had a 11.0 version hanging around, which was causing faiss to hang when jit compiling ptx code.  
meh, took too long to correct.  
But getting used to finagling cuda installations. 
	Could this be the same problem that's occurring on the desktop with DP / DDP?? 

Anyway, longer context: want to solve problems the way humans do, by repeated interactions & the ability to break chains of actions into their components.  
So, when we're converting a MNIST digit to code, first we produce some segments, then re-arrange the segments into curves, then convert the curves into loops or function calls, then adjust the parameters of the loops/calls to better fit the data... 

Key thing: always train on what actually happened = ground truth from simulator.  
Break episode up into small steps, if they improved some metric (e.g. implicit reinforcement learning). 
Small steps = sequence of edits then <fin> to indicate "run the simulator". 
	In the case that the program does not compile ... do nothing, same as current 'hallucinate'. 
	
Currently the system is trained to reproduce edits which result from taking diffs of arbitrary programs. 
- Some of these of course are impossible to invert and or the programs are poorly made / possibly chaotic; should gradually weed these out of the database (alternately: try harder). 
- Other edits go from nothing to something, e.g. 'A' is the empty program.
Instead of B being the output of a hidden program, it can be a MNIST digit. 
The transformer needs to emit some segments to try to reproduce it.. if successful (e.g. improved MSE match), then add to the database & train accordingly.  
	- Might need to make the database even larger? 
	- These entries can't be diffed, of course, because the generating program is unknown! 
	- They *can* be continued, though: context is then first round: 
		image A (empty) 
		image B (MNIST digit)
		program A (empty) --> 
			emit edits
		--> program C & image C & trace C (?!)
		
Ok, what to do with this extended context?? 
Clearly, we can somehow encode it all in a memorizing transformer -- and in the present, can fit much into length 512 context
** but: how to bootstrap? **
One mechanism: if the model comes up with an improvement ... then add it to the database, and later train on the full context (above) to produce the edits.  
	Hope: by training on small / simple examples, the transformer will "get the gist" of what to do in various situations. 
	Further: *probably* want to add noise to the system .. not really sure how (in the image?) to force the agent to explore properly.
		I guess this shouldn't be too hard. 
		Adding a feedback loop to explore -- something like "hammer" could be done with forms of memory other than a transformer, of course. 
Realistically: given the cache of programs, need to divide them into smaller bite-sizes so that the trasformer doesn't get confused when exposed to longer contexts.  That is, need to have a way of breaking the diffs up into minimum peices/edits, but applying them in longer contexts. 
Also might be right to do a bit of fuzzing on e.g. integer values? 
	Desirable to filter the database based on utility + understandability (see chaotic code above). 

Later: add in greater context through program-traces. 
Needed: some way of tagging the keys in the long-term memory so we know if it's from image or code or trace etc... hmm. 

January 23 2023

Note: this was a good read
https://komoroske.com/slime-mold/
	Organizations are like slime-molds: they can solve complex problems, but develop organizational headwinds to keep them from moving quickly.  
	These organizational headwinds arise from basic mathematical scaling. 
	Suggests some tips / ideas for keeping them (e.g. google) moving along.

Problem: in supervised mode, you'll be propagating gradients back.  
In open-ended generation mode, there is no supervised signal, so no gradients to propagate backwards! 
Makes me think that we need to have two processes running, not interleaved as now -- one supervised, for training the model, another unsupervised, for generating solutions to problems & sorting / storing them.  
Not exactly sure why I didn't think of this earlier; brain was too fatigued / distracted?  Well, anyway, I'm only human... 

Plan: 
-- One thread / GPU constantly training, on exact edits: the program and the image that it produces (delta), not the image that cued it. 
	No change from present. 
-- Other thread / GPU constantly hallucinating, on a mix of both open-ended problems (MNIST) and problems with a known solution.  
	The resulting programs are re-introduced into the database if: 
	-- The program generates a image/problem not in the database. 
	-- If the imnage/problem is in the database, the program has less or equal cost than those already there. 
		Want to keep around a database of program equivalents, for training a compressor. 
		Want also to cannonicalize program representation: if it is A, but the transformer reconstructs it as B, and B is a valid similar-length program, then put B at the head of the list. 
			Will need a generation-counter for each equivalent? 
	
So, one task is to split the ocaml program into two threads 
& the python into two instances -- with different mmap files 
	(mmap will be faster than decoding -> encoding into characters -> decoding the characters again)
& figure out how to synchronize model weights across python instances. 
(easy way: just periodically reload the file by restarting the script & save the weights periodically, as we do now.)

This does not solve the extended-interaction problem, though. 
Seems like we need a means of outputting variable-length edits: sometimes just add a 'move' command, sometimes add a for loop, sometimes a function call.. 
How to know when to stop editing and try running the program? PBE, program being edited? 
One answer, heavy, is to try running the PBE at every edit step.  Kinda unrealistic; doesn't scale as the programs get larger. 
Another is to just assume that the structure of the training data -- diffs used in supervised training can be required to be short and end in <fin>, so the model learns to make bite-sized edits. 
	That seems OK. 
	
But now there is a graph route-finding problem: given a starting point (say.. the null program), and some longer generated program, need to find programs that are working intermediates. 
Probably can use a smart datastructure for this, like prefix or infix trees? 
	Or e-graphs? 
Then, when generating training data for the transformer, can recreate the whole path *including intermediate output*. 

The alternative that I've been thinking about, is to use the hallucinate network with half-finished programs, including the greater context, see what it comes up with (garbage? The weights from memformer very-long-context will be large & affect output (probably) so you might never get sane output. 
Idk.  
The point of the transformer is to constrain program editing to things that are likely; when pushing the 'frontier of knowledge' so to speak, needs to be conditioned, like evolution, on things that have worked in the past. 
	Thus, the transformer can be thought of as a compositional copy-paster. 
	The compositionality is conditional on a memory of image / program context. 
Maybe weight-decay will take care of keeping context from unduly corrupting output, so that the edits are sane? 

January 24 2023 
"All professional athletes say it should feel 1/3 horrible, 1/3 ok and 1/3 great or you’re not setting high enough goals."
	
A general outline of how I’m thinking about the problem:
Programming is the process of iteratively converting a specification into executable code, e.g. a tree-structured list of actions & declarations that manipulate a state-machine to produce outputs\footnote{There may be wisdom in relaxing this constraint; see neuro-symbolic computing.}.  Programs can (to a degree) be considered a ‘minimum description length” of the specification: when combined with the internal mechanisms of the state machine, they reproduce the specification with a minimal of data + free parameters.  Human internal or verbal models ‘probably’ approach a MDL.  A programmatic MDL is causal: the program causes the output; in a weak sense causes in the real world relate to causes in the program.  (This sense can be strengthened via the scientific method.) 

Humans perform programming tasks iteratively, learning causes and effects as they go, and using this to further explore & attain goals.  This is the essence of memory-based inversion, or UDRL: causes -> effects are put into a database, and are used to train a neural network which learns to map effects back to causes.  When a new goal is presented, it is assumed to be the composition of a cascade of effects; by querying the database / model, these are inverted into a series of causes.  In the case here of a program, these causes are edits to the “program being edited”.  

UDRL or memory-based inversion only gets so far, though: experience and experimentation must be guided and purposeful; the space of possible programs is too large for random exploration.   
There are at many ways that humans encourage a degree of purposefulness in experimentation: 

1.  Reward.  Reward is a scalar-valued signal that orders states & actions with a ‘preference’.  Animals are excel at this.  \footnote{This is of course the domain of reinforcement learning (RL), which is typically considered via the formalism of a Markov Decision Process (MDP), in which states only depend on the previous state and the previous action; no further history is allowed.  This is limiting but useful.}  Programming definitely needs an element of this: you do not repeat actions that fail, crash, or produce chaotic outputs; you do repeat actions that lead to goals or ‘useful progress’.  
	1.1 Useful progress is difficult to codify.  A pragmatic stand-in is to compress outputs (effects) via a VAE, and measure the distance to goal in this compressed space: keep around and select actions that move you toward a goal. 
	1.2 Programs which solve goals can be reinforced implicitly by representing them more highly in the training data, e.g. how often given edits or program intermediates were used in the solution process.  This can be accomplished simply by using truncation selection of memories (‘replay buffer’ in RL parlance); a later option is to propagate credit backwards in time, ala Q-learning or TD-learning.  I do not think this is necessary at this point; implicit should be sufficient. 
2.  Aesthetics.  Very slippery term; seems to depend in humans on essences of symmetry (visual, musical), novelty, and simplicity, and often similarity to etiologically relevant stimuli, like the natural world.  Symmetry and simplicity can be approximated by putting a parsimony term in the program selection, and one could argue that we have it as a evolution-given heuristic for seeking the MDL of stimuli; novelty is implicit in keeping around only new/different effects.   
	2.1 Yet adding a real preference to any model for say rotational symmetry or fractal symmetry is hard and would require manual hacks.  
	2.2 Adding aesthetics beyond toy models could be very hard or very unnecessary; could be that interaction of multiple levels of hierarchy in edit production causes aesthetics to emerge.  
	2.3 Leave as a problem for later. 
3. Multiple levels of representation.  Pixels convert to lines; lines convert to serial commands; commands convert to loops, function calls, variable manipulations; functions are composed into classes and libraries.  The way that we are able to ‘make sense of’ or decompose the real world is through hierarchies (obviously).  This is a very strong prior to restrict the solution space, but these restrictions make search harder; neural networks circumvent the problem by fixing the hierarchy & making optimization a process of navigating saddle-points rather than local minima (c.f. programs are usually surrounded by mostly non-functional programs). 
	3.1 Multiple hierarchical levels make the cause-effect inversion easier.  Break the problem into multiple steps.  We do this by having the agent interact multiple times with the programming environment, and by supplying intermediate representations.  
	3.1.1 Intermediate representations are hard-coded for now, though.  
	3.2 Requires careful thought about the mechanism of representation, and how NN training occurs: presently, assemble longer interactions from paths through the edit-graph of programs.  This implies a planning module, which provides a bias toward ‘reward’ above, 1.  
4.  Debugging.  Humans use this to make the internal state(s) of the computer and program visible, which allows them to infer how to change the program to move closer to a specification.  
	4.1 Debugging is complicated to induce and train, as it’s a second-level action: it yields information on what subsequent action to choose.  In humans, it can be triggered by means-ends cognition, e.g. knowing what to instrument or look at depends on a search over the program tree / data flow, which can trigger it’s own search for information about how the program works...
	4.2 This + (1) suggests using some sort of planning or explicit search algorithm; with UDRL, search is implicit: you search over effects to select causes.  ‘Just’ using a transformer might not work so well here; instead, you’d want to implement a heuristic search algorithm, like A-star, wherein possible edits are enumerated, tried, and recursed.  
	4.3 Debugging plugs well into such a system, as it can be an explored action, which can inform subsequent actions in the search tree.. 
	4.3.1 And, of course, very very open ended debugging problems are fundamentally hard!  Which is why humans explicitly use behavioral cloning and indicator / suggestions, both via the internet, to figure out what to change & how.   We would have a really hard time figuring out how to use a new programming language without examples.  Templates matter greatly.  Also the meaning of names in the language matter, as they come with built-in action-association (something that a bootstrapping model will have to learn.)
	4.3.2 Might be that you don’t need debugging if the transformer has full trace visibility.  But this becomes onerous for realistically large programs. 
	4.4 At some point, though, they system needs to fall back on brute-force search / enumeration so as to not get stuck in perpetual loops.  Here we amortize this search through both a database and a neural network. 
	4.4.1 But, I think the lesson is: even if you only have brute-force search and a database, it must be able to solve the problem, albeit in possibly unbounded time; the NN is mostly a fast and and smart way of organizing this data and otherwise constraining the search to limit the space.  (See also DreamCoder footnote below)
	4.4.2 The dumb way (done now) is to just sample repeatedly, without local memory of what’s been done. 
	4.4.2.1 Seems you need many different levels / forms of memory, like a CPU. 
	4.5 DreamCoder has a stateful search mechanism via lambda calculus expression enumeration (enumeration is slightly harder with a DSL). 
	4.6 Likewise, MuZero and EfficientZero leverage Monte-Carlo Tree Search to do a bit of planning on small-ish action sequences.  Both these are stateful search. 
5. Libraries!  These are ways of dramatically reducing the search space.  
	5.1 In the DreamCoder paper, hidden in a footnote: “The difficulty of search during waking is roughly proportional to breadth^depth , where depth is the total size of a program and breadth is the number of library functions with high probability at each decision point in the search tree spanning the space of all programs. Library learning decreases depth at the expense of breadth, while training a neural recognition model effectively decreases breadth by decreasing the number of bits of entropy consumed by each decision (function call) made when constructing a program solving a task.”
	5.2 DreamCoder also uses a clever E-graph+version space data structure for handling the program refactors:  “This results in substantial efficiency gains: A version space with 10e6 nodes, calculated in minutes, can represent the 10e14 refactorings in Fig. 3 that would otherwise take centuries to explicitly enumerate and search.”
	5.2.1  Humans, of course, don’t have such a datastructure in their minds, and yet are well capable of refactoring code and creating libraries.  I think that this is a different form of compression; rather than compressing images to code, this takes populations of code, and compresses it by finding commonalities by looking at code or data flow.  
	5.3  It almost might be possible to refactor it (approximately) by doing a full code and data expansion, then gzipping it (or some other LZW compression), and looking what the symbols are.  Those symbols are likely library elements, and can be replaced in the program population. Conceivable also to use a neural network for this compression, ala the PAQ compressors. 
	5.3.1 This is a very superficial form of library generation; yet it is in some sense isomorphic to the first task and so possibly can be solved in the same way: but now the programs operate on programs themselves. 
	5.4 Another alternative is to operate on the code and dataflow via DNA aligners: look for replicated snippets.  Prefix tree representations may be used for this. 
	5.5 And of course research presses ever forward: https://arxiv.org/abs/2211.16605
6. Reprojection / recasting the problem / inventing new intermediate representations.  This is deeply interesting, and new intermediate representations are a cornerstone of creative programming, but all are well beyond the scope of current work (for now). 
7. Related is analogical thinking, which people like Hofstadter think to be the root of cognition.  Analogy fortunately is easy to emulate or approximate via vectoral spaces, hence again may be emergent in the neural network.
8. See also https://en.wikipedia.org/wiki/Problem_solving

Action plan: 
1 -- Bifurcate Ocaml-Python interface to allow for Training and Dreaming models.  Two sets of mmaped files.  2-3 days.  Maybe shift the decode to Ocaml.  Test thoroughly. 
2 -- For the dreamer module, add in not just imagined task, but also MNIST digits as targets -- eg. actual simple inverse graphics.  2-3 days.
3 -- Expand the Ocaml code to use Vector storage of program-image pairs; keep synced with torch; add in some sort of priority queue (reinforcement, above)
4 -- Add in VAE encoding to the Vector storage for ranking programs. 
5 -- Convert the VAE to a graph, where the edges are edits.  Use these to generate longer interactions with the programming environment, for transformer training.  
6 -- Examine how and where this is failing (?!?)
7 -- Now that we have a large population of programs, start factoring them into a library via https://arxiv.org/abs/2211.16605 
8 -- Switch to a memorizing transformer for more context, more intermediate representations.  

All this will realistically take me 1.5 - 2 months & only get us partway to the natural-language video game programming demo: we’ll still be inverting images, not videos.  I do think, if small-toy inverse graphics works, scaling  *should* be a matter of compute and infrastructure.  Here I think we want to hire other people; but there are bound to be sharp edges.  Regarding this, writing this forces consideration that the level of research required is higher than anticipated; the lego pieces are all there, but unlike Precision Neuro, where there have been POC ECoG arrays for a long time & so all steps can be enumerated for industrialization, certain elements, namely (5) debugging and (6) reprojection above are unsolved.  This project is also unlike the sewing machine (which became Neuralink, as you know) in that there you can envision all the steps needed to get to target, but maybe not all the details; here some steps like (5) and (6) have the more scary / thrilling essence.  

That does not invalidate the approach, though: the payoff is extremely large, the time is now, competition is heating up, and many other companies were founded on as much a dream as a demonstrated product (like OpenAI; who knew that you could get such amazing emergence out of basically one paper?  (And indeed they bounced around a fair bit, gaining experience as they went.)).  So, we have to do it & have to figure out how to pay for it & locate the right people to work with.  

The absolute critical thing is, as we have spoken about, process: you move to a goal, gaining information along the way, first manually adding information to the model but later *automating the process of information addition itself*.  Which, lo, is exactly what the model is doing!  Springtail is an AI company, but focused on bootstrapping and feedback; feedback which is omnipresent in biology and particularly the brain.   Unlike other information process companies, though, here the plan is that as high a feedback loop (in the abstraction sense) is closed by the machine, not humans.  \footnote{Aside: cognition itself involves a high degree of re-entrancy which is might be possible via neurobio-inspired model I’ve been noodling with, but again as mentioned best to cut teeth now on hard data.}


January 25 2023

Above text has moved here: 
https://docs.google.com/document/d/1W-sXMd7ZFqlrYDxZhUFfQutCUIjg6OxZXaQGfzHU7cM

January 26 2023

I just realized something obvious: by keeping around equivalents, we can train the transformer to make lateral (e.g. nullspace) moves!  
The equivalent programs can also be part of the graph of programs! 

January 27 2023

Hmm meetings ... socalization .. eats up time.  Eating eats up time too :S
Work for today: 
integrate with bidi mmap files & test, start line 827. 
check that the model is outputting correct code
add in mutex to 'db'
(later) longer episodes, finally! 

January 28th 2023

Need to get python & ocaml working together again before adding the dreaming python. 
todo: 
add edits into the database!!
select an MNIST starting point stochastically close to target character.
        For this, I think we need to include the VAE.. ?
stopped on program.ml:889 

January 30 2023

Mostly have the ocaml framework in place ... need to get it to compile, 
and then work on the python side (reverse mmap: python writes)
Todo: make sure thread state is initialized properly for the train and dream threads. 

January 31 2023

Bug in bedts!  Does not consistently fill out the whole array ... not sure why? 
