May 13 2021

The mode was (not so much is anyore..) decent at nulling the input neurons w inhibition, 
but is absoulte crap at the route-cause: it divergese to flat, all units identical.
It appears that rci (and, by extension, w_rci) degenerates ito flat before rce. 
Wait .. w_rci is not updated! It's ones - diag matrix, scaled. 
Yeah that might not be a good regularization... 
Also, with time rce becomes flat; starts out diverse enough. 
Ok, apparently the problem is that, in w_rc, weights corresponding to the input go to zero, while weights corresponding to output seem to saturate.  
This is definitey not the behavior we want, and is likely due to a combination of the different scales of the input->rc weights and the output->rc weights.  
wrc_update is simple hebbian outer-product of rc activation and concatenated ll & l2.
(aka only local information)
passgate update is similarly hebbian outer-product between in, out, and rc.  

Well,  what do we want?  
'rc' stands for route-cause: they should route (multiply) input to output (causes) conditional on both.  
Thus, it would seem that they need faster dynamics than the object neurons. 
But also one-hot dynamics?  Does this need to be iterated? Or does the w_rci matrix need to update too? Needs to be an asymmetry relative to the out neurons. 

June 9 2021

Testing the very simplified model with four input patterns, and only positive generator synapses, yields a network that (usually) predicts well (sometimes NaNs creep in there) -- but it's not sparse.  
One output neuron always takes care of the dominant mode of the data vector, the DC component i guess; it doesn't make sense to combat this, since the data is not independent! 
Should the network be whitening the data?  
In biology, sensory input is very typically whitened, often at the receptors themselves, due to adaptation. In ANN, this term is taken care of with a bias term (1.0).. 

On the one-hot four-pattern task, 
With dynamic inhibition: error mean 0.0570 / std 0.0146 rep 0.0471 std 0.0079
Without dynamic inhibition: error mean 0.0572 / std 0.0192 rep 0.0621 / 0.0201

So, the dynamic inhibition seems to be doing something 'useful'. 
But ... why is it not driving the error to zero? 
There are only four patterns, this should be possible.

OK so -- supplying the true information to l2 results in zero error, 
but only if the cube learning rule is turned off. 
If the true information is not supplied, and the learning rule is linear, then the error mean 0.0184 / std 0.0095 
This is with symmetric learning rates on the forward and reverse matrices. 

Making the backward learning slower than the forward makes the error greater. 
Making the forward learning slower than the backward makes the error less: 
tensor(0.0100) tensor(0.0096)
Making the inhibition learning faster than the forward makes error worse. 
Making the inhibition learning slower than forward makes error worse, too
tensor(0.0191) tensor(0.0099)
Increasing the backward learning to 0.013 slightly degrades performance
tensor(0.0122) tensor(0.0107)

If we extend the training time to 30k samples, Usually it solves the task exactly. 
tensor(0.0037) tensor(0.0063)
but sometimes it doesn't ...
If we increase the learning rates to 0.008 (forward) 0.008 (inhib) 0.014 (backward), it's again worse -- seems to occasionally fail (?)

This network, which can usually solve the four-pattern one-hot, cannot accurately solve the free pattern: 
tensor(0.0606) tensor(0.0065)
Why is this?? 

Can we solve it with linear algebra? 
Yes, clearly we can solve the linear section ('gen') not the inverse section ('invgen') -- this results in (unsurprsinigly) half the error: 
tensor(0.0352) tensor(0.0098)
@ 10k steps. 
Increasing the number of steps to 20k does not improve it much:
tensor(0.0293) tensor(0.0032)
it just reduces the variance. 
'Cheating' by supplying g2 to l2 pushes the error to zero, as it should. 
tensor(0.0009) tensor(0.0005)
further reducing the learning rate to 0.003 on forward weights @ 10k increases error: 
tensor(0.0532) tensor(0.0090)
same learning rates @ 20k: 
tensor(0.0218) tensor(0.0083)
@ 30k: 
tensor(0.0294) tensor(0.0061)

Ok will stick with 0.005 / 0.005 / 0.01 @ 10k. 

Increasing the input size to 16 (from 8) slightly reduces the error: 
tensor(0.0275) tensor(0.0052)
This makes sense as it's easier to estimate the 4 causes with more predictors. 

It does seem .. for this linear problem .. that disabling inhibition improves things. 
tensor(0.0284) tensor(0.0095)

What about changing the dimensionality of the output layer? 
size 6, 
tensor(0.0279) tensor(0.0076)
size 4 (same as generator)
tensor(0.0340) tensor(0.0091)
(of course then the network is doing much less compression..
zero compression should be perfect: nope!
tensor(0.0244) tensor(0.0055)

I guess now should think about multiple layers & further nonlinearities? 


June 18 2021

Fiddled around with the 2-axis task. 
With noise, we can reliably get solutions -- good!  I was discouraged last night. 
Notably, the other forms of homeostasis (slow homeostasis) don't work; they cause weight instabilities.  
This may be the cause of the problem with the other tasks.  
Still using the cubic hebbian learning rule on this binary task, 
since that worked so well for the XOR task.  
Also still using asymmetric learning rules, as discovered in the vectoral learning task; solutions are sensitive to these learning rates.  Doubling them breaks the network!  (And it does learn slowly already.. though I'bve not explored this extensively..)

This shows clearly how important small, interpretable, comprehensible tasks are.  The 2-axis task, I can solve easily 'by hand'. 
We should continue this development avenue with small numbers of inputs and outputs, while also testing on larger equivalent tasks! 

Note: linear hebbian learning rule does not work on this task. 
Note: inhibition is not required to solve the task, but it does not seem to effect the solution either. 
@ noise level 0.025, it does sometimes fail!  But this seems to be a sweet spot c.f. 0.05 or 0.002. 
Depends on the initialization..

Dec 2021 .. 

next test would be to see if, given both l2 and rcdes, the pg net can output l1i. 
Need to do that above.. 

yes, that seems to work as well, to a very high fidelity 
-- can predict l1 reliably with rc one-hot encoding. 
however however, it does not seem to set l2 to a static representation. 
will need to penalize (?) to get this behavior ? 

Dec 16 2021 
let's try one update step instead of five for improving continuity / smoothness in L2 representation.

Yes, one update step does work -- but it doesn't improve continutiy per se. 

Also what works: clamping the RC activation, but propagating the gradients fully through l2. 
If anything, this seems to work better than blocking the gradients. 
Works: error is ~ 5e-4 after 60k steps w batches of 8. 
Next, need to try propagating the gradients through all, including RC. 

++ Running the network twice, with no gradient-stop, and smoothing the activations of rc and l2 seems to further improve the performance of the network, without significantly slowing execution. 
batch size = 16.
slowloss ~ 2e-4 at 140k iterations. 
GPU load is only 2% when rc size = 14..

++ Trying to increase the size of the rc layer, see how that changes things. 
++ need to add a better form of logging -- eg print out gifs every 20k iterations or so. 
-- Huh, even with this very small network, am using an inordinate ammount of vram - 8gb!  what??!! 
-- Increasing rc size to 32 did not slow computation at all. GPU usage si now 3%, from 2%. 

Dec 22 2021

Tried a few things: 
-- clamping the PG weights to be [-0.001 1.0]
-- adding a ReLU to the matmul(w_, rc)
This revealed something interesting: 
the network has sufficient capacity to represent all stimuli just in the RC layer; 
(green); no l2 activation (black) is required!  
Black / l2, in this case, is all high / on. 
This is consistent with there being only 41 vision sensors, and the spatial frequency of the stimuli being well below f_s / 2 -- so, in practice, there are less than 16 independent degrees of variation in the data. 
Thus, even though this experiment 'failed', it revealed something useful:
we can't expect the network to generalize well if it doesn't need to. 
Loss gradually bottoms out to 6e-5 at 4e6 steps. 

Dec 30 2021

Tried dramatically decreasing the network size -- 4 l2 units, 8 rc units. 
Keeping the weight clamps resulted in L2 staying high all the time (near 1). 
Trying now with weights clamped to [-1 .. 1]
... 
This did not change L2, it's still always high. 
Try removing the ReLU when forming w2 from w_ and rc. 

Dec 31 2021

4 l2, 8 rc seems to be working (unrestricted..)
Let's try making the objects move faster so the contextual window is shorter! 
(Might need to add explicit set / reset..)
Increasing the velocity by 2.5x: 
velocity = (1.75 + randn() * 0.35) * scl * -2.5

Jan 1 2022
That seemed to make things worse!  prediction was more accurate and 'smoother' with the orginal speed.  
What if we increase the iterations? 
	from one to two. 
		obviously this is a good bit slower... 

Other idea: add dynamics to the synapses (or dendrites) themselves. 

Jan 2 2022

I've run the network for a long time now -- 
20440800 loss: 1.763e-03; (20e6 steps!)
And it does seem to be representing the 1D field with something that appears to be a shifter in the RC network, and something that appears to be more constant, in the l2 network. 
This is with two passes through the network in forward(self, inp)
And also spatial bluring on both l2 (2 passes) and rc (3 passes). 

I think, per 1999 Schmidhuber Feature extraction through Lococode, that we need to ammend the network such that the objective function forces sparse or factorial coding. 
Factorial coding is definitely what we want here ...
If we need to implement their rather complicated objective function modification, then so be it. 
First, I think, we need to replicate the 5 x 5 bar experiment, both with SGD and with their fancy modifications. 

Options: 
-- Change the objective function to improve source separation. 
	-- Problem: there is no guarantee that things will be topological; indeed it may be asking too much to assume that the arrived-at representation is topological... 
		-- Schmidhuber result did not find topological mappings
			It also hasn't been cited that much. 
		-- Well the cortex seems to do this just fine ... need to replicate some special sauce there! 
-- Add to the network additional layers that can un-permute the arbitrary mappings found by SGD
	-- Also consistent with above.  
