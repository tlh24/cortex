May 13 2021

The mode was (not so much is anyore..) decent at nulling the input neurons w inhibition, 
but is absoulte crap at the route-cause: it divergese to flat, all units identical.
It appears that rci (and, by extension, w_rci) degenerates ito flat before rce. 
Wait .. w_rci is not updated! It's ones - diag matrix, scaled. 
Yeah that might not be a good regularization... 
Also, with time rce becomes flat; starts out diverse enough. 
Ok, apparently the problem is that, in w_rc, weights corresponding to the input go to zero, while weights corresponding to output seem to saturate.  
This is definitey not the behavior we want, and is likely due to a combination of the different scales of the input->rc weights and the output->rc weights.  
wrc_update is simple hebbian outer-product of rc activation and concatenated ll & l2.
(aka only local information)
passgate update is similarly hebbian outer-product between in, out, and rc.  

Well,  what do we want?  
'rc' stands for route-cause: they should route (multiply) input to output (causes) conditional on both.  
Thus, it would seem that they need faster dynamics than the object neurons. 
But also one-hot dynamics?  Does this need to be iterated? Or does the w_rci matrix need to update too? Needs to be an asymmetry relative to the out neurons. 

June 9 2021

Testing the very simplified model with four input patterns, and only positive generator synapses, yields a network that (usually) predicts well (sometimes NaNs creep in there) -- but it's not sparse.  
One output neuron always takes care of the dominant mode of the data vector, the DC component i guess; it doesn't make sense to combat this, since the data is not independent! 
Should the network be whitening the data?  
In biology, sensory input is very typically whitened, often at the receptors themselves, due to adaptation. In ANN, this term is taken care of with a bias term (1.0).. 

On the one-hot four-pattern task, 
With dynamic inhibition: error mean 0.0570 / std 0.0146 rep 0.0471 std 0.0079
Without dynamic inhibition: error mean 0.0572 / std 0.0192 rep 0.0621 / 0.0201

So, the dynamic inhibition seems to be doing something 'useful'. 
But ... why is it not driving the error to zero? 
There are only four patterns, this should be possible.

OK so -- supplying the true information to l2 results in zero error, 
but only if the cube learning rule is turned off. 
If the true information is not supplied, and the learning rule is linear, then the error mean 0.0184 / std 0.0095 
This is with symmetric learning rates on the forward and reverse matrices. 

Making the backward learning slower than the forward makes the error greater. 
Making the forward learning slower than the backward makes the error less: 
tensor(0.0100) tensor(0.0096)
Making the inhibition learning faster than the forward makes error worse. 
Making the inhibition learning slower than forward makes error worse, too
tensor(0.0191) tensor(0.0099)
Increasing the backward learning to 0.013 slightly degrades performance
tensor(0.0122) tensor(0.0107)

If we extend the training time to 30k samples, Usually it solves the task exactly. 
tensor(0.0037) tensor(0.0063)
but sometimes it doesn't ...
If we increase the learning rates to 0.008 (forward) 0.008 (inhib) 0.014 (backward), it's again worse -- seems to occasionally fail (?)

This network, which can usually solve the four-pattern one-hot, cannot accurately solve the free pattern: 
tensor(0.0606) tensor(0.0065)
Why is this?? 

Can we solve it with linear algebra? 
Yes, clearly we can solve the linear section ('gen') not the inverse section ('invgen') -- this results in (unsurprsinigly) half the error: 
tensor(0.0352) tensor(0.0098)
@ 10k steps. 
Increasing the number of steps to 20k does not improve it much:
tensor(0.0293) tensor(0.0032)
it just reduces the variance. 
'Cheating' by supplying g2 to l2 pushes the error to zero, as it should. 
tensor(0.0009) tensor(0.0005)
further reducing the learning rate to 0.003 on forward weights @ 10k increases error: 
tensor(0.0532) tensor(0.0090)
same learning rates @ 20k: 
tensor(0.0218) tensor(0.0083)
@ 30k: 
tensor(0.0294) tensor(0.0061)

Ok will stick with 0.005 / 0.005 / 0.01 @ 10k. 

Increasing the input size to 16 (from 8) slightly reduces the error: 
tensor(0.0275) tensor(0.0052)
This makes sense as it's easier to estimate the 4 causes with more predictors. 

It does seem .. for this linear problem .. that disabling inhibition improves things. 
tensor(0.0284) tensor(0.0095)

What about changing the dimensionality of the output layer? 
size 6, 
tensor(0.0279) tensor(0.0076)
size 4 (same as generator)
tensor(0.0340) tensor(0.0091)
(of course then the network is doing much less compression..
zero compression should be perfect: nope!
tensor(0.0244) tensor(0.0055)

I guess now should think about multiple layers & further nonlinearities? 


June 18 2021

Fiddled around with the 2-axis task. 
With noise, we can reliably get solutions -- good!  I was discouraged last night. 
Notably, the other forms of homeostasis (slow homeostasis) don't work; they cause weight instabilities.  
This may be the cause of the problem with the other tasks.  
Still using the cubic hebbian learning rule on this binary task, 
since that worked so well for the XOR task.  
Also still using asymmetric learning rules, as discovered in the vectoral learning task; solutions are sensitive to these learning rates.  Doubling them breaks the network!  (And it does learn slowly already.. though I'bve not explored this extensively..)

This shows clearly how important small, interpretable, comprehensible tasks are.  The 2-axis task, I can solve easily 'by hand'. 
We should continue this development avenue with small numbers of inputs and outputs, while also testing on larger equivalent tasks! 

Note: linear hebbian learning rule does not work on this task. 
Note: inhibition is not required to solve the task, but it does not seem to effect the solution either. 
@ noise level 0.025, it does sometimes fail!  But this seems to be a sweet spot c.f. 0.05 or 0.002. 
Depends on the initialization..

Dec 2021 .. 

next test would be to see if, given both l2 and rcdes, the pg net can output l1i. 
Need to do that above.. 

yes, that seems to work as well, to a very high fidelity 
-- can predict l1 reliably with rc one-hot encoding. 
however however, it does not seem to set l2 to a static representation. 
will need to penalize (?) to get this behavior ? 

Dec 16 2021 
let's try one update step instead of five for improving continuity / smoothness in L2 representation.

Yes, one update step does work -- but it doesn't improve continutiy per se. 

Also what works: clamping the RC activation, but propagating the gradients fully through l2. 
If anything, this seems to work better than blocking the gradients. 
Works: error is ~ 5e-4 after 60k steps w batches of 8. 
Next, need to try propagating the gradients through all, including RC. 

++ Running the network twice, with no gradient-stop, and smoothing the activations of rc and l2 seems to further improve the performance of the network, without significantly slowing execution. 
batch size = 16.
slowloss ~ 2e-4 at 140k iterations. 
GPU load is only 2% when rc size = 14..

++ Trying to increase the size of the rc layer, see how that changes things. 
++ need to add a better form of logging -- eg print out gifs every 20k iterations or so. 
-- Huh, even with this very small network, am using an inordinate ammount of vram - 8gb!  what??!! 
-- Increasing rc size to 32 did not slow computation at all. GPU usage si now 3%, from 2%. 

Dec 22 2021

Tried a few things: 
-- clamping the PG weights to be [-0.001 1.0]
-- adding a ReLU to the matmul(w_, rc)
This revealed something interesting: 
the network has sufficient capacity to represent all stimuli just in the RC layer; 
(green); no l2 activation (black) is required!  
Black / l2, in this case, is all high / on. 
This is consistent with there being only 41 vision sensors, and the spatial frequency of the stimuli being well below f_s / 2 -- so, in practice, there are less than 16 independent degrees of variation in the data. 
Thus, even though this experiment 'failed', it revealed something useful:
we can't expect the network to generalize well if it doesn't need to. 
Loss gradually bottoms out to 6e-5 at 4e6 steps. 

Dec 30 2021

Tried dramatically decreasing the network size -- 4 l2 units, 8 rc units. 
Keeping the weight clamps resulted in L2 staying high all the time (near 1). 
Trying now with weights clamped to [-1 .. 1]
... 
This did not change L2, it's still always high. 
Try removing the ReLU when forming w2 from w_ and rc. 

Dec 31 2021

4 l2, 8 rc seems to be working (unrestricted..)
Let's try making the objects move faster so the contextual window is shorter! 
(Might need to add explicit set / reset..)
Increasing the velocity by 2.5x: 
velocity = (1.75 + randn() * 0.35) * scl * -2.5

Jan 1 2022
That seemed to make things worse!  prediction was more accurate and 'smoother' with the orginal speed.  
What if we increase the iterations? 
	from one to two. 
		obviously this is a good bit slower... 

Other idea: add dynamics to the synapses (or dendrites) themselves. 

Jan 2 2022

I've run the network for a long time now -- 
20440800 loss: 1.763e-03; (20e6 steps!)
And it does seem to be representing the 1D field with something that appears to be a shifter in the RC network, and something that appears to be more constant, in the l2 network. 
This is with two passes through the network in forward(self, inp)
And also spatial bluring on both l2 (2 passes) and rc (3 passes). 

I think, per 1999 Schmidhuber Feature extraction through Lococode, that we need to ammend the network such that the objective function forces sparse or factorial coding. 
Factorial coding is definitely what we want here ...
If we need to implement their rather complicated objective function modification, then so be it. 
First, I think, we need to replicate the 5 x 5 bar experiment, both with SGD and with their fancy modifications. 

Options: 
-- Change the objective function to improve source separation. 
	-- Problem: there is no guarantee that things will be topological; indeed it may be asking too much to assume that the arrived-at representation is topological... 
		-- Schmidhuber result did not find topological mappings
			It also hasn't been cited that much. 
		-- Well the cortex seems to do this just fine ... need to replicate some special sauce there! 
-- Add to the network additional layers that can un-permute the arbitrary mappings found by SGD
	-- Also consistent with above.  

Jan 20 2022
Spent yesterday trying to get MNIST working via the simplified feedback path. 
It works, sorta -- but is prone to instability, possibly chaotic instability. 
Dialing it back & trying a stimuli with a known distribution : just a bunch of gaussian bumps
It's still very unstable. 
But, l1e activations don't seem to decay to zero (at least)
(This problem should be trivially solvable with PCA)

Alright, it's working better -- but I can't be sure about the inhibition though. 
It takes a long time to wind up, and once it is wound up, it seems unstable / diverges. hm. 

Two current problems: 
-- The inhibition is sensitive to tuning / it does not completely abolish correlational structure in the sequence. 
-- Learning rate very much depends on the number of inputs, which determines the strength of the resulting activation. 
	While the first layer seems to have a broad plateau of stability, the second layer does not; it likes to either go to all-on E or all off E (input)
	Meanwhile, there are still modulations to S. 
	
Todo presently: 
	** Test & verify the function of inhibition to approximate the inverse covaraince matrix. 
	Might need multiple steps.. 
	** Make sure learning rate scaling makes sense no matter the input - output size of a neuron
	
Jan 21 2022
OK, the inhibition is decidedly not working the way we expect or want -- it does not reliably converge to the desired number of of underlying factors. 
(However, in combination with the feedforward hebbian rule, it might ...)
Nonetheless, keeping the inhibition in the network generally seems to improve network function. 

The first layer now is broadly stable, it just tends to have low-activity tuning artifacts, due to the nonlinearity in the hebbian rule: infrequent strong LTP compensates for frequent small LTD.  
Might be able to tune a meta-parameter to correct for this. 
	In the brain, I suspect this is additionally handled with space: synapses cluster together on dendrites, which spatially organize (maybe???) to allow for the nonlinear interactions suporting local filter maintenance / removal of non spatially concordant info.

Need to tackle the problem of learning rate depending on the number of inputs. 
	In the brain: most neurons of a given class have about the same number of synapses? 
Can't you just scale the learning rate by the 1/(number of inputs)? 
This would allow equivalent steps to the resulting activity. 

March 11 2022

Been awhile ... eh wells.  
In further pondering of the task, it seems unreasonable or difficult to force invariance transformations, such as translations, to adopt a one-hot encoding.  
In the case of a 32x32 input, 32x32 output, and one-hot 1D translation vector, this requires storage of a full 32MB translation tensor.  Quite insane.  

Instead, we know we have ~ 86e9 neurons, of which 19-23e9 neurons are in the neocortex.  
Allegedly the cortex contains about 1.5e14 synapses (150 trillion) .. I don't know why i seem to recall there being 100 trillion synapses in the brain. 
This equates to 7500 synapses / neuron, which is maybe in the right ballpark. 
Obviously there are a good number in the cerebellum, thalamus, hippocampus, etc.
Meanwhile, the cerebellum allegedly has 70 billion neurons (this doesnt add up?) 

Anyway, if we have a naive shifter architecture, there are 1k input neurons, 1k output neurons, 32 translation neurons, and 32M three-part synapses, which equates to 15k synapses / neuron.  
The problem is that the number of synapses scales as the third power of the input dimensionality, which clearly doesn't work
n synapses per neuron = (n * n) * (n * n) * n / (2*n*n +n) ~= n^3
Yeah, not ok. 

In comparison, using activity rather than space to encode location, you need like one operation to shift things around, and only a handful of synapses. 
	That said, learning in the dumb af expansion above is easy, whereas learning with activity = location is ???

March 15 2022
Some shower thoughts today:
-- Conjunctive features = signals that co-occur, and hence are easy to extract with nonlinear, Hebbian synapses.  This is done.
	-- This is similar to a linear "AND" gate -- signals co-occur commonly. Patches ala Olshausen. 
	-- The reason why we can't do anything else with conjunctive extraction on the second layer is that there are no more easily accessible statistics. (??)
	-- Discriminative learning does have purchase on these problems; it will push apart clusters with different labels & build up discrimination (=classification) in this way.  We don't have this luxury with unsupervised learning.
		Maybe we do have it with self-supervised learning?
		It certainly seems to work with GANs (very well) / VAEs (pretty well)
-- Need an equivalent to the "OR" gate, which can 'destroy' information ala a max-pooling layer. OR = invariance; ignore the patterns of variation within a weight matrix.
	Thinking: in a biological neuron, everything in the set of inputs that create a spike can be considered to be in the invariant set.
		This of course is nonlinear and sorta additive: two inputs that are both in the projection set will add sub-linearly
		-- A normal ReLU artificial neuron has a threshold hyperplane which divides up the set, and planes parallel to this division are invariant. (and planes on the left side are straight ignored!)
	In MNIST, when you train with augmented data (small translations and rotations), you explicitly 'teach' the network that these patterns of variation don't matter.
	-- This could be done in a different way, though: if you move your eyes around, one assumption is that the world has not changed, and hence that the observed patterns of variation you can OR over.
		The other idea is that the patterns of variation can be well explained by oculomotor behavior, head motion, or self motion.
		-- Want generated behavior to "explain away" factors of variation of the sensory input,
			And for motor-generated factors to train up a more general purpose extractor in the case of non-self-generated behavior.
			"Self supervised" in this sense, or at least "scientifically (experiments) supervised"

A priori, how do we know that certain axes are irrelevant?
	If we do know, then can move the discrimination plane in a perceptron to be parallel to these axes.
		There is the added complexity of inhibition & plastic inhibitory synapses; originally I was thinking of using these as purely sparsifying, but that proved to be non-essential.  They can of course effect the invariant planes (dramatically!)
	Without supervisory signals, there is no way to know what is irrelevant!? 
		Other than things that do-not co-occur (manifold learning)
	With supervision (eg. knowing all the digits in a given class), it becomes easier to know dimensions to be irrelevant, and to OR over them.
What if we approach it a different way?
	In a given digit, dark pixels say in the middle of the image can curve to the left or right or up; these depend of course on what digit the image is part of.  From the data, you can infer that completely random instances of dark patches do not co-occur; they tend to have higher level structure.  
	Why does the cortex_mnist.py not sparsely extract these features?  
	It should be possible just with the basic thresholding nonlinearity, right? 
		This would only implicitly start “oring” irrelevant dimensions.  But that should be OK. 
	The problem is that -- when there is an “or” of irrelevant dimensions, **we don’t know which one to recreate.**
		In the trivial sense, we have multiple translations or variations of the same digit. 
		If you know that one patch is active, then you can update your prior that neighboring patches are also active, conditional on these patches, and or conditional on higher-level features, hypothetically with a high degree of sparsity.  
	Then, with BTSP and-or STDP, you can ‘store’ the linkages (causes) of the activations, and thereby store information like location in a scene.  
		I would think that nonlinear interaction of synapses in dendrites to be a critical element of this algorithm. 

March 17 2022
For the AND circuit, i guess log-transform?  but then have to exponentiate..
really want:
	weight * AND ( outer ( input_vector ))
or even:
	weight * AND ( outer ( outer (input_vector), input_vector) )
In the shape task, just a single outer product should suffice:
the number of terms in individual AND expressions is two.
but more broadly, we need something with more expressive power,
something that allows arbitrary number of elements into the expressions.
(i'm thinking this mostly spatially, in terms of dendritic trees..)
but maybe for now we just stick with outer()..

If we want to "reasonably" approximate biophysics:
1 -- Expand the input vector length N by sampling, with replacement, Np times.
2 -- Take a given input vector and project through a random Npx3 matrix into 3d space.
3 -- Do the 1 & 2 for the next layer, size M and Mp.
4 -- These are the locations of the synapses. Run a partnering algorithm over the location to set up the presence - possibility matrix.
	(It's not all-to-all, but might be close in a sparse sense)
5 -- When simuulating, record the location as well as post-synaptic cell identity of each presynaptic potential.
6 -- Convolve the PSPs with a spatial (temporal) filter; if the 'membrane potential' exceeds some threshold (how is this adapted or learned??), then emit a dendritic spike.
7 -- Forward the local dendritic membrane potential to the soma / SIZ for the "OR" operation.

THe aspect which most impresses me is that the biological realizaton is smoothly sloppy: there is a continuum of sets of synapses which may interact nonlinearly, due to spatial location.
To accurately emulate this, would need to do a maybe 2d projection of sparse sampling w replacement of the presynaptic neurons, convolve this with a gaussian filter (discretized?), threshold.
	The convolution of course can be precomputed based on location and represented as a matrix-vector product (i think)

Maybe a way to emulate this sort of phenomena is to use hashing:
Np is hashed into Mp buckets.
nonlinear thresholding operates over the buckets (and learning also operates over the buckets?!)
This doesn't offer a continuum, but it might work well enough for binary-type problems.
For more spatial problems we can do the embedding and pre-bake a convolution matrix.
	You still need continuous variables tp project into..
		The convolution may help with generalization, I think!

Well, anyway, start with outer, maybe move to hashing, then maybe 2d
Need to make sure learning works.
And inhibition / inversion is nonobvious / has not be fully understood quite yet.
	Maybe it's easy when we have negative weights .. ?

