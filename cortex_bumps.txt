May 13 2021

The mode was (not so much is anyore..) decent at nulling the input neurons w inhibition, 
but is absoulte crap at the route-cause: it divergese to flat, all units identical.
It appears that rci (and, by extension, w_rci) degenerates ito flat before rce. 
Wait .. w_rci is not updated! It's ones - diag matrix, scaled. 
Yeah that might not be a good regularization... 
Also, with time rce becomes flat; starts out diverse enough. 
Ok, apparently the problem is that, in w_rc, weights corresponding to the input go to zero, while weights corresponding to output seem to saturate.  
This is definitey not the behavior we want, and is likely due to a combination of the different scales of the input->rc weights and the output->rc weights.  
wrc_update is simple hebbian outer-product of rc activation and concatenated ll & l2.
(aka only local information)
passgate update is similarly hebbian outer-product between in, out, and rc.  

Well,  what do we want?  
'rc' stands for route-cause: they should route (multiply) input to output (causes) conditional on both.  
Thus, it would seem that they need faster dynamics than the object neurons. 
But also one-hot dynamics?  Does this need to be iterated? Or does the w_rci matrix need to update too? Needs to be an asymmetry relative to the out neurons. 

June 9 2021

Testing the very simplified model with four input patterns, and only positive generator synapses, yields a network that (usually) predicts well (sometimes NaNs creep in there) -- but it's not sparse.  
One output neuron always takes care of the dominant mode of the data vector, the DC component i guess; it doesn't make sense to combat this, since the data is not independent! 
Should the network be whitening the data?  
In biology, sensory input is very typically whitened, often at the receptors themselves, due to adaptation. In ANN, this term is taken care of with a bias term (1.0).. 

On the one-hot four-pattern task, 
With dynamic inhibition: error mean 0.0570 / std 0.0146 rep 0.0471 std 0.0079
Without dynamic inhibition: error mean 0.0572 / std 0.0192 rep 0.0621 / 0.0201

So, the dynamic inhibition seems to be doing something 'useful'. 
But ... why is it not driving the error to zero? 
There are only four patterns, this should be possible.

OK so -- supplying the true information to l2 results in zero error, 
but only if the cube learning rule is turned off. 
If the true information is not supplied, and the learning rule is linear, then the error mean 0.0184 / std 0.0095 
This is with symmetric learning rates on the forward and reverse matrices. 

Making the backward learning slower than the forward makes the error greater. 
Making the forward learning slower than the backward makes the error less: 
tensor(0.0100) tensor(0.0096)
Making the inhibition learning faster than the forward makes error worse. 
Making the inhibition learning slower than forward makes error worse, too
tensor(0.0191) tensor(0.0099)
Increasing the backward learning to 0.013 slightly degrades performance
tensor(0.0122) tensor(0.0107)

If we extend the training time to 30k samples, Usually it solves the task exactly. 
tensor(0.0037) tensor(0.0063)
but sometimes it doesn't ...
If we increase the learning rates to 0.008 (forward) 0.008 (inhib) 0.014 (backward), it's again worse -- seems to occasionally fail (?)

This network, which can usually solve the four-pattern one-hot, cannot accurately solve the free pattern: 
tensor(0.0606) tensor(0.0065)
Why is this?? 

Can we solve it with linear algebra? 
Yes, clearly we can solve the linear section ('gen') not the inverse section ('invgen') -- this results in (unsurprsinigly) half the error: 
tensor(0.0352) tensor(0.0098)
@ 10k steps. 
Increasing the number of steps to 20k does not improve it much:
tensor(0.0293) tensor(0.0032)
it just reduces the variance. 
'Cheating' by supplying g2 to l2 pushes the error to zero, as it should. 
tensor(0.0009) tensor(0.0005)
further reducing the learning rate to 0.003 on forward weights @ 10k increases error: 
tensor(0.0532) tensor(0.0090)
same learning rates @ 20k: 
tensor(0.0218) tensor(0.0083)
@ 30k: 
tensor(0.0294) tensor(0.0061)

Ok will stick with 0.005 / 0.005 / 0.01 @ 10k. 

Increasing the input size to 16 (from 8) slightly reduces the error: 
tensor(0.0275) tensor(0.0052)
This makes sense as it's easier to estimate the 4 causes with more predictors. 

It does seem .. for this linear problem .. that disabling inhibition improves things. 
tensor(0.0284) tensor(0.0095)

What about changing the dimensionality of the output layer? 
size 6, 
tensor(0.0279) tensor(0.0076)
size 4 (same as generator)
tensor(0.0340) tensor(0.0091)
(of course then the network is doing much less compression..
zero compression should be perfect: nope!
tensor(0.0244) tensor(0.0055)

I guess now should think about multiple layers & further nonlinearities? 


June 18 2021

Fiddled around with the 2-axis task. 
With noise, we can reliably get solutions -- good!  I was discouraged last night. 
Notably, the other forms of homeostasis (slow homeostasis) don't work; they cause weight instabilities.  
This may be the cause of the problem with the other tasks.  
Still using the cubic hebbian learning rule on this binary task, 
since that worked so well for the XOR task.  
Also still using asymmetric learning rules, as discovered in the vectoral learning task; solutions are sensitive to these learning rates.  Doubling them breaks the network!  (And it does learn slowly already.. though I'bve not explored this extensively..)

This shows clearly how important small, interpretable, comprehensible tasks are.  The 2-axis task, I can solve easily 'by hand'. 
We should continue this development avenue with small numbers of inputs and outputs, while also testing on larger equivalent tasks! 

Note: linear hebbian learning rule does not work on this task. 
Note: inhibition is not required to solve the task, but it does not seem to effect the solution either. 
@ noise level 0.025, it does sometimes fail!  But this seems to be a sweet spot c.f. 0.05 or 0.002. 
Depends on the initialization..

Dec 2021 .. 

next test would be to see if, given both l2 and rcdes, the pg net can output l1i. 
Need to do that above.. 

yes, that seems to work as well, to a very high fidelity 
-- can predict l1 reliably with rc one-hot encoding. 
however however, it does not seem to set l2 to a static representation. 
will need to penalize (?) to get this behavior ? 

Dec 16 2021 
let's try one update step instead of five for improving continuity / smoothness in L2 representation.

Yes, one update step does work -- but it doesn't improve continutiy per se. 

Also what works: clamping the RC activation, but propagating the gradients fully through l2. 
If anything, this seems to work better than blocking the gradients. 
Works: error is ~ 5e-4 after 60k steps w batches of 8. 
Next, need to try propagating the gradients through all, including RC. 

++ Running the network twice, with no gradient-stop, and smoothing the activations of rc and l2 seems to further improve the performance of the network, without significantly slowing execution. 
batch size = 16.
slowloss ~ 2e-4 at 140k iterations. 
GPU load is only 2% when rc size = 14..

++ Trying to increase the size of the rc layer, see how that changes things. 
++ need to add a better form of logging -- eg print out gifs every 20k iterations or so. 
-- Huh, even with this very small network, am using an inordinate ammount of vram - 8gb!  what??!! 
-- Increasing rc size to 32 did not slow computation at all. GPU usage si now 3%, from 2%. 

Dec 22 2021

Tried a few things: 
-- clamping the PG weights to be [-0.001 1.0]
-- adding a ReLU to the matmul(w_, rc)
This revealed something interesting: 
the network has sufficient capacity to represent all stimuli just in the RC layer; 
(green); no l2 activation (black) is required!  
Black / l2, in this case, is all high / on. 
This is consistent with there being only 41 vision sensors, and the spatial frequency of the stimuli being well below f_s / 2 -- so, in practice, there are less than 16 independent degrees of variation in the data. 
Thus, even though this experiment 'failed', it revealed something useful:
we can't expect the network to generalize well if it doesn't need to. 
Loss gradually bottoms out to 6e-5 at 4e6 steps. 

Dec 30 2021

Tried dramatically decreasing the network size -- 4 l2 units, 8 rc units. 
Keeping the weight clamps resulted in L2 staying high all the time (near 1). 
Trying now with weights clamped to [-1 .. 1]
... 
This did not change L2, it's still always high. 
Try removing the ReLU when forming w2 from w_ and rc. 

Dec 31 2021

4 l2, 8 rc seems to be working (unrestricted..)
Let's try making the objects move faster so the contextual window is shorter! 
(Might need to add explicit set / reset..)
Increasing the velocity by 2.5x: 
velocity = (1.75 + randn() * 0.35) * scl * -2.5

Jan 1 2022
That seemed to make things worse!  prediction was more accurate and 'smoother' with the orginal speed.  
What if we increase the iterations? 
	from one to two. 
		obviously this is a good bit slower... 

Other idea: add dynamics to the synapses (or dendrites) themselves. 

Jan 2 2022

I've run the network for a long time now -- 
20440800 loss: 1.763e-03; (20e6 steps!)
And it does seem to be representing the 1D field with something that appears to be a shifter in the RC network, and something that appears to be more constant, in the l2 network. 
This is with two passes through the network in forward(self, inp)
And also spatial bluring on both l2 (2 passes) and rc (3 passes). 

I think, per 1999 Schmidhuber Feature extraction through Lococode, that we need to ammend the network such that the objective function forces sparse or factorial coding. 
Factorial coding is definitely what we want here ...
If we need to implement their rather complicated objective function modification, then so be it. 
First, I think, we need to replicate the 5 x 5 bar experiment, both with SGD and with their fancy modifications. 

Options: 
-- Change the objective function to improve source separation. 
	-- Problem: there is no guarantee that things will be topological; indeed it may be asking too much to assume that the arrived-at representation is topological... 
		-- Schmidhuber result did not find topological mappings
			It also hasn't been cited that much. 
		-- Well the cortex seems to do this just fine ... need to replicate some special sauce there! 
-- Add to the network additional layers that can un-permute the arbitrary mappings found by SGD
	-- Also consistent with above.  

Jan 20 2022
Spent yesterday trying to get MNIST working via the simplified feedback path. 
It works, sorta -- but is prone to instability, possibly chaotic instability. 
Dialing it back & trying a stimuli with a known distribution : just a bunch of gaussian bumps
It's still very unstable. 
But, l1e activations don't seem to decay to zero (at least)
(This problem should be trivially solvable with PCA)

Alright, it's working better -- but I can't be sure about the inhibition though. 
It takes a long time to wind up, and once it is wound up, it seems unstable / diverges. hm. 

Two current problems: 
-- The inhibition is sensitive to tuning / it does not completely abolish correlational structure in the sequence. 
-- Learning rate very much depends on the number of inputs, which determines the strength of the resulting activation. 
	While the first layer seems to have a broad plateau of stability, the second layer does not; it likes to either go to all-on E or all off E (input)
	Meanwhile, there are still modulations to S. 
	
Todo presently: 
	** Test & verify the function of inhibition to approximate the inverse covaraince matrix. 
	Might need multiple steps.. 
	** Make sure learning rate scaling makes sense no matter the input - output size of a neuron
	
Jan 21 2022
OK, the inhibition is decidedly not working the way we expect or want -- it does not reliably converge to the desired number of of underlying factors. 
(However, in combination with the feedforward hebbian rule, it might ...)
Nonetheless, keeping the inhibition in the network generally seems to improve network function. 

The first layer now is broadly stable, it just tends to have low-activity tuning artifacts, due to the nonlinearity in the hebbian rule: infrequent strong LTP compensates for frequent small LTD.  
Might be able to tune a meta-parameter to correct for this. 
	In the brain, I suspect this is additionally handled with space: synapses cluster together on dendrites, which spatially organize (maybe???) to allow for the nonlinear interactions suporting local filter maintenance / removal of non spatially concordant info.

Need to tackle the problem of learning rate depending on the number of inputs. 
	In the brain: most neurons of a given class have about the same number of synapses? 
Can't you just scale the learning rate by the 1/(number of inputs)? 
This would allow equivalent steps to the resulting activity. 

March 11 2022

Been awhile ... eh wells.  
In further pondering of the task, it seems unreasonable or difficult to force invariance transformations, such as translations, to adopt a one-hot encoding.  
In the case of a 32x32 input, 32x32 output, and one-hot 1D translation vector, this requires storage of a full 32MB translation tensor.  Quite insane.  

Instead, we know we have ~ 86e9 neurons, of which 19-23e9 neurons are in the neocortex.  
Allegedly the cortex contains about 1.5e14 synapses (150 trillion) .. I don't know why i seem to recall there being 100 trillion synapses in the brain. 
This equates to 7500 synapses / neuron, which is maybe in the right ballpark. 
Obviously there are a good number in the cerebellum, thalamus, hippocampus, etc.
Meanwhile, the cerebellum allegedly has 70 billion neurons (this doesnt add up?) 

Anyway, if we have a naive shifter architecture, there are 1k input neurons, 1k output neurons, 32 translation neurons, and 32M three-part synapses, which equates to 15k synapses / neuron.  
The problem is that the number of synapses scales as the third power of the input dimensionality, which clearly doesn't work
n synapses per neuron = (n * n) * (n * n) * n / (2*n*n +n) ~= n^3
Yeah, not ok. 

In comparison, using activity rather than space to encode location, you need like one operation to shift things around, and only a handful of synapses. 
	That said, learning in the dumb af expansion above is easy, whereas learning with activity = location is ???

March 15 2022
Some shower thoughts today:
-- Conjunctive features = signals that co-occur, and hence are easy to extract with nonlinear, Hebbian synapses.  This is done.
	-- This is similar to a linear "AND" gate -- signals co-occur commonly. Patches ala Olshausen. 
	-- The reason why we can't do anything else with conjunctive extraction on the second layer is that there are no more easily accessible statistics. (??)
	-- Discriminative learning does have purchase on these problems; it will push apart clusters with different labels & build up discrimination (=classification) in this way.  We don't have this luxury with unsupervised learning.
		Maybe we do have it with self-supervised learning?
		It certainly seems to work with GANs (very well) / VAEs (pretty well)
-- Need an equivalent to the "OR" gate, which can 'destroy' information ala a max-pooling layer. OR = invariance; ignore the patterns of variation within a weight matrix.
	Thinking: in a biological neuron, everything in the set of inputs that create a spike can be considered to be in the invariant set.
		This of course is nonlinear and sorta additive: two inputs that are both in the projection set will add sub-linearly
		-- A normal ReLU artificial neuron has a threshold hyperplane which divides up the set, and planes parallel to this division are invariant. (and planes on the left side are straight ignored!)
	In MNIST, when you train with augmented data (small translations and rotations), you explicitly 'teach' the network that these patterns of variation don't matter.
	-- This could be done in a different way, though: if you move your eyes around, one assumption is that the world has not changed, and hence that the observed patterns of variation you can OR over.
		The other idea is that the patterns of variation can be well explained by oculomotor behavior, head motion, or self motion.
		-- Want generated behavior to "explain away" factors of variation of the sensory input,
			And for motor-generated factors to train up a more general purpose extractor in the case of non-self-generated behavior.
			"Self supervised" in this sense, or at least "scientifically (experiments) supervised"

A priori, how do we know that certain axes are irrelevant?
	If we do know, then can move the discrimination plane in a perceptron to be parallel to these axes.
		There is the added complexity of inhibition & plastic inhibitory synapses; originally I was thinking of using these as purely sparsifying, but that proved to be non-essential.  They can of course effect the invariant planes (dramatically!)
	Without supervisory signals, there is no way to know what is irrelevant!? 
		Other than things that do-not co-occur (manifold learning)
	With supervision (eg. knowing all the digits in a given class), it becomes easier to know dimensions to be irrelevant, and to OR over them.
What if we approach it a different way?
	In a given digit, dark pixels say in the middle of the image can curve to the left or right or up; these depend of course on what digit the image is part of.  From the data, you can infer that completely random instances of dark patches do not co-occur; they tend to have higher level structure.  
	Why does the cortex_mnist.py not sparsely extract these features?  
	It should be possible just with the basic thresholding nonlinearity, right? 
		This would only implicitly start “oring” irrelevant dimensions.  But that should be OK. 
	The problem is that -- when there is an “or” of irrelevant dimensions, **we don’t know which one to recreate.**
		In the trivial sense, we have multiple translations or variations of the same digit. 
		If you know that one patch is active, then you can update your prior that neighboring patches are also active, conditional on these patches, and or conditional on higher-level features, hypothetically with a high degree of sparsity.  
	Then, with BTSP and-or STDP, you can ‘store’ the linkages (causes) of the activations, and thereby store information like location in a scene.  
		I would think that nonlinear interaction of synapses in dendrites to be a critical element of this algorithm. 

March 17 2022
For the AND circuit, i guess log-transform?  but then have to exponentiate..
really want:
	weight * AND ( outer ( input_vector ))
or even:
	weight * AND ( outer ( outer (input_vector), input_vector) )
In the shape task, just a single outer product should suffice:
the number of terms in individual AND expressions is two.
but more broadly, we need something with more expressive power,
something that allows arbitrary number of elements into the expressions.
(i'm thinking this mostly spatially, in terms of dendritic trees..)
but maybe for now we just stick with outer()..

If we want to "reasonably" approximate biophysics:
1 -- Expand the input vector length N by sampling, with replacement, Np times.
2 -- Take a given input vector and project through a random Npx3 matrix into 3d space.
3 -- Do the 1 & 2 for the next layer, size M and Mp.
4 -- These are the locations of the synapses. Run a partnering algorithm over the location to set up the presence - possibility matrix.
	(It's not all-to-all, but might be close in a sparse sense)
5 -- When simuulating, record the location as well as post-synaptic cell identity of each presynaptic potential.
6 -- Convolve the PSPs with a spatial (temporal) filter; if the 'membrane potential' exceeds some threshold (how is this adapted or learned??), then emit a dendritic spike.
7 -- Forward the local dendritic membrane potential to the soma / SIZ for the "OR" operation.

THe aspect which most impresses me is that the biological realizaton is smoothly sloppy: there is a continuum of sets of synapses which may interact nonlinearly, due to spatial location.
To accurately emulate this, would need to do a maybe 2d projection of sparse sampling w replacement of the presynaptic neurons, convolve this with a gaussian filter (discretized?), threshold.
	The convolution of course can be precomputed based on location and represented as a matrix-vector product (i think)

Maybe a way to emulate this sort of phenomena is to use hashing:
Np is hashed into Mp buckets.
nonlinear thresholding operates over the buckets (and learning also operates over the buckets?!)
This doesn't offer a continuum, but it might work well enough for binary-type problems.
For more spatial problems we can do the embedding and pre-bake a convolution matrix.
	You still need continuous variables tp project into..
		The convolution may help with generalization, I think!

Well, anyway, start with outer, maybe move to hashing, then maybe 2d
Need to make sure learning works.
And inhibition / inversion is nonobvious / has not be fully understood quite yet.
	Maybe it's easy when we have negative weights .. ?

March 17 2022

Thinking we might need a learnable bias here -- for the hidden variables (3),
you need their complement (boolean NOT) for conjunctive learning,
When learning starts, the magnitude of the hidden variables are small,
so the magnitude of the naive complements are large,
which skews the learning.
What if we just invert the signals and allow for negative weights?
No, negative weights did not work well in the past..
from this experience, staying with positive reals is better. (and biologically realistic..)
The other option is to modulate the inverse based on 2x the moving average.
So if the average is @ target, 0.5, then the inverse is the boolean NOT
If the average is lower, say 0.1, then the inverse is 0.2
	Maybe this is where plastic inhibition comes in
		It means that generally inverse variables are only local
		(GABA neurons usually don't have long axons)
		which is consistent with biology at least..

Ok ... that maybe works.  But the forward weights seem to be getting stuck!
Let's check.
Yes, definitely getting stuck. This is a primary problem.
Also a problem: hebbian learning rule from cortex_mist strongly favors sparse representations.
	The backwards network works perfectly.
	The weight matrix is not sparse, though.. it ideally should be only a few terms, but is about 20% full.
	Maybe this is ok?
		Changing the scaling on the cube nonlinearity does not seem to change much ...
		it learns the same statistics.
	Yet, when i write it out by hand, I don't need all these terms.
		It doesn't help that I haven't eliminated the 2x redundancy in the second outer() application.
Observation:
	Clearly, the bins are unnecessary.
	If a AND-product is on, then it can instantly sum, and we don't need ddendritic bins.
Other thought:
	In biology the weights are before the AND; here we have the weights after.
	Either one might be workable.
	Right now after seems to be sufficient: just make it work!
---
Wonder if even the most trivial gating (e.g. which dendrite has residual calcium -> enable LTP/LTD) might be sufficient for forward and reverse credit-assignment?

March 21 2022
Spent most of the day looking over Baldwin effect 1992 and Symmetry / Simplicity in evolved structures,
Then about an hour working on cortex_andor.py
Enabled supervision, and in this case - when the network is taught to encode the 8 different images in 3 bits, it seems to work just fine.
This experiment did uncover the fact that it's sensitive to the learning rate.
So, need to make sure that supervised learning works before proceeding to unsupervised learning.
Now running unsupervised, and the second layer has a strong tendency to get stuck in encoding schemes (= forward weight matrices) that poorly represent the data / poorly encode the space.
Clearly, need to think about this more, and possibly consider how 'errors' and 'reinforcement' is propagated around this (rather simple) two-layer network.
Will rest on it!
	Aside: with the supervised task, as time goes on the backward weights tend to sparsify = reverse projection is simpler hence more likely to be getting at the true structure of the data (?!)

March 22 2022.
So many twos!
Ok, need to work on the 'stuck' effect.
Supervised: yes it works, very quickly.
Unsupervised: seems to depend on the noise level; lower noise slows learning and gives more time for better codes?

OK, so reducing the problem to just two images (both in the upper left hand side) does not cause convergence.
it *should* just be a one-bit task, super duper easy.
	(the backward pass is still good, obviously)
Right .. it does seem to work just fine, with alternating presentations.

Ugh, the system is very sensitive to parameters.
For example, it doesn't work well if the allowable negative weights are clamped to
	0.25
	or
	0.0025
	but it works if it's clamped to
	0.025
	why??
		This is not necessarily a good sign;
		Might need to think about symmetries
			perhaps some levels of clamp allow escape w noise?

Reparameterized the network, to allow for 4 bits of hidden state ..
Seems to solve the 4-symbol task (sometimes .. hmm)
What about 8-symbol?
Still seems to get stuck!

What about adding push-pull capability?
E.g. the forward regresses toward the 'n' variable (with negation)
If there is strong residual error, you can push the weights in the opposite direction from the positive hebbian term:
just reverse the error term, and complement the hidden state.
-- This doesn't provide much more information, alas.
The problem is that if a unit is 'off', you can't turn it more off. Hah!

Definitely the problem is gettign stuck in local minima -- when iterating through data presentations, the + dw exactly counterbalances the - dw.
At least, with identical ordering of the stimulus.

Another problem: the system is not long-term stable, it seems to degenerate to a two-state represenation.  Hm. One thing at a time, now.

March 23 2022

Ok, trying to figure out this metastable / stuck problem.
<didn’t make it far, apparently.. 
think I need to just implement biology hah>

March 27 2022

Woke up this morning with a pretty clear idea of what needs to happen at the company / what the pitch needs to be: 

1. How do you ‘solve’ arbitrarily hard problems, perhaps problems with difficult scaling properties, e.g. large or unbounded action / perception dimensionality?  
1.1 RL works to some degree, and has ‘solved’ tasks like Atari (Agent 57, etc).  This is a very significant advancement.  But the action space is small (a few buttons), and the perception space is constrained (8 colors, limited resolution, 2D sprite world, simple game mechanics, etc). 
1.2 Humans obviously can solve these problems, and most importantly they become very effective at solving them. 
1.3 How? 
2. One avenue is that humans factorize distributions in both perceptual and motor domains.  Perception is perhaps most obvious: we factorize objects into attributes; the flip side of this process is the ‘binding’ problem, whereby attributes are bound into hierarchies of objects.  These attributes include important details like pose, lighting, etc.  (Details that Hinton’s Capsule nets / GLOM model try to get at..)
2.1 A deeper philosophical question is *why* the world tends to be readily factorizable into low-dimension components, almost universally (but see: our brain; and also see, gene regulatory networks)
2.2 Motor is sorta the opposite direction: the brain factorizes patterns of output over the space of muscles and their synergies.  
2.3 Of course there are a lot of additional details to both problems .. 
2.4 ... but an element that can readily solve both action and perception problems is a memory that can be indexed either by “output” or “input” (e.g. auto-associator, old idea here), and, most importantly -- to counter the curse of dimensionality and the aforementioned factorizable nature of the physical world -- the “output” addressing is factorized / disentangled. 
2.4.1 This in turn allows low-power learning algorithms (like RL) to have enough leverage to do things like fast-mapping (in dogs) and interaction with the physical world... and maybe even language / grammar?  
3. Very well, so we need a self-factorizing auto-associator.  I propose this is what the cortex is doing in mammals. 
3.1 And probably plenty of other things... but still. 
3.2 What is the cortex, then?  Laminar structure, with some columnar structure, notable input (layer 4) association (1, 2-3) output (5 & 6); each layer has ~80% excitatory neurons and 20% inhibitory neurons.  The inhibitory neurons have a great number of specializations, both morphologically, electrochemically, and gene-expression wise.  Three major classes: PV, SST, VIP.  Exitatory neurons in contrast are of fewer classes and firing properties (at least, to my limited knowledge).  The excitatory neurons project, the inhibitory neurons generally do not, though there are exceptions here. 
3.3  Strong motifs in the cortex: 
3.3.1  E-I balance.  Most neurons, of both polarities, are in a constant state of push-pull, where usually excitatory and inhibitory input balances
3.3.2  Hebbian or local-type plasticity: synaptic ‘weight’ or coupling changes happen based on almost-entirely local changes, with modulation from retrograde signals, neuromodulators, iternal signaling pathways, etc. 
3.3.2.1  Some of these pre & post changes are very quick, so called behavioral timescale plasticity.  
3.3.3 Dales law: neurons generally emit one primary neurotransmitter; glutamate or GABA.  This of course has exceptions & many release multiple neurotransmitters -- but is a useful starting heuristic. 
3.3.4 Highly nonlinear membranes, inclusive the dendrites.  Some (many?) dendrites have dendritic spikes / plateau potentials, which can change the way signals are transmitted to the soma / spike initiation zone.  
3.3.5 1k - 10k synapses per neuron, roughly.  Most synapses have about the same ‘weight’, and the distribution of these weights is not agreed upon but does not have the resolution of 32-bit or even 16-bit floating point numbers.  (Nvidia suggests 8-bit logarithmic..)
3.3.5.1 Yet synapses certainly have more state! 
3.3.6 (Most obvious) The same rough cytoarchitecture throughout, with specializations say in auditory cortex (faster firing rates / high resolution timing information?) or in striate cortex (axons, e.g. -- probably to represent long-distance spatial relationships?)
3.3.7 Speaking of timing, there is a preponderance of spike-timing dependent plasticity.  Though the degree to which this exists, and how it works in the overall function of the cortex, is not agreed on.  STDP likely depends on much more than just pre-post timing; pre-post-pre and post-pre-post triplets have their own changes. 
3.3.8 Prediction *seems* to be a common motif, particularly for sensory encoding. This has obvious utility for animals interacting with the world (or simulating it, in the case of cognition)
3.3.9 Most feedback is local and low-order (in the poles and zeros sense).  Homeostatic effects are ubiquitous in biology; it’s just a good way to build a system. 
(.....)
4. OK, how to synthesize all these disparate details?  
4.1 It’s a feedback control system that tries to compress and predict (aka represent) the “input data” (say vision). 
4.2 Compression is via factorization, per above, to fight the curse of dimensionality. 
4.3 Factorization allows for comprehension, interaction, and manipulation. 
4.4 This is learned in a wholly *unsupervised way*
4.4.1 This is yet an unsolved problem in machine learning!  
4.4.1.1 But is it partly solved?  Seems like there isn’t a scientific consensus.  High-dimensional vector representations definitely work (see Dall-E 2, PaLM; each of these are transformers and they do seem to compose the physical world (visual or linguistic) pretty well, with perhaps limitations due to their feed-forward structure hence what relationships that they can represent. 
4.4.2 Sure, these self-supervised transformers seem to get at it, albeit indirectly, and they are sample / data / energy inefficient (ish ... they work). 
4.3 Yet transformers have a key to the puzzle: they multiply activities, not just weights by activities.  Hinton pointed this out (as I’m sure other people have earlier).  The remarkably impressive Nvidia StyleGAN 1 & 2 also relies on activity-activity multiplication.  
4.4 Activity-activity or activity-weight-activity multiplication is possible if not easy in a biological neural network; indeed many of the nonlinear dendritic effects (spikes) and morphology (inhibitory synapses on the spine neck) directly support this. 
4.5 I propose that (this is not unique) dendrites act as both activity-weight products (linearly summing the dot product between incoming activities and synaptic weighs) and activity-weight-activity products (taking the morphology-aware product of activity*weight groups, and (possibly nonlinearly) summing that to the soma.  
4.5.1 Probably this offers a continuum: as the connections grow, their nonlinear summation / multiplication increases. 
4.5.2 The seemingly random encoding possibly works as a hashing function, which has been shown to be super effective in eg super-resolution and NERF training. (of course part of this effect is the adroit mapping to hardware.)
4.6  Put these things together: it should be possible to make a feedback control system (E-I balance) that uses only local learning (biologically plausible), *_a continuum between weight-activity and weight-activity-activity multiplication_*. 
4.6.1 Gradients not required!  Can use feedback-alignment trick: the forward and reverse (perhaps higher order) weights can be encouraged to mirror each other.  This effects a degree of weight transport or error propagation; fun things like visual illusions may well be the result of slow bottom-top-bottom feedback loops oscillating.  (This also suggests that feedback readily spans layers)
4.7 And this is what I’m working on now! The race is most definitely on; so many bright people working very hard in this field.

April 1 2022

Doing effectively regression testing. 
It seems -- very importantly -- that we need one 'bias' bit, to allow explaining DC activity, without the limitation of the PID activity controller.
Yeah, that didn't change anything; there were already always-on bits by virtue of the complement system. 
So, there has to be something else keeping the error from going all the way to zero? 
From inspection, it looks like the center pixel is oscillating between positive and negative changes... 
Hm. I guess we don't really need the DC term.  
The middle pixel is on 5/8 of the time, which is kinda a weird cycle. 
it's 1,2,3,4,7
Let me see if i can make the system more expressive .. ? 
Meh, going in circles a little bit here. 
Think I need to rewind & think concretely about the task / what sort of codes need to be extracted. 
It could be that I'm working on a far too difficult task, where everything is perfectly entangled & unblanaced to boot... 
We have the representations sliding around well, check.  But this seems to be an effect of noise, not directionality; the way to get the 4-symbol task to work is to anneal the noise with time, which is not a good sign! 
If you leave the noise variance constant, it's always bopping around (albeit with some positive convergence properties; the solutions / encoding are ''close'' just not perfect. 
:-/
(...)
Maybe need separate noise on the forward and reverse passes? 
..
At least I've improved the plotting. 

April 13 2022

Back from CSHL NAISys -- mostly finished intake of references from all the talks.  Trying desperately to synthesize all the data presented; quite a wide variety of studies to build upon (and plenty that might be pure distractors..). 

Overshadowing all was the presentation by Blaise Aguera y Arcas which described, at a survey level, the very recent advancements made by Google’s PaTH and OpenAI’s Dall-E 2.  Both are foundation models, with huge numbers of parameters, and massive training sets.  Path was trained on 45TB of cleaned data, has 500B parameters, and was trained across 6144 TPUv4s.  Rather than offering key theoretical insights or methodological breakthroughs, the model seems to be succeeding purely due to scale and engineering effort.  A lot of people have put a lot of time into this, the silicon, JAX and XLA frameworks, compute resources, cleaning the data, and making all the details of the model stable and train well (e.g hyperparameters, batch sizes, network widths, parallelization.. deeply inspiring when you think of it all!)  (Aside: seems  daunting, but best to have a youthful attitude here and excess of confidence.)  Furthermore, Blaise suggested that these are yet again only the beginning, and even better models will allow for comprehension / modeling / generation of multi-sensory data, including video and self motion.  

These models are transformative, and some at the conference were quipping “it’s been solved! what do we do now?”, yet many questions remain: 
* Is there a more efficient way of training a model from this data?  PaTH cost something like $15M to train, while the brain appears to have much more in the way of priors hence data efficiency (and power efficiency -- though this is more a hardware
factor).  
** While it is hard to directly challenge backprop, as historically & mathematically there seems to be nothing better than following the gradient, provided that a problem can be formulated as an optimization.  Had a long discussion with Brian Cheung about this at one of the lunches at CSHL; I argued that breaking the global problem into a hierarchical optimization or “data organization is at least more representative of what the brain is doing, and should increase parallelization & data locality (but does not avoid the coordination problem).  With SGD, data needs to flow forward while recording activations, then backwards; the two can be interleaved but it requires some shoehorning.  The brain does not have this form of locking, so far as we know.  
*** There is the example of brain rhythms, but conflicting evidence here: according to someone at the conference (I forget who), some people don’t have an alpha rhythm.  Yet other studies have shown that the phase of microstimulation with respect to the theta rhythm affects memory encoding. 
** Furthermore, the whole idea of using SGD in a self-supervised method -- which is 99% of what foundation models are doing -- also seems like shoehorning.  Certainly there is a more direct way of going about it, and arguably this is what the brain is doing. It doesn’t need gradients in the same way as it’s occupied with storing and generalizing data directly.  (Or rather: this happens as an emergent phenomena of what it does, just like it happens with SGD and MLPs as an emergent phenomena / due to the simplicity bias.  
* Is there a better way of representing statistical dependencies?  Transformers are brilliant in that they natively can encode two-factor conditional probabilities (and hence in many-layered networks, can represent conditional functions of arbitrary numbers of terms), but I think this is a general instance of activity*activity multiplication combined with weight*activity projection.  Both of these individual neurons, or even individual dendrites, are capable of representing both [], though this is just now being explored for deep learning.  
** Biological neurons seem to be able to extract invariances and equivariances, as opposed to ANNs where convolution determines weight sharing and                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               and invariance is approximated through MLP layers or max-pooling.  So far as I know, this remains an open problem, one that is related to but probably not to be confused with disentanglement, factorization, and compositionality.  
* The transformer architecture is feedforward, which makes inference and backprop simpler (recurrent networks are harder to run backprop on), but at the same time this limits their computational expressivity.  Psychophysics has shown that the brain solves some problems in a parallel fashion (e.g. search for outliers in an oriented field of lines), and some problems serially (search for a particular shape or object, usually).  That said, this limitation may be justified as the training demands are already excessive. 
* Data is fed during training in a random order, and the learning agent does not directly interact with it; it cannot perform experiments like a child.  These sorts of natural experiments are very likely to ‘’dramatically’’ simplify problems of disambiguation and disentanglement, but joint sensory-motor learning is, as mentioned, just beginning to be worked on. 

In aggregate, it ‘’should’’ still be possible to improve machine learning models with insights from biology and theory, and these improvements can go to the ‘’bottom’’ of the technology stack. Otherwise, most changes will be at the ‘top’ of the technology stack, eg scaling.  This circulates back to what to change, and how to change it.  

* Backprop.  Many many people have been working for a while on this and - lo! - it is possible.  There a large variety of alternatives now to backprop, like feedback alignment, brainprop[], error correction through emergent phenomena [], eligability traces / use of auxillary variables, RPE-gated plasticity etc, but none seem to approach the simplicity and generality of backprop + SGD.  Not a small factor in this is that the chainrule is built into several highly performant ML packages, and most problems are ordinal and can be formulated as an optimization (they are duals). 
* Network topology.  I actually think that, if you stay with backprop, there are incremental, moderate gains to be had here.  Definitely worth the engineering effort.  For example, Nvidia presented using 8-bit logarithmically-scaled weights to reduce weight storage / bandwidth.  (This was biologically motivated, too.).  
** If you forgo backprop, the sky is the limit!  But lack of constraint means that exploring the space and selecting starting points is hard.  Wolfgang Maass’s Probabalistic Skeleton work is inspiring here: sure it took a supercomputer, but evolutionary strategies was well capable of finding robust solutions to make an ant walk.  (With zero learning as well)
*** I think their cleverness was to make the encoding straightforward: a vector of developmental weights, which create the probabilities of connection, which creates the overall computational structure. 
* Training schedule.  Make it closed loop, as mentioned.  Very open and interesting field here.  Maass’s probabalistic skeletons 

The world is hierarchically structured, the brain is also hierarchicially structured and modular; want to be able to edit the data flow graph accordingly (which will perhaps take some cleverness). 

April 14 2022

PS2
	Fixed parameters
		num neurons
		num neuron types
		num dendritic branches
		Topological dimensions available
		num synaptic learning rules (blends?)
	Variable parameters
		Instance locations (N-d space... ? )
		Instance connectivity (very sparse)
		Instance synaptic location / coupling (also sparsre)
		Instance types

Steps in evaluating a given PS
	Childhood, N steps
		Episode, K steps
			Set the input neurons to image
			Propagate signal around
			Update synaptic weights
	Compression test
		Inner, J
			Set input to image
			Record input, output, true encoding
		Linear regression to see how well the compression works.
			(might need a slightly better disentanglement metric here)
	Prediction test
		Inner, L
			Set input to corrupted image (known image + noise)
			Record error / novelty neurons
		Straight MSE over all examples


Thinking about representing the whole recurrent network as a sparse matrix multiply, which of course simplifies forward propagation by a lot!
Then the development simply entails unpacking the code (which presumably will be simple MLPs over input space) to fill the matrix.
This should set the default synaptic weight / presence thereof.

But then how do we represent weight changes / synaptic plasticity?
At present it's a factor of pre and post activity, exclusively.
In practice it's a function of pre, post, neuromodulator, Ca+2 in dendrite / spine, eligability trace, ???.
	(biology also likes to gate presynapses.  Not here, not yet; can do with postsynapse)

Simulation Flow:
	presynaptic activity is broadcast to all axon terminals
	postsynaptic activity is computed by multiplying by weight
	postsynaptic activity is computed as a function of input and space (e.g. threshold function)
		I'm not sure about this .. segments of dendrites are supposed to implement the AND gate, and the overall tree implements OR, but ..
		(Needed boolean gates:
			AND, OR, NOT, PASSGATE.
			MUL, ADD, INV, IF.
				(FOR, WHILE, *PTR, TEMP, LET, LET REC, .. the standard substrates of computation.  Only we want this to be meta-computation.)
		Passgate = PV spine neck inhibition.
	Postsynaptic net activity is computed
	Synapses are updated based on rule..

Synapse update basically means a kernel which takes in the various factors and spits out a delta, same as what we did earlier in Mojave musings in JAX.  (could be faster..)
This means using kernels, rather than sparse matrices.
Then development is basically populating the 1-D array of synapses, aka the COO sparse matrix format.
Lots of scatter-gather operations.. meh, so long as it can be made embarassingly parallel, its ok!
	Seems like it's conceptually simpler to do gather operations, so will thing this way for now..
Dimension for iterating:
	1. synapse number along dendritic segment
	2. dendritic segment in neuron
	3. neuron in organism
	4. organism in population

So, a 4-d tensor of synapses (which can be reprojected into 1-D, of course..)
Simulation Flow, again:
	+ Update activity
		- Over all synapses, perform GET operation for the presynaptic activity from the vector (matrix, population) of activity.
		- Over all segments, perform nonlinear sum (multiplication). Emit Ca+2
		- Over all neurons, compute output, and write to vector (pop matrix)
	+ Update synapses
		- Over all synapses, perform GET operation for pre, post (dendrite, soma).  Emit weight update. Emit state update (as per jax implentation).

I really have no idea how well this all will work, or if it will work; need to scale gradually, starting from very toylike models. (glider task!)
Suppose it makes sense to do it in jax, for sure.
	Will have to turn off the gradient accumulation.

April 30 2022

Have the JAX code up and working!  Though it does not seem to be evolving complex behavior...
I think one problem is that connection probability is necessarily normalized -- neurons 'have to' be connected to other neurons, instead of defaulting to no connection.
Defaulting to no connection / making the probabilities absolute should fix this, but we'll need a 'null' connection & need to think of how to implement it.

Also, it's not clear why the learning rules & biases are all clustered around zero.  These are effectively under brownian motion, so should not matter at all; they should only be constrained by the limits, yet they do not seem to bounce into them at all .

May 2 2022

OK need to present Springtail! 

The situation has been evolving (pun intended?), and it’s worth going back and reflecting on the path. 

Last year: 
-- Current ML methods adopt a grossly simplified version of what synapses & dendrites are.  There are three areas rife for disruption: 
--- If we were to add behavioral timescale plasticity, it should serve well as ephermenal ‘binding variables’, much like the ‘attentional’ variables in the now-prevalent Transformer architecture.  
---- Notably, this is very much at odds with the current ML approaches, which adopt high-dimensional vector representations.  (Are these really so bad? If we have only a few examples, say one positive, one negative, it’s easy to measure the difference between the two: it’s a vector.  And, if the representations are truly disentangled, then there won’t be much per-class off-axis variance.  That is, the vector will be invariant and true. Hmm.  So there is not really such a problem with vectoral representations, which is much more consistent with modern ML approaches.  Provided, of course, that they are disentangled. 
--- This also solves the (infamous) binding problem: if neurons are feature detectors, and their activity represents the presence (or absence) of an object, how do you represent the linkages between parts to form a whole? 
--- Dendrites are inherently nonlinear, and have plateau potentials & forward / back-propagating spikes.  This can flexibly serve to allow for activity-activity multiplication, which again is an essential component of Transformers.  It should also support some basic logical operations: if the soma (summation node) is ‘or’, then the dendrites can be ‘and’.  (Add in ‘not’ and you can express any boolean logic.)
--- The nervous system is notoriously closed-loop; the balance of excitation and inhibition is just one example of this. In comparison, most machine learning models are sequential, with one feedback loop: that of error propagation through SGD.  Finer granularity in feedback permits much higher level of parallelization, and supports perceptual-infill (e.g. visual illusions; the creation of an approximate scene graph in the first place.  


-------
Random thoughts: 
* Why do this at all?  why not just allow for the current pace of ML to continue?  It’s hyper competitive, which is super bad.  Lots of plenty bright, ambitious people working in the are.  And, as the CSHL NAIsys conference showed, it’s all different bits of optimization and search; one of the reasons that academic projects don’t get so far is that they simply don’t apply enough effort. 

Generally the idea is that biology supports higher levels of parallelzation and sample efficiency.  E.g. it seems like a bit of a shoehorn to use SGD with self-supervised methods.  But then again, this works!  Also, the degree of invariance compression / equivariance extraction in ANNs is still somewhat weakly defined.  For the 

In comparison, the brain basically 

May 3 2022

Well I seem to have stopped on that last part and got back to work on skeleton.py .. ahhah
All tests with population of 3k.

train_jit:
	1.906 sec / iter.
train no jit but jit on sim_step_update and no scan:
	1.16 sec / iter. (faster!!!)
train no jit and no jit on sim_step_update:
	1.16 sec/iter (interesting -- does jax jit everything internally?)
train with jit, no jit on sim_step_update:
	1.88 sec (??!)
train with no jit, no jit on sim_step_update, but scan on:
	1.18 sec or 1.17 sec
		*** this is the winner.
		( slightly slower if sim_step_update is jitted )
(double check) train no jit, no jit on sim_step_update, no scan:
	26.46 sec (!!!)
(double check) train no jit, sim_step_update_jit:
	1.91 sec

eval_gar_jit:
	3.37 sec
eval_gar no jit:
	1.42 sec

POP is 7680
float32:
	train 2.412 make_pop 0.003 eval_gar 4.596
float16:
	train 0.795 make_pop 0.0006 eval_gar 2.574

Ok so ... we need to figure out how to make float16 work!
Much much faster and or can run larger models / larger populations!
	(might well be limited by memory bandwidth here..)

Note note:
Needed to add the following lines to the kernel boot flags to allow the (nearly) full memory of the 3090 to be used:
> sudo nano /etc/default/grub
GRUB_CMDLINE_LINUX="pci=assign-busses,realloc,hpmemsize=128M,hpbussize=0x33,hpmmiosize=128M,hpmmioprefsize=16G"
> sudo update-grub
On the 11Gb 2080Ti, I can get a 3k population size before running out of memory;
On the 3090 w/ above, it's possible to get 6k, with the current neuron type configuration.
(No changes were made to bios).
To get nvidia-settings to display something,
sudo nano /etc/X11/xorg.conf.d/80-igpu-primary-egpu-offload.conf
:
Section "Device"
    Identifier "Device0"
    Driver     "modesetting"
EndSection

Section "Device"
    Identifier "Device1"
    Driver     "nvidia"
    BusID      "PCI:05:0:0"                 # Edit according to lspci, translate from hex to decimal.
    Option     "AllowExternalGpus" "True"    # Required for proprietary NVIDIA driver.
EndSection
:
That all seems to work and is better than looking at nvidia-smi.
Success!
Now need to encode an objective function for the 'output' neurons.
(& this one needs to be rotation invariant)


May 5 2022

Tried out pure simulated annealing overnight, best solution seems to be:
correct 42.475266 err 40.931347 out 26.354053  /  109.76066
Let's compare this to search with recombination:
seems to oscillate a whole bunch
correct 87.72359 err 58.33085 out 31.88125  /  177.93568
take that solution and switch back to simulated annealing
Doesn't seem to be working any better!
Annealing is also oscillating around 170-180 total loss.
correct 87.258545 err 59.643234 out 31.868845  /  178.77063

May 6 2022
After reading a bunch of Kenneth Stanley's papers, I've concluded that we need some form of novelty search -- to push the networks out of local minima.
Propose doing this by recording activity on the evaluation phase, storing it in a large matrix, and sorting this matrix based on self similarity over time..
Problem is that we *probably* need speciation or so ... novelty will encourage diversity, but crossover and recombination will collapse this through in-mixing (??)
Will still need to address the problem of recombination: how to keep effective cabals / coordinated genes around. hmm.

May 9 2022
Can we get rid of the only-positive acitivities limitation?
That's how ANNs work .. but of course it's not how BNNs function.
It would simplify and streamline things considerably, of course: the rules could allocate however many excitatory and inhibitory neurons as needed.

The limitation is seeing if the learning rules can be stable / or how to formulate them so that they are stable.

It likely makes sense to revisit some of the old experiments of hand-crafted 'cortical' networks to see what and how the subtraction is happening.

(This thinking is.. )

May 11 2022
cortex_andor3.py seems to be (finally) working ok.  Critical advancement was to add both nonlinear term expansion as well as compression, using nonlinear least squares.
It solves the checker-glider problem when the output /address space is 4 bits, but sometimes gets stuck when the output space is only 3 bits (which is the minumum required to describe the stimuli..).
Let me try just running the thing for longer, maybe it will get unstuck due to symmetry-breaking & nonlinear hebbian magic? hhm.

How do these networks work anyway?
Input l1e is expanded to l1o
l1o is projected to l2x via w_f
l2x is converted to l2c using nonlinear least squares
l2c is expanded to l2r
l2r is projected to l1x via w_b
l1c is converted to l1c via nonlinear least squares

Update:
error is l1o - l1x
	aka actual expansion minus estimated expansion.
Considering w_b,
	change w_b based on the nonlinear outer product of
	l2r and error
		where l2r is the nonlinear expansion of l2c
Considering w_f,
	change w_f based on the nonlinear outer product of
	error and l2r

IF error is positive & output is positive, increase corresponding weights in both w_f and w_b
likewise, if error is negative, decrease both corresponding weights.
For the reverse weights, this is obvious
for the forward weights, less so: whatever led to the current output configuration, decrease that!
This has the obvious effect of changing the output representation to be less similar to whatever it was ...
which pushes it to some other representation (via weights)

Yeah, the system works reasonably well with 4 output bits.
Still some residual errors, but that's part of nonlinear hebb.
With three, it always can't represent one of the input patterns (suggesting that if there were 7, it would represent all?
answer:  No, still seems to get stuck on one symbol.
Probably need to boost / maybe PID to quash those errors.

Also, fwiw, seems to use a vectoral addressing scheme for the symbols (as opposed to a digital address.. hm. )


May 17 2022

Much improved understanding from Anthropic's investigations on Transformers (they work via routing information -- typically keys -- forward in time, making the softmax operation a soft 'search' or 'match' operation.  All this arises through very opportunistic use of local information, which is perhaps why SGD is such an effective learning algorithm.)
+ the FAIR paper on solving recusive sequence tasks using neurosymbolic computing, again with transformers.  Wow, it performs better than Mathematica! And probably could be better.  (see the m8ta blog entry for more.. probably could be expanded to programming, with effort.

Anyway, this proves it all seems to come down to
(1) Mechanism.  You need to provide sufficient representational structure to solve the problem, which is a form of meta-problem and requires some thought.
(2) The transformer architectures are able to solve these recursive sequence problems much faster than evolutionary techniques; they have much better 'inductive bias' and *memory* of everything they've been exposed to.
	Hell, this means that the old boostrapping dream is solved!
(3) As mentioned, SGD is opportunistic but needs to be poised where opportunities occur semi-spontaneously.
(4) Probably routing / gating is as important as multiplication; I was wrong by over-emphasizing the relevance of MUL above PASS
(5) Transformers can represent not just statistics, but more relevantly and importantly, basic algorithms e.g. conditional copy or analogy-making.  That these are evolved and not baked in is truly amazing!
(6) Transformers are a remarkably linear system: almost everything ouside of the attention matrix is linear (or ReLU), and when the attentional matrix is fixed, its all basically linear.

The reading experience gave me far more respect for the emergent ingenuity of SGD and the remarkable generality of Transformers, and lead to significant questioning of what I'm doing here (again.. hah)

Intuition suggests that trying to avoid SGD might well be a purely academic exercise, useful by virtue of getting notoriety, but one must be cautious and thoughtful about what needs to be done.

I think the original motivation was right: you just need too many training samples with standard deep learning to scale well to large, diverse, real-world datasets.  (This with the caveat that many passes through a dataset might well be required to infer the structure ; fewer passes would require more prior information.)
Or, put another way, need a method to learn representations & routing control protocols (aka algorithms) in a way that solves the
(a) coordination problem (= determine representations per layer)
(b) alignment problem (= reduce the error of reconstruction).
(c) Noisy inference problem (= make sense of an ambiguous world based on accrued experience)


May 23 2022
Need to record thoughts from Friday (Boom in the forest) and this weekend (Yosemite with Tom and Will).

Boom in the Forest:
Jason Yosinski: ML researcher working on wind turbines using networks of pressure sensors.
He's worked in machine learning for decades, and evolutionary algorithms for at least this long too.
Suggests that in any sufficiently high-dimensional space, half the gradient directions will be better than the other half (but parhsp not better than the current solution), so improving a network or genome requires only finite bits of luck -- only a few bitch of coordination need to come together at the same time.
He also reiterated that many solutions, like eyes, had intermediate forms that were selectively beneficial.
Led me to think (again) that predator-prey dynamics, which may have been essential to the Cambrian explosion, is useful for iteratively improving the 'loss landscape' ala GANs, allowing effective exploration without needing to have large quantities of luck at any given point (or makes getting stuck in local minima less likely).
After digesting this for a while -- and looking at
A -- How current machine learning models seem to work (they exploit latent structure in a network)
B -- The small successes of the maiden-ogre experiments
I think it's work revisiting the skeleton experiments, just with a different eye to the objective function (must be dynamic) and a possibly different conceptualization of the meta-objective (make it incrementally possible; no leaps are required..)

Yosemite / Hetch Hetchy:
I'm remembering what Chongxi said about his son, that he could recite a Chinese poem long before he learned the meaning of the words.
This also came up while thinking of long-term episodic memory & its incredible *depth*.  How many roads have i travelled, how many places and events are actually well recorded in my memory, if only they can be queued / accessed!
If we turn the task of the neural auto-encoder on its head, and say that memory (remembering) occurs before generalization, then the task is quite a bit different: take a database of all examples, learn what to throw away and learn an auto-associative index on them.
This is hard, of course; you need to know an ordinality/ordering to make decision about what to throw away and how to assess similarity.
	(The difficulty in assessing similarity is one reason why both skeleton.py and cortex_andor4.py have a hard time: they have to search over many possible encodings / addressing schemes (in addition to there being plenty of bugs haha)
Which led to the next thought: the way to infer an ordering / indexing on highly nonlinear compressed yet low dimensional data is to
(1) *experiment* with the natural world, eg. be able to move about it.
(2) Assume that most causal factors are low-d, piecewise constant or piecewise linear.

Plenty of oldish literature to suggest the latter, but I think that the former is perhaps more interesting and relevant, esp. considering the sample efficiency of the human brain
	Ish .. Yan LeCunn just tweeted that the average five year old has on the order of 5 * 365 * 12 * 60 * 60 * 20 = 1.5 billion frames of video from which to infer the structure of the world.
		* Some things, like the emergent algorithms of transformers, may be emergent only at large scale..
		* Still, you really need to start with the right substrate.

I'm sorta imagining something like recursive LSH, where there is enough structure in the indexers / predictors to allow low-gradient selection of working addressing schemes.
The task of inverse graphics remains a great testbed, but needs to be broken down in significant ways before scaling.



Hmm, seems transformers can already do something interesting:
Geometry-Free View Synthesis: Transformers and no 3D Priors
https://arxiv.org/abs/2104.07652
Ah, transformers.  But need to see what the VQGAN is doing in there.

May 24 2022
Critical point of the mammalian cortex as compared to current machine learning models:
--> The preponderance of feedback to disambiguate sensory experience!

Meta thoughts:
Algebra and theoretical models are an essential "crutch" or tool for problem solving.
I may be running in circles because I've not engaged mathematics / algebra nearly well enough.
And perhaps the reason deep learning is working so well is that it does engage algebra and calculus (at least the chain rule of differentiation) *explicitly*
Which implies that trying to avoid derivatives / gradient free computation is unecessarily cripling the approaches.  As I've intuited for a while.
Therefore, it's a good idea to use gradients for the forseeable future, where practical (e.g. not analytical gradients in evolution).  Yes!

Alright, gradients work really well and are super easy ..
as we well expected.
Can easily do nonlinear compression with the outerupt functions.
Only need a little bit of tweaking of the representation to get this working..
Results were with a roi of 5 on the 28 x 28 image.
fi1, fi2 = outerupt_indx2_flat(siz, roi, True)

Try with ROI of 1 -- this will be much closer to PCA (at least for the forward pass)
should run faster.  fewer terms in the expansion.  (doesn't seem to be running much faster..)
Really should add a batch dimesion to improve speed..
End loss is 11
w_f (32, 784)
w_b (784, 528)
HUH seems you ** don't need the extra forward terms? **
	Also note that there are digital-ish errors in the reconstruction.

Back to roi = 5
end loss is 10 (not much better eh?)
w_f (32, 7696)
w_b (784, 528)
What about straight PCA?
oops didn't do that.

What about only nearest-neighbor cross-terms?
end loss ~15
w_f (32, 784)
w_b (784, 63)
	(some pixel errors here too, fyi)

OK, now real (nonlinear) PCA / one hidden-layer network:
end loss ~ 16
w_f (32, 784)
w_b (784, 32)
(Why this obsession with DBN?  This seems to work just fine TYVM)

If we run it for 2 epochs,
end loss ~ 13
(Still getting the pixel errors..)

Turn back on the nonlinear feedback. Q = 528
w_f (32, 784)
w_b (784, 528)
Also run for 2 epochs.
end loss ~ 9
(not sure if it could go down further?)

What happens if we reinstate roi, only even bigger, 7?
	Really need to quantify based on the test set not train set!!
	If we increase the matrices enough, eventually it can store the whole dataset..
w_f (32, 14772)
w_b (784, 528)
end loss ~9 again.

Question: Can this compression be used to predict class labels?

May 25 2022
YEs, it can! 
However, if you use the full 60k training set, it's still better to just do linear regression to select the label (as assessed by MSE..)
If you cut down the linear regression size to 100, 
then the compressed representation is slightly better. 
	compressed mean label prediction error 5.7132864
	naive mean label prediction error 6.7690516
That's with a ROI of 7. 
Try with ROI=1
	mean train loss 9.757882
	mean test loss 9.6124525
	compressed mean label prediction error 5.7888865
	naive mean label prediction error 6.9488473
Ok, so ... about the same. 
What if we increase the representation size? (48)
	mean train loss 7.382174
	mean test loss 7.2273593
	compressed mean label prediction error 6.494618
	naive mean label prediction error 6.2625747
What if we decrease the representation size? (20)
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 14.872782
	mean test loss 14.594089
	compressed mean label prediction error 5.20966
	naive mean label prediction error 6.1839123
Good!  So, it does better.. 
Adding some translations: 
	torchvision.transforms.RandomAffine(10.0),
gives: 
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 15.875707
	mean test loss 14.274618
	compressed mean label prediction error 5.3931966
	naive mean label prediction error 7.572067
Looks promising!! 
Try again with 20deg rotations: 
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 16.331211
	mean test loss 13.986706
	compressed mean label prediction error 4.5654993
	naive mean label prediction error 6.855
Oh wow!!?? Even lower error. 
Let's throw the ROI=5 back in there. 
		{ Aside: really need to enable batch mode here.. speed up training}
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 15.717631
	mean test loss 13.437547
	compressed mean label prediction error 5.3180842
	naive mean label prediction error 7.040411

Wnat to run more experiments but really need to eat something! 
ROI = 5, rotation = 40,
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 16.888586
	mean test loss 14.305665
	compressed mean label prediction error 5.9888306
	naive mean label prediction error 8.747594

Added in a batch dimension.. speeds up computation, but slows down convergence.
Interesting.

With a batch size of 8:
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 17.341217
	mean test loss 14.789629
	compressed mean label prediction error 5.616935
	naive mean label prediction error 8.502002
With a batch size of 32:
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 30.184021
	mean test loss 28.65005
	compressed mean label prediction error 5.768119
	naive mean label prediction error 8.599775
BS 32, ROI 3:
	w_f (20, 3700)
	w_b (784, 210)
	mean train loss 38.49564
	mean test loss 37.350113
	compressed mean label prediction error 6.2187138
	naive mean label prediction error 6.6117

Umm.  I think I might be doing this wrong.
The label ought to be one-hot!  Categorical!
Of course it won't work super well with just the numbers 0,1,2..
Yeah, duh.
Below, one epoch, roi=3, batch=32
	w_f (20, 3700)
	w_b (784, 210)
	mean train loss 38.642357
	mean test loss 36.807293
	compressed mean label prediction error 0.072603054
	naive mean label prediction error 0.095275916
Below, two epochs, roi=1, batch=8
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 18.2238
	mean test loss 15.588934
	compressed mean label prediction error 0.07381655
	naive mean label prediction error 0.10375466
Below, two epochs, roi=1, batch=64
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 38.745132
	mean test loss 37.064823
	compressed mean label prediction error 0.06895668
	naive mean label prediction error 0.09322322
Run again (since it's fast..):
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 37.436176
	mean test loss 36.331757
	compressed mean label prediction error 0.08552185
	naive mean label prediction error 0.106565624
Increase TN, the number of examples used in the linear regression, to 200.
	w_f (20, 784)
	w_b (784, 210)
	mean train loss 41.640827
	mean test loss 40.023956
	compressed mean label prediction error 0.06269046
	naive mean label prediction error 0.11225252
Humm..
Bump the hidden dimension to 32.
	w_f (32, 784)
	w_b (784, 528)
	mean train loss 20.714134
	mean test loss 17.512472
	compressed mean label prediction error 0.061895754
	naive mean label prediction error 0.11251253
Without rotation:
	w_f (32, 784)
	w_b (784, 528)
	mean train loss 17.407146
	mean test loss 17.232634
	compressed mean label prediction error 0.055899657
	naive mean label prediction error 0.10356578
Without rotation, hidden dim 48, roi 3:
	w_f (48, 3700)
	w_b (784, 1176)
	mean train loss 17.438473
	mean test loss 17.392864
	compressed mean label prediction error 0.089243285
	naive mean label prediction error 0.11814148

Note:
tried changing the output / final nonlinearity.
It's quite sensitive to this!
Changing to
	l1i = jnp.clip(w_b @ l2r, -0.2, 1.2)
Causes the model to not converge (!!!)
It makes it so the outout has only a few bits on.. why?

Sigmoid output on l1i gives nice results, but the codes are saturated and bad.
Am thinking that we're missing some bias terms here .. ?
So that the slopes can be inverted?
Maybe helps, maybe not.
Seems to help with lost pixels at least.
	w_f (32, 3700)
	w_b (784, 528)
	mean train loss 10.11068
	mean test loss 8.062884
	compressed mean label prediction error 0.057571307
	naive mean label prediction error 0.11125314
Again:
	w_f (32, 3700)
	w_b (784, 528)
	mean train loss 10.0058565
	mean test loss 7.905799
	compressed mean label prediction error 0.056958407
	naive mean label prediction error 0.10198992


Seems this is about as good as we might get --
roi = 5 batch = 16, P = 32, epochs = 10
	w_f (32, 7696)
	w_b (784, 528)
	mean train loss 9.989642
	mean test loss 8.041153
	compressed mean label prediction error 0.05843653
	compressed one-hot label prediction error 0.053519998
	naive mean label prediction error 0.10049593
	naive one-hot label prediction error 0.079059996
Lacking any further information, seems like ~ 5% error is as good as possible without using supervised data (probably?)
roi = 5, batch = 16, P = 20, epochs = 10
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 15.351827
	mean test loss 13.229023
	compressed mean label prediction error 0.056146413
	compressed one-hot label prediction error 0.04976  <-- slightly better
	naive mean label prediction error 0.09369152
	naive one-hot label prediction error 0.08028

roi = 5, batch = 16, P = 20, epochs = 100
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 16.233717
	mean test loss 13.735222
	compressed mean label prediction error 0.060591716
	compressed one-hot label prediction error 0.057
	naive mean label prediction error 0.105436824
	naive one-hot label prediction error 0.095359996

Ok!  I think this is as optimized as I might well get at this point ...
Sure I could fiddle with it more but .. eh?
A days+ experiment seems effective, time for a

May 26 2022
Looks like my fancy nonlinear system works ...
exactly the same as good old PCA in terms of dimensionality reduction. 
With 16 epochs: 
	w_f (20, 7696)
	w_b (784, 210)
	mean train loss 16.316217
	mean test loss 13.730327
	compressed one-hot label prediction error 0.061139997
	naive one-hot label prediction error 0.08794
	PCA one-hot label prediction error 0.06172
	
OK, turn off all the goofy multiplication: 
epochs = 5, roi = 1, no nonlinear expansion of hidden state, 
This and the previous used 40deg affine transforms
	w_f (20, 784)
	w_b (784, 20)
	mean train loss 21.786966
	mean test loss 19.35434
	compressed one-hot label prediction error 0.05706
	naive one-hot label prediction error 0.09804
	PCA one-hot label prediction error 0.06388
YEah .. just nonlniear does very slightly better. 
	w_f (20, 784)
	w_b (784, 20)
	mean train loss 20.900702
	mean test loss 18.523268
	compressed one-hot label prediction error 0.06888
	naive one-hot label prediction error 0.09566
	PCA one-hot label prediction error 0.0729
{ of note: some of the examples even I can't recognize! } 
Let's try only nonlinear input expansion !? 
( should be almost exactly the same)
Yes no change. 
	compressed one-hot label prediction error 0.067719996
	naive one-hot label prediction error 0.097219996
	PCA one-hot label prediction error 0.070379995


Friday May 27 2022
Seems like it's time for a bit of review!
Let's go through the source code (and maybe some papers) and review the lessons learned.
* cortex_nn.py
	Used to iteratively estimate a 8x4 matrix (4 real dimensions, 8 apparent)
	This with either one-hot activations or random combinations of the 4 vectors.
	Started the l1 l2 nomenclature...
	Not really sure if this worked so well.

* cortex_nn2.py
	Iterative forward-backward trail, with three layers.
	Raised awareness to the fact that that hidden layer is hard to correct -- how to allocate responsibility / credit to either the top layer or the bottom layer weights?  How do the upper layers contribute to prediction in the lower layers, if the hidden layer depends on both?
	Fun expermients, but inconclusive.
	Still don't really know the answer to responsibility / credit issue .. I think, when the stimulus is ambiguous, there is no way to really represent hidden layer activity without upper causes (hm?)

* cortex_2axis.py
	This one actually works.
	The task is to take a 3-dim input (6 with complements) and produce a 2-bit address / index using feedback alignment.
	Yes, the nonlinear hebbian rules can do this in the tiny toy example.
	Well, better than not working!

	Gave some confidence that the Hebbian learning + E-I balance + feedback alignment is not totally riddiculous..

* schmid_test.py
	This is after Schmidhuber 1999
	VAE is a 3-layer, one hidden MLP
	Probably I got this from the web somewhere, particuarly given the print statements.. using save_miage as opposed to plt.imshow, fwiw.
	It's a beta-VAE, where the beta term grows from 0.25 to 0.75 (ish)
	The latent representations seem not so disentangled,
	consistent with the architecture..
		[1]S. Hochreiter and J. Schmidhuber, “Feature Extraction Through LOCOCODE,” Neural Computation, vol. 11, no. 3, pp. 679–714, Apr. 1999, doi: 10.1162/089976699300016629.

* cortex_bumps.py
	This was written in the Mojave (partly) .. idea is to predict a series of moving bumps, rather than predicting binary patterns, using the same framework as above.
	That is, the next frame was predicted from the current frame + the current hidden activation.
	Spent quite some time trying to figure out how to represent something like 'pass gate' topology.
	This is really another, perhaps overy complicated, version of activity-activity multiplication, that required the use of a weight *tensor* not matrix, which is quite the excessive memory representation.
	(Don't think we should generally use weight tensors in the future; there has to be a better way to represent or factorize .. that said, the Anthropic work makes quite successful use of tensor products in understanding how transformers work.. hmm but that's for understanding, transformers are still mstrix-matrix mults.
	I don't think it ever really worked, partly because what information was kept around was not clear.

* cortex_bumps_sgd.py
	One of the several times I check to see if the topology actually works by using SGD and autodiff.
	* but still with the pass-gates
	It does, pretty sure.  But quite slow; this is not a great way to represent the problem, per thoughts above.
	Unsurprisingly.
	{Still not a bad torch implementation.  Building the calouses and foundations.. }
	Looking at the plotted output, unclear if it's really disentangling anything... the reconstructions are fairly noisy around zero (might well just need appropriate clamps / nonlinearities.. )

* cortex_mnist.py
	This was the first 'complete' script written while at Loyal, and it is actually pretty successful!
	Is able to quickly and semi-reliably reconstruct MNIST digits in the two-layer autoencoder topology per cortex_2axis above.
	The nonlinear Hebbian learning works well, even if it never fully cleans up the residual errors.
	Most importantly, 'conjunctive' code allocation + explain-away sparsification does seem to work well, and it all converges rapidly, ~ 3000 steps or so.
	Feedback alignment (in the local meaning of the term, not lit) also works correctly.  Asymmetric learning rates may contribute to this.

	Problem is that conjunctive features only go so far for representing the digits ... need conditional features, at least.

* cortex_andor.py
	This version had nonlinear dendrites, was written in torch, and is trained to 'compress' or index the glider-checker task.
	As of today, it sort-of works..
	This features the product-of-activity outer product, nonlinear hebbian correlative weight update, noise bootstrapping (which i like), and a supervised option.
	It was designed as the simplest toy case to think about & test conditional / activity * activity features, in a quasi-biological way (nonlinear dendrites).

	Lessons: learning arbitrary indexes/compressions is quite hard, system is prone to instabilities.
	A highly compressed representation is fragile: if you change one address line or bit, then all the other ones need to be changed as well!
		Of course this is not true of the real world ...
		it is fractionally spaced, not full for a given set of axes.
		E.g. not all combinations of all things; there are categories of axes, axes of axes so-to-speak.

cortex_andor2_be27787.py
	So this solves the glider-checker problem!  Almost perfectly!
	Proving above wrong, or at least incomplete, I guess.
	Yet it's a tricky system that required a fair bit of work to function,
	and is also unstable.
		(One part of the solution is simulated annealing, though even without this it seems to converge quickly..)
	I think in general something in this realm could more generally work when, per above, the full address space is not used, or the address space itself is factorized / structured.

cortex_andor3.py
	This was after skeleton.py (below) but in the same series; I switched from pytorch to JAX here.
	Got rid of dendrites, kept the activity-activity outer product mostly intact
	When encoding eight symbols with 4 bits, it always seems to miss one.
		Seems that it gets stuck in a local minima, where updates on one symbol exactly counteract updates on another, and the feedback can't push it to a new representation... and noise just can't push it out of the hole.
	Increasing the latent dimension fixes this, i think.
	Model still works with a latent of three dim, i think.

	Remains toy, but still pretty OK.

cortex_andor4.py
	Took andor3 above and switched to MNIST.
	No SGD, still E-I balanced feedback.
	But but, did use non-linear least squares to convert from the expanded latent dimension to the compressed latent.
	Doesn't work that great -- turning down synapses to and from the uncompressed latent is very tricky!
	Spent some time on this one, but the approximations / reconstructions never were that good..
	Some aspect of this might be tweaking the representations and feedback, some aspect might just be the overall sensitivity of the model / instability of the encoding.
	As mentioned, if one address / index changes many other encodings also need to change, which makes the problem more tangled hence harder to solve.
		-- Seems like a fundamental issue!!
	What if you simply don't change an encoding once it is set?
	(Other than if other encodings completely take over predicting for one mode .. then you need some learning rule or something to make sure the weights decay.)
	* Model is surprisingly sensitive to learning rate!
	Needs more negative feedback loops for sure ...
	but that said, at this point I was getting itchy that the network struture itself was limiting, and that I didn't understand what the problem was / how a 'good' way of solving it might be.
	As in: are activity*activity maps a reasonable representational basis?

	All this was prefaced, as indicated by the names of the scripts, that current MLPs are pretty good at linear 'OR' operations. Input A or input B (times weight) gets summed to produce output (unless negative weight..)
	What you seem to get in dendrites is more of an 'AND' operation.. you need both conditions for there to be output (~= multiply).
	Hinton definitely mentioned this.
	Many people have worked on it, but of course the reresentations become entangled then ... which is part of their power.
	It can also be thought of as an 'if' operation.

	IDK, looking at the output again now, I think there is a bug somewhere.

* cortex_andor5.py
	Finally gave up and went with SGD!
	Are these representations even reasonable?
	Yes, they are.
	Note: did non use non-linear least squares here, just propagated the gradient through the whole model & had asymmetric w_f and w_b matrices.
		e.g. w_f went expanded l1 to l2; w_b went expanded l2 to l1
	Yep, it works, and offers reasonable reconstructions.
	Can be used for few-shot learning of classes with an accuracy very slightly better than boring-old PCA
		PCA on whole training set,
		Bio-autoencoder (BAE) on whole training set,
		then supervised learning on 200 samples of the training set
		and testing on the whole test set for both conditions.
	Plenty of fiddling showed that it's mostly the nonlinearity, not the AxA, that lends efficiency in semi-supervised learning.
	A result in and of itself!

* skeleton.py
	Much work went into this & I think much work remains.
	From Tim Saliman's work, I think that virtual (or not) batch norm might be required to get better per-neuron behavior hence overall behavior-solution diversity [1]
		(notes that this was essential to get some behavior out of convnets, where intial noise does not produce meaningful actions.)
	He also offers some good insights on parameter fuzzing vs policy fuzzing [2]

		[1]T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, “Improved Techniques for Training GANs.” arXiv, Jun. 10, 2016. Accessed: May 27, 2022. [Online]. Available: http://arxiv.org/abs/1606.03498
		[2]T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever, “Evolution Strategies as a Scalable Alternative to Reinforcement Learning.” arXiv, Sep. 07, 2017. Accessed: May 27, 2022. [Online]. Available: http://arxiv.org/abs/1703.03864

	Big open question of how to set the objective function properly to get good encodings?
	Also, we need to incorporate environmental interactions!
	(Spoke with Tarin about this yesterday; see alt-ai.org)
	Will have to write more later.

June 2 2022
Since the last entry, have had dinner with Luke & learned a bunch of things.

(1) A number of theoretical physicists have entered the deep learning field, I think at GB or DM, and they have done very well.  According to Luke, this is because they have been *careful* -- I'd assume, do the associated math..

(2) Very large image and language models are getting to be both compute ($$) and data-limited; it's hard to curate more of the web to improve the results further.  Very interesting!

(3) Luke mirrored my questioning: So what exactly do we do, now that Transformers, or at least objects with attention and resnet units (and probably convolution) seem to be cleaning up?
	-- emphasis on "seem to be".  See the caveats above, and questions about the data / compute efficiency of this class of methods.

(4) Mentioned Adept.ai, which is a new spin-out of Google Brain, working on using large language models + program synthesis stuff to automate common tasks on the computer.
	It's a good idea, think it could definitely work!  Strong team! (Though allegedly Augustus Odena is a character.  Are we not all..)
	-- Aside: some aspects of the Transformer may seem obvious in retrospect; isn't that true of many things though.  I suppose Vaswani and Parmar needed a lot of experience to gain insight that attention is indeed all you need.

(5) Luke has been working on a meta-learning 100+ task corpra, and has found that whatever algorithm he has works for most, but it can never work for all / and it can't work on itself.  Have to see what the tasks are to better understand this.
	This led to a short discussion of conciuosness and symbols / meta-symbols and the possible need for recurrence / a recursive syumbol manipulation system for "fully" closing the loop.
	Talked a good bit about this on my "swan hike" with Judy, Arthur, and Alex..
	Hard to recreate that magic, but I do think it all comes down to computed-addressing / "pointer arithmetic" and amortized search.

(6) Which is really what transformers are doing, I think.
	-- They compose / create both algorithms (as in the Anthropic paper, where cetain heads are shown to copy data based on key-query matching; presumably more complicated algorithms can arise with this sort of match-search-forward algorithmic substrate?) and do good function mapping (via the many layers of MLP and resnets.)
	-- Can do pointer math, maybe even!
	-- Certainly can do conditional execution & express higher-level probabilities.  Probably do this in a way that is better (?) than just activity-activity multiplication because of the added (minimal) structure (e.g. search)
	-- My rough estimate is that for these sorts of models to work, you want to use the absolute simplest representation possible (e.g. MLP + ReLU / swish seems like just barely enough nonlinearity to represent anything; query-key-value is probably just the right quantity of complexity to allow algorithm composition
	-- Keeping everything as linear as possible, in high-dimensional spaces .. just  seems to work with SGD.  The two are compatible.  SGD is a very effective way of tuning weights in high-d spaces. (where everything a saddle and vectors tend to be orthogonal..)
		-- I think that the "secret language" of Dall-E is an example of this.  Vector representation of what is roughly discrete or factorial representations... you get aliasing, but also generalization.

(7) skeleton.py:
We're asking it to do roughly a NP hard problem.
It can't reliably do this in the alotted time; cortex_andor* has a lot more time to anneal down to factorized variables.
If we want it to work, need a sensory-motor loop to allow experimentation to cut the ambiguity & allow for quick factorization.
There is really no other way, other than brute force exploration, and then you get into the problem of entanglement of addressing lines / factors.
	(This might be a problem in the task too?)
Yeah anyway, time to not be lazy ... and probably increase the complexity and capacity of the network (at the expense of a smaller population, but so what)
Definitely worth a shot
	(though I'm leaning more toward the 'artisinal' now ;-)

June 8 2022
Spent the last week-ish binging on papers. I think I have a better lay of the land, at least as it stands at present; now need to organize my thoughts (and also re-read all my old notes...)

1. Predictive coding and equilibrium propagation are fundamentally the same thing -- an iterative means of propagating error derivatives backwards through a computational graph.  Provided you have error units, predictive coding even reduces to Hebbian learning (which is what I've implemented).
	Notable that this can be derived from Friston's free-energy principle.  While I've shied away from this in the past, I think that, as Donald Knuth says, you need to lean into mathematics -- it may be a crutch, but it's also a tool for navigating difficult conceptual areas.
	In light of this, I should be writzing more calculus!
	And need to read the free-energy tutorial.
2. Predictive coding / equilibrium propagation *do* solve the forwards-backwards coordination problem, but but some elements of coordination may be avoidable, vs-a-vis Sindy Lowe Putting an end to end-to-end.  You may be able to learn OK representations locally & greedily.
	Caveat: in all Hinton's DBN work, the layers are trained greedily, but then you need a period of backprop fine-tuning to "get the elements to work together" or so.
	Greedy training gets the parameters approximately in the right starting point. (although he offers no proof of this; it seems rather to be an intuition.)
	One assumpion in the Lowe work is that the underlying latent / generating factors do not change on a rapid timescale, hence end to end-to-end uses a local contrastive loss, InfoNCE, (Oord 2018).
		This seems to be sound & has much precedence e.g. Linsker
	Another caveat: both predictive coding and equilibrium propagation require many passes to move their errors around the hierarchies.
		Thought: depends on the markov blanket / complexity of the computational elements you are trying to select for!
3. Contrastive methods, as used in wave2vec, can be used with backprop to very effectively build a unsupervised decoder.
	The way that this system works is that they train contrastively:
	They mask off certain portions of the sampled, compressed latents, input that into a transformer, then use the transformer to predict the class membership of a quantized version of the masked latent.
		Quantization is through a differentiable Gumbell softmax (which includes a sampling step!)
		Quantization was important to improve the latents, and is consistent with the underlying discrete nature of speech (though they do not analyze this..)
	That is, they don't predict the actual quantized latent ... just try to maximize the difference between that and other codewords in the same utterance (humm?)
		This is a common refrain -- you do not want to predict the whole input sequence,
		but you want to be able to discriminate that sequence from everything else.
		This is different from the 'pure' version of an auto-encoder;
		having to re-represent all of the features as required by e.g. L2 loss, puts a heavy requirement on the encoder half.
	Other contrastive systems work on the principle of predicting the next sampled symbol (which is predictive coding in another form)
	There is a transformer in there too, which is trained via the contrastive loss...
		Note that the fill-in codeword is trained as well.  Probably some tricky clever engineering in there!

	These contrastive losses on semi-auto-encoder models seems like a much more data efficient way of training compared to GANs.

4. Search!  Distributed search (hierarchically and spatially) is vital to breaking inference problems down when perfect / algorithmic inferences cannot be found.

5. Spent some time on hashing / locality sensitive hashing / spectral hashing etc.
	Hashing can of course be very fast,
	but LSH does not seem to be per se a powerful way of organizing or generalizing over data.
	E.g. the way to make LSH work better is to train a DBN (greedy + fine-tune) to do the hashing,
	Or to look at spectral methods (e.g. find the PCA of the data, break the space up using non-outer-product eigenvectors = hash functions)
	This is not directly performing computation on the dataset in the same way as SGD on epochs of data is doing it..
	You could imagine extensions of the process that refactor, contrastively perhaps, the hash functions (= adresses) of the data based on input occurrence.
	This would be consistent with phenomenology.  You don't need an auto-encoder of the data if you simply ... remember it!

	As for addressing, can do some sort of online incremental version, ala NEAT etc, where once some old index no longer is needed, it can be repurposed...

	Problems with hashing:
	1 -- It's not clear how to make it hierarchical.
		Do you hash addresses, recurrently?
		Or concatenate the addresses to bits of the data?
	2 -- Not obvious how to deal with inference of / use of latent variables.
		Is this like a Hopfield network, auto-associative fill-in?
		Latent variables = position of objects, identity of objects, binding of attributes into whole, etc.
	3 -- Not obvious how to use straight memory to do 'computation'.
		Computation is necessary for generalization / compression.
		Computation is (surprisingly) effectively done well by transformers, which do some sort of variable binding and forwarding magic & can iteratively explore this space via SGD.
		Networks like ResNets, CNNs do less computation than function approximation.
			Hedon: can you have meta-symbols, functors sorta, that work on higher-level symbols?
			That would allow for even better generalization performance.
			Maybe this is possible by making the system properly re-entrant and reflective (per consciousness discussion above)?

Ok, start with a database. MNIST, SHFN, all youtube videos etc.
This already has an indexing variable
	imperfect but very useful: time and space
Find other indexing variables that
	-- Allow you to rapidly search over the recorded data, ala LSH
		* ANNs are definitely a form of fast readout: you just propagate activations through a network to read out the memory (function approx)
	-- Allow interpolation
		* ANNs are also great at this, by interpolating in the latent space & again reading out the input (reverse propagation).
		StyleGAN and Transformers are perhaps the best ...
			because they do the best at approximating data with pseudo-algorithms.
	-- Correspond to real factors of variation in the world
		* This is similar to the disentangling hypothesis / set of ideas.
		You want to make the latent space roughly correspond to the structure of the world.
		This obviously makes indexing much faster / easier,
		and it makes the database required much smaller: you can combine values (as in key-value store) compositionally to predict the world.
	-- Perform processing gain on the stimulus
		* Similar to above.  procesing gain = compression.
		There seems to be no way around the need for extensive computation to find modules that evince processing gain. You need a lot of compute!
		Also, it would seem that making an advancement on SGD is very hard if not impossible.
		That said: SGD is harder to parallelize than biological update
		yet it's not obvious how biology solves coordination problems (finding coalitions of good settings across levels of a hierarchy)
		and SGD is ideally good for this.
	-- Utilize motor output / reafferent / experimentation to disentangle the world
		In this sense, there are XXX ways of pulling out independent factors of variation in data:
		1. PCA.  linear methods. SVD.  Generally efficient. Not so powerful (just a rotation and scale.)
		2. Linear projection + token nonlinearity (ReLU),
			trained via SGD, or *prop.
			Even rectification offers enough processing gain to support completely general function approximation; modules learned in the process of supervised learning have proven to be useful for other tasks and/or tend to be general factors of variation.
		3. Nonlinear PCA / ICA
			Use higher moments of the distribution of data to extract separable components (blind separation)
			E.g. data is never perfectly white & tends to follow categorical or discrete distributions while noise may follow gaussian distributions.
				The original Sejonowski and Bell paper (1995) focuses on linear mixtures of non-gaussian sources based on the InfoMax principle.
				What about nonlinear mixtures of non-gaussian sources?
		4. Kernel trick, then PCA or ICA.
			Take a nonlinear problem and make it linear by dimensionality expansion.
			Suffers from the curse of dimensionality, alas -- and still needs a metric for selecting the (linear combination of) output dimensions.
		5. Slow variance of latent factors in the real world.
			Wiskott and Sejnowski 2002
			For example, when hallucinating a video of mountain biking, the only thing changing is position (and even that is conditional on velocity, conditional the terrain elevation etc. )
			The world is not changing (except the clouds wind etc)
			We have reduced things to maximally static!
			Seems like a good goal.
			A perhaps more realistic model is piecewise linear:
			There are abrupt transitions in the underlying latent variables.
			Like speech!
				Note note: "performance degrades if the model is forced to learn multiple invariances simultaneously" hmm.
		6. Experimentation.  'Natural scientist'
			Some papers on using self-motion signals as a supervisory signal (predict the motor signal from the visual field, e.g.)
			More can definitely be done here.
		7. Brute force.
			This is equivalent to partitioning a graph, which allgedly is NP-hard.
			Ran into this problem directly in the glider-checker task in that maximally compressing address variables are inherently entangled with each other.
			If we were to address sequentially, might work?
			Or might just find bad encodings.
			Or perhaps if we allow much larger encoding /addressing spaces, then subsequently weight-decay pare down it should work?
			Is this sufficient to evince computation hence compression?
			Also: how to do this in a small-enough way (e.g. ReLU is just barely enough of a nonlinearity) that it's explorable.


Thoughts:
	Using backprop makes it much easier to use only feedforward architectures, but it does not preclude you from using feedback (e.g LSTMs, BPTT)
	cortex_andor5:
		Has about the same efficiency as PCA for image classification
		Which is saying a bit .. PCA is easy to compute and (present day) obvious.
		Predicts (is an auto encoder) in pixel space, MSE in pixel space ... one way to regularize this is to add a contrastive term.
			Might be worth experimenting with.
		Far better is to seed disentangling with motor reafferent.

Hedon:
	One reason for the highly asymmetrical morphology of cortical excitatory neurons is the need for address / value computing.
	Some things, such as removing translation or zoom variance natuurally factorizes as an addressing operation: you add the eye position to the 2D address, and automatically re-index to head coordinates.
	Likewise for head to body, body to world..
	Indeed, indexing (gather) or even scatter opeartions are very general, and can easily be used to generate algorithms, which is really the (only) way to permit low-data generalization.

Yes but:
	It makes sense, but how to actually implement it?
	Addressing signals are back-propagating to L1/L2?
	Value signals are axons that ramify to L4? From L5/6?
	There are definitely more layers here than two, and many more neuronal subtypes to deal with.


July 4 2022
	Happy independence day!
	Going on a trip to NM in a few days, so it's good to get some work done today.
	Finally got key-query mapping working for circles; need to rejigger the weight / key update rule. 
	Rather obvious in retrospect. 
	Might not be biologically plausible in the current instantiation / present meta-parameters, but meh. 
	
	Todo: 
	-- One-hot obviously will not cut it for more complicated or interesting input data. 
	As before, we want something closer to a factorial encoding or 'dense' encoding where addresses (here 100..) are maximally (or fairly) uncorrelated, and can contribute to different parts of recognizing and recreating the sensory space. 
	
	I suspcect that a second layer of tuning is required here; cortex_mnist *does* decompose the space into conjunctive features, which sum to reproduce the digits.
	(And with SGD, cortex_mnist5 does offer slight processing gain..)
	Presently the key matches on the entire input space, which doesn't make sense; there needs to be a 'don't care' or 'don't connect' vector. 
	Alternately, we combine the two topologies so that 'don't care' is equivalent to zero forward or backward weights; it hence does not contribute to the reconstruction. 
	Or, as I proposed with Laura Demming: key-query indexes a value, which subsequently sums.

	There are two problems here:
	1 -- Re-creating the stimulus, ala an auto-encoder.
		Work so far has focused on E-I balance, which is plausible, though
		- there does not seem to be a ton of evidence for backward inhibitory projections, and
		- it's not clear how to scale the system to multiple layers.
			Oldhausen also ran into the problem ... you get a conjuction of linear features, but need backprop to make them work together (coordination problem)
	2 -- Deliberately segregating the query/address space to allow invariance: 'don't care'.

	re 2, Probably the most obvious solution is the best?
	Add a second set of coefficients which acts as a weight on the K-Q match?
	But then, how to update the gate?
	Maybe a momentum term ... if the input data consistently matches the key, then increase to saturation.
	If it's unreliable, then decay to zero.
	But try to make the gate vector as large as possible.
	re 1, perhaps the solution is some sort of reprocity?

	Getting a weird bistable behavior here ...
	that of course needs to be fixed (probably wrong learning rule?)
	Also probably need a dataset with a bit more statistical structure
	Or hidden layers that are smaller.

July 5 2022
	Looking over what I did last night:
	-- The 'gate' weight seems redundant, even though it does support the desired operation: allowing for partial key matches (keys 'focus' on maybe 1/4 of particular circles..)
	-- The redundancy encourages a moving dynamic of keys and gate, which maybe makes sense, but is not really desireable..
	-- I wonder if the right solution is to incorporate E-I feedback control here.
		The current network is not trying to minimize any error.
		So it's challenging to say "how well it's doing".
	-- System is super sensitive to hyperparameter (weight decay) change.
		0.999978 works, 0.999878 does not work.
		Not a good sign.
		Probably if we increase WD further (slow weight decay), the gate variable will move around even more.
		Nope.  Slowing weight decay makes it more stable.
		Decided to get rid of weight decay and replace it with total weight normalization ..
		Divisive normalization doesn't work,
		but substractive normalization works just fine & properly encourages the keys and gates to focus on smaller regions of the stimulus space.
		(see 20220705_circles2)

	Question is: beyond address-space segmentation, what really do we want to do here?
	Ultimate goal: via a series of layers, convert this circles tasks into a compressed, disentangled representation: cx, cy, radius.
	* With, of course, the help of these supervised variables!
	* Without, of course, any SGD or actual supervision.
		This may require some heuristics to solve coordination problems.
	* In such a way that, with additional time, it could *conceivably* also disentangle in a wholly unsupervised way.
	* In a way that it generalizes to problems other than circles, eg MNIST, images, video, stereo video...
	* Incorporates something like E-M or iterative inference of address/query latents in the case they are not supplied. (network that is able to do both? )

	Alright, it seems that I can finagle the network to mostly do what I want.
	"Artisinal"
	I think the next priority is to add in auto-encoding feedback?

	Notes on circles2:
	PCA can extract most of the variance of this dumb simple dataset.
	The top-3 PCAs can predict w/
		r^2 of 0.938 for cx and cy, and 0.737 for radius.
	Bumping to top-10,
		r^2 of 0.967 for cx and cy, 0.770 for radius.
	(Corresponding weight matrix looks like a scale/rotation, good).
	** Suggests that a lot of structure might well be just reasonable PCA..
		which many dot-product architectures should be able to extract.
		(Pelehan et al.. )
			Hmm, this is good to know!
	Yet .. recreating the circles from those PCA vectors. Hmm.

July 7 2022
	Have a version of the closed-ish loop inverse-graphics version of circles started. 
	It doesn't work yet, as the expanded variable doesn't vary too much; 
	Think I need to whiten and center it. 
		(This seems to be a common feature of all sorts of statistical processing algos)
	OK, 'ex' is whitened, but ... still not clear that it's finding a good mapping from the expanded supervised cx,cy,r to the l2 variables. 
	Let's remove the 'gate' variable, which should make it more interpretable. 
	Yeah, now need to see if it actually inverts the data -- e.g. can do approximate forward graphics, with no error and limited feedback .. ?? 

July 11 2022
	With the gate variable removed, not clear that it's really finding a good address from 'ex' variable ... need to do a reconstruction?

July 13 2022
	OK, have the reconstruction implemented.. it's a useful feedback mechanism.  
	But I seem to have broken the key-query matching. 
	(... stream-of-work writing here ... )
	It all seems to be working again, though circles3 does not match the larger circles well. 
	Circles2 is back to matching segments of circles; this seems to be dependent on the *form* of computation of "gate".
	Here gate is just an outer product of activations between L1 and L2, decayed.
	(Hence is of Hebbian form). 
	
	Regarding what exactly the keys are aka how the space should be segmented, this is a fundamental problem!
		- Clearly, with circles3, we can do both forward and inverse graphics, with some (untuned) efficiency. 
		- Yet still making a LUT for 100 circles sure seems like cheating. 
		- Need something that factorizes the space, implicitly (in an unsupervised manner) first, then in a quasi-supervised manner later. 
		
	Outstanding problems: 
		- Need an encoding that compresses and simplifies the stimuli, while ignoring invariances / extracting equivariances. 
			So, early in the pipeline, we worry only about local statistics; as we ascend the hierarchy, worry about larger and larger receptive fields. 
				Currently don't have this.  Units need to have a topology, axons / dendrites spatial limits (certainly dendrites)
		- Said encoding might start out as one-hot but gradually entangle itself to be factorial (?)
			I like this intuition & it's consistent with 'aha' moments. 
			From the run:
			Distributed, parcelized search based on the fractal structure of the world is bound to be better than SGD (?), and should be able to find an efficient "address space"
		- The system ought to do inference in a iterative E-M manner, consistent with visual illusions and the timing of visual perception 
		- Slack variables: 
			Inference probably involves use of ephermenal variables; could be BTSP. 
			We don't really have any data, other than in the hippocampus, that BTSP is important,
			but from a logical perspective, it would seem so! Natural place to store the information. 
				The alternative is to store everythnig in vectors of activations. 
				Which, to be honest, is not *totally* unreasonable.
				See transformers, which work well with 1024-2048 dim latents. 
				(I also wonder if BTSP might be a bit of a distraction..)
		- Time:
			We don't have any elments of time or timing in the model.  The CNS seems to put quite a lot of energy into timing though.  It might be needed for stability, and it might be needed for causality or slow-feature analysis.  (All of the above?)
		- Feedback:
			Allocation of neurons & their keys ought to be subject to being able to predict the stimulus / lower layer.
			That is, if some part of the stimulus is unexplained, keys should move annd/or unactive neurons should take up that space.
			The previous series of experiments worked well with this, and used the 'explanining away' effect to some advantage. 
				Still, I think that the K-Q model 'feels better' than E-I feedback balance (though that too may be a part of the system, perhaps through the thalamus?)
				// Yeah well the overall architecture of the brain generally (exception: the basal ganglia) does not employ inhibitory long-distance projections.  
	
July 14 2022
	// Last night read an interesting thread on hacker news describing how perception is not a problem with Tesla's autopilot, it's commonsense reasoning.
	// The perceptual system can notice things like trash cans, stoplights etc, but it just never learns the local geography / does not have more than 15-30 seconds of memory for what the world is.
	// This is an important bit of insight: am I solving the wrong problem?

	Reading back over things. Might want to loop back to the overarching goals / remind why motor-informed disentangled representations are important.
	1 -- Boostrapped knowledge.  We want to be able to explore an unknown problem domain purely based on experimentation.
		Networks like EfficientZero RL algorithms might be getting pretty close (review that?)
			Right, three improvements:
			1. Vision model (project to latent space) + Action model trained to match next observation + state ala Siamese networks.  DUH
			2. LSTM to predict value based on series of states to prevent state-reward aliasing
			3. Off-policy reajustment when updating value prefix
				(this i understand less well)
		To properly make this work, need:
	2 -- Disentangled / factorized representations
		So that the agent 'knows' how to manipulate the world & does not suffer from the curse of dimensionality.
		** Need to critically inspect this assumption **
	3 -- Computational gain on the perceptual module
		This is a correlary to 2.  Factorization requires computation; in the current conception, this is done with some sort of address arithmetic which supports abstraction beyond just PCA.

	----

	To make the 'circles' set of algorithms work, I do think that these are needed
	1 -- Iteration / feedback
	2 -- Nonlinearity
		(otherwise, it's just PCA --
		of course the activation is highly nonlinear, but presently I suspect that we're still doing a degree of PCA..)
	3 -- Active allocation of K-Q units / elimination of redundant units

	4 (?) Random projection is a cool idea, but it's not exactly invertible.

	----
	Plugged in the mnisty data, and it appears that among many other things, we need to improve allocation (even more obvious now!!)
	and to add in slack variables to make reconstruction with just the supervised signal work.

	Thought that needs to be clarified:
	Without added interactions and added assumptions (temporal consistency, e.g.), everything will revert to PCA, or some other form of "conjunctive feature" analysis.
		All we have, at the end of the day, are the statistics in the data.
	Still, even given this, it should be possible to iteratively-anneal representation space.
		MNIST + circles should be a good simple testbed for this.

	----
	With two different values of NL2, 100, 500 and 768, we don't get a uniform representation of all the digits. 
	Instead, it all seems to be 1, 9, and a few 8's. 
	If we change the RandomAffine to 5 (from 40), with NL2 at 256, it looks better -- though 1 is definitely over-represented. 
	Adding translate, scale and shear deformations (so easy with PIL/torch!) makes the problem even worse! 
	I think that the way key matching is being done is wrong; we need a different statistical model, or a different form of the gate variable. 
	Switching back to the gate calulation scheme (decayed outer product) from circles2, doesn't fix the problem. 
	Get a bunch more 9's replacing the 1's, fwiw.. 
	System really should be segmenting the tiny visual scene into line segments... 
		Say, shouldn't there be some calculation on the address lines or something? 
		Even if it's not invertible, make something like a nonlinear expansion, cross-terms yadda yadda?? 
		Let's think more about this in the morning! 
	After, of course, fixing the model to get segmentation working as well as the nonlinear hebbian models
	(Could just replace the outer product K-Q update with a cube or something... 
		yeah that looks like crap.)

July 15 2022
	Some thoughts from last night:
	1 -- Transformers very much has activity-activity modulation:
		The key, query, and value are all generated at the same step with one continuous weight matrix.
			Github that illustrates this:
			https://github.com/openai/glide-text2im/blob/69b530740eb6cef69442d6180579ef5ba9ef063e/glide_text2im/xf.py
			Note: Gaussian ReLU, or GELU = swish
	2 -- The 'slack variables' or 'ephermental variables' that I was talking about can serve the same purpose: they can 'route information around' based on activity.
		Ideally, they have concrete meaning, eg location, pose etc.
		These should be inferrable through slow-feature analysis + active experimentation -- that's what we do!
	3 -- Such variables need to interact with the other address variables, probably in linear and non-linear ways, as PSPs do in dendrites (neverming NMDA spikes, calcium waves, up/down states... oy)
		Transformers mix things up via their input encoding and decoding matrices.

	Just read:
	https://www.newyorker.com/tech/annals-of-technology/the-pastry-ai-that-learned-to-fight-cancer
	Proves .. yet again .. that at the end of the day, you want to search for *algorithms* to describe & perceive the data (not just feature detectors)

	Need to get the statistics of the first layer working reasonably well with MNIST first.  Then later more complexity.
		I think we can let go of principles for a bit,
		Try out various heuristics.
	First of all:
		We are matching only on ones.
		Need to match more on *all* the digits!
		Removing the population mean did not improve much.
		then removing the gating also did not improve much.
		Need to have some sort of normalization that encourages ... forces total activation to be constant per key.

		-->> Why does it appear that many of the units have exactly the same keys?
		If a query matches one key, it should inhibit other keys. Balance.
		But not WTA: the units need to be decorrelated, something I've worked on a bunch.
		Explaning-away works well for this.
		Think about other feedback first? Hard to fit explaining-away into a multi-level hierarchy.
		What about just weight decay? or LTD?
			Hard because the E and I eelments of the K-Q calculation are not separated.
			In the brain obviously PV neurons are busily doing their own thing..
		Also, in terms of activations, why are '0' and '1' very active, but '9' shows up more??

July 18 2022
	More time passing.. well.
	Decided to just straight implement feedback control, since need to experiment and alignment of encodings to data is not obvious otherwise.

July 26 2022
	Effectively re-implemented Hebbian learning, though with key normalization (normalize by incoming activity), and it works ... okay.
	The cricles aren't perfect, and some of the weights need to be negative for it to work properly, and it's still just linear projection.
	But, as before, it does tend to vectorally auto-encode the circles.
	It does /not/ automatically form a good non-negative basis, as with cortex_mnist.
	And it takes longer to arrive at the basis -- cortex_mnist forms the bases almost immediately, without GPU acceleration.
	What is this missing, again?
	Weak nonlinear interactions?

	Problem!
	cortex_mnist does not work with circles stimuli (?!)
	But it does work fine with MNIST input.
	Why?
	Seems that l2e is too low; even though the forward weights are strongly positive, the reverse weights are even more positive; it;s become imbalanced.
	Turning off boosting drdamatically slowed convergence!
	But the circles system does not work with boost = 0.33.. ??
	Nowo seems ok.
	Still plenty of residual errors, but does find a reasonably good basis for the circles.
	Not quite as good as circles3.. ?

	Eh, anyway, I'm not addressing the real problem here ... addressing!

August 1 2022
	bumps.py
	The key averaging ideaa works perfectly well, progressively tiles the space with low-d functions.
	e.g. topology of space is learnable with temporal coorellations, and nothing else.
	Now need to add feedback / allocation & use that to update the keys.

August 2 2022
	Still not clear on the feedback allocation of addresses.
	Varaince normalization helped, though!
	& This network (bumps.py) can continually represent the data, despite the adresses changing & becoming more simple
	Also it appears that the network completely fails at the cricles task.
	Issue:
	-- keys forward and keys reverse default to large DC static values.
	-- Removing the random key seed & replacing it with a noise term does not markedly change things.
	-- Adding in mean subtraction, in both time and P, improves things slightly..
		But, the network no longer immediately encodes the stimulus, as before.
	-- Alas, the learning rule does not seem quite right.. forward keys arise, then fade- ?
		Stimulus is not amenable to this averaging!
		(If the pixel values are just the x,y coordinates, then the center of the circle is easy / immediate to fall out
		But the radius is not obvious, and cannot be calculated by averaging; it needs a distance calculation r = mean( sqrt( (x-cx)^2 + (y-cy)^2 ) where cx and cy are the mean / center of the circle.
		Of course any pair of 'on' pixels can be used to calculate the distance ..
		and also of course we want an algorithm that generalizes beyond just circles!
			Like squares and squiggles and such!
		Try going back to K-Q matching??
	-- The other aspect is that, without knowledge from other pixels,
		you a priori *don't know* whether that activation is part of any particular circle.
		+ One solution to this is to add in local cross terms to the forward key calculation.
		  (Have explored this idea before.. it's obvious)
		+ Another solution is to make both forward and reverse into K-Q maps.
			Will this work?
			Say we decompose the input into an ideal 3D representation.
			Each pixel now has a 3D address ...
			Determining if the pixel is "on" is then a distance calculation:
				(px - cx)^2 + (py - cy)^2 - (pr - cr)^2 ~= 0
					Where pr = 0 (pixels should have no knowledge of radius)
			Actual distance calculation:
				sqrt( (px - cx)^2 + (py - cy)^2 + (pr - cr)^2) < thresh
			Seems we'd need at least one extra term there -- a a per-axis 'gain', which would be allowed to go negative (!!)
			(Which we already have :)
			(And also go to zero ... how to calculate or update this??
				So:
			sqrt( gx*(px - cx)^2 + gy*(py - cy)^2 + gr*(pr - cr)^2 ) < thresh
				(thresh can be constant, absorbed into gain vector).
	-- Assuming that we *know* (e.g. supervised learing) what cx,cy,cr are
		Is it possible to infer the gains on each of those address lines?
			(In addition to the c* varaibles)
		Assume that it's a central pixel @ 0.5, 0.5
		physical space is [0..1]
		any circles where dist(cx, cy ; 0.5, 0.5) = cr will activate this pixel.
		aka dist(...) - 1.0*cr = 0
		example pattern:
		0.25, 0.50, 0.25
		0.75, 0.50, 0.25
		0.50, 0.25, 0.25
		0.50, 0.75, 0.25
		0.2, 0.2, 0.424
		0.8, 0.8, 0.424
		etc.
		again, invariance is that dist(cx, cy ; 0.5, 0.5) - cr = 0
		can we somehow gradient-descent into this?
		or otherwise detect this invariance -- which is of course a parametric circle!
			Yes .. detection of invariance!!
	-- The most obvious way of detecting invariance is to straight search for it.
		Randomly combine, perhaps heuristically, different operators on the data.
		Ala "Distilling free-form laws from natural data" 2009.
			This used the heuristic of derivatives:
			Conservation equations should be both conserved (duh) and
			predict the relations between derivatives of state variables over time.
				This is probably too big of an ask in s'okae,
				but that should not detract ffrom the search for invariances in other ways.
	-- Let's see if gradient descent can solve the problem.
		Why not be lazy.
		As usual, implementing it has revealed a lacunae of thinking:
		In the current architecture, each pixel has a key and gain.
		key -> should be 2-d location
		gain -> width of activation
		realistically, you don't need independent keys / gains for all of these.
		keys are basically axis-aligned linear ramps.
		gain is just three variables used by all keys
			which is equivalent to pre-multiplying those addresses / queries.
		Assuming that we are given the 2D linear ramp keys, can the gains be learned?

cirlces_grad.py:
	Yes, SGD can estimate the gains based on MSE from all the photos + supervised 3d coordinates ...
	But, if you seed it improperly, it doesn't work.
	eg. if the radius gain is positive, it does not find the 'good' solution of
	[ 1.5 1.5, -1.2] (ish)
	Soo.. in this case, seems you need to pick the sign.

	Recap:
	To fully analyticaly do forward-graphics from cx, cy, cr using the K-Q distance measurement framework,
	In one layer,
	You need to do per-address line gain, with one of those addresses negaitve.
		(e.g. subtract the radius)
	Then, *this* needs to be passed through another distance function -- or at least square prior exponential. ('match')
	Thus, really need two layers --
		which is of course exactly what circles3.py does!
			(if imperfectly)
			& it uses one-hot encoding to do this.. which seems slightly like a cheat?

	Invarainces, covariances, descriptors, how to allocate iteratively, how to allocate properly..
		the truth is still obscured.
	1 -- Patterns which co-occur  *can* help to describe a topology of the space.
	2 -- K-Q distance calculation *can* be used (in layers) to do forward graphics.
	3 -- K-Q distance calculation *can* also be used for inverse graphics,
			But atm, this seems only for one-hot representations.
	4 -- NL Hebbian learning is good at NNMF.
	5 -- Cross-product terms between pixels can readily compress MNIST to a low-d vectoral space (20 dimensions), but this compression does not yield any classification improvements relative to straight PCA.
		-- This is probably possible with the NL Hebbian work,
			But that is limited in that allocation & self-org is driven by feedback,
			which in turn doesn't work well when the forward or reverse transforms are significantly nonlinear.
		-- Notably, with K-Q, the forward and reverse transforms are very nonlinear!
	6 -- In the circles task, supervised activation is determined as a result of an invariance relation.
		-- A priori, it's not clear how to find these.
		-- But, one hope is that restricting the building blocks + allowing implicit search (matching), invariances will be easier to find.
	7 -- Intra-layer normalization schemes (lateral + homeostasis) are effective hacks for whitening / centering.

As I was telling LiAn last night,
	1 -- All patterns of retinal ganlion cell firing is not equally likely, despite the > 100:1 compression of the retina itself (at least for luminance)
	There are common patterns, on multiple scales, and these tend to be semi-algorithmically related to underlying factors.
	Such underlying factors of causality or variation are cognitively available, and lead to a compositional nature of the world.
	Such forms of compositionality have been seen in both StyleGAN and Transformer based text to image models (w/ pre-trained language models)
	Transformers, and to a lesser extent, StlyeGANS, have a computational structure which allows pointer-arithmetic like algorithm composition.
	Said pointer arithmetic is conditional MLP-layers, which are easy to train with SGD.
	Transformers also evince an implicit searching through Softmax attention (though other topologies also work).
	MLPs, in turn, can theoretically do the same functions -- the projection allows for invariance (can ignore non-projecting dimensions), but it takes potentially many layers to combine vectors to allow something as useful as 'pointer arithmetic' (e.g. distance function must be created with two opposite-signed nonlinearities in a lower layer)
	This motivates the creation of 'address spaces' as a fundamental operation in bio-infused artisinal neural networks.
	Addressing, of course, implies an ordering of the stimuli, which either needs to be inferred through spatial-temporal statistics (correlations), or though active experimentation.
	Addressing neatly solves the memory / function approximation dichotomy, too: you can use all your data efficiently by only updating the addresses, but keeping the contents relatively constant.
	This, of course, is complicated by the potential presence of hierarchical or layered address calculation, where intermediate layers need to change both their output and input keys.
	Still, there is a way.

	It may be commented that human appreciation of beauty -- which frequently reflects some degree of self-similarity, fractial or integer-dimensioned, seems to be reflective of some fundamental operation and or assumption of the brain.  It assumes some quasi-algorithmic compressability of sensory space, and when that does happen, it can be pleasureable.

August 3 2022
	Thinking while running:
	Probably need to decompose the problem into something simpler, particuarly given the math / logic above.
	What about lines, which is also 3D (inersect and angle / slope) but can also reasonable be parameterized as 4D (two points on line).
	How would the K-Q net deal with this?
	Even simpler parameterization: just angle. line always goes through the center.
	This is very easy to parameterize, pixels have an address based on angle (slope)
	Assuming key is 1D angle
	On = distance(angle - address) < threshold
	OK, now add 1 DOF: x-intersect.
	Add in an additional key variable, 'x', which is a linear ramp
	On = distance(angle - address, k_x - a_x) < threshold
		(a_x shifts the line horizontally)
	Thus it's easy to add an additional variable, y-intersect:
	On = distance(angle-address, k_x - a_x, k_y - a_y) < threshold
		I *think* this should work, but better make sure...
	No no, the angle argument needs to be modulated, or the angle look-up needs to be translated.
	Modulation: (hmm ... please go to the notebook!)


August 5 2022
	A. Coates and A. Y. Ng, “Learning Feature Representations with K-Means,” in Neural Networks: Tricks of the Trade, vol. 7700, G. Montavon, G. B. Orr, and K.-R. Müller, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2012, pp. 561–580. doi: 10.1007/978-3-642-35289-8_30.

	Uses power-correlation between keys to organize / group K-means filters into smaller-dimensional groups for subsequent K-means (e.g. hierarchical!)
	Interesting / smart.

	Also just discovered UMAP!  Beautiful explanation at
	https://umap-learn.readthedocs.io/en/latest/how_umap_works.html
-------
	Had lunch with Jascha, some salient feedback:
	1 -- Do you really need RBF representation?
		Divisive normalization might well be good enough.
		DN constrains the vectors to be on the surface of a sphere (if you divide by the l2 norm), or some other geometric form with a different norm.
		This is sorta less interesting in high-d spaces, where most of the mass is at the periphery anyway .. ???
	2 -- He thinks that the current transformer architecture can't possibly be perfect -- how did the original authors back in 2017 get many of the details correct?
		I think that there very well maybe an engineering plateau (which is what I said), but more relevantly, if transformers enable algorithms like pointer arithmetic, then while there may be better ways of training them, there really aren't more fundamental ways of representing pointer (or vector) arithmetic.
		Or, perhaps, doing matching other than cosine distance.
	3 -- JSD thinks that there is much opportunity for networks that learn to ignore or take advantage of invariances.
		This, of course, can decrease the representation dimension, which makes learning easier (perhaps consistent with his theoretical work?  I should read it..)
		Says that current networks can have one or two transition points corresponding to dimensionality reduction, but there is much room for more, particuarly in the algorithm design.
		This relates intimately to above, and ideed to what I've been fiddling with.
	4 -- Some discussion of ethics of AI research, what human jobs will be automated first, and moralistic ambiguity of working it (you are accelerating it's arrival, but also have some leverage over its use?)
		This segued to the current polarization problems, and how imperfect algorithms are already really good at gaming human minds.
		His point that patches can be applied to software, but not to humans (!) ... we're really going to need some collective action here / laws to regulate advertizing and mind-manipulation.
		Case in point: even mediocre algorithms are quite good at propagating misinformation and deeply messing with society.

	Re 1 & 2: True, softmax is divisive normalization, where alignment is a dot-product, not distance measurement.
	(see link July 15 2022)
	 weight = th.einsum(
            "bthc,bshc->bhts", q * scale, k * scale
        )
	here q and k are activity vectors from the lower MLP.
	This is probably better for gradient-flow; the normalization distributes the gradient to other members of the same vector.
	I've been thinking mostly about l2 distance while trying to do forward graphics on the circles and lines etc.
	Absolutely worth trying to do the same with cosine distance or divisevely normalized dot-product (as in softmax).

	Would love to have some intuition on what exactly it's doing with space; perhaps my addressing ideas are wrong?
		Well, in computer RAM, perfectly entangled addressing can be achieved not only by distance calculation (which has the unfortunate effect of sign change of the derivative, hence optimization is less straghtforward)
		but also by dot-product & threshold, if you include the complement on the address bits (as with the XOR task, way back when), and set the threshold to match on *all* bits.
		Provided the complement is perfect, this will work
			if it's not, then can cheat by setting everything to 1 and everything is selected.
		Anyway, if the input is not normalized, then it does succumb to attack -- dot-product if the vector is eg [5 0 0 0 0] selects 16 things .. not one of 32.
			(distance : would select nothing)
		But if its normalized, then yes basically have a map which is the surface of a N-sphere, rather than a N-dimensional cartesian space, and cosine distance is probably just as good as cartesian distance.
		So, equivalence?
	I think there is probably something deeper here.
	UMAP e.g. is already decent at classification, and that uses a very simple euclidean distance in high-d (k nearest neighbors) to map down to low-dimensions (using a binary cross-entropy loss).
	In supervised learning / classification, we know the approximate distances, at least the distances for same-class objects (low), which is non-euclidean; you then tune the parameters of the network to minimize this 'distance'.
		Distance is another way of representing multi-dimensional ordinality,
		and SGD is a way of converting between the two,
		Where location is represented in high-dimensional, possibly hierarchical activation space.
		Yet if we have actual location / ordinality, should use that.

	Why not just have some sort of datastructure that represents the distances / ordinality directly??
++ Use distances
	(including various internal metrics, such as L2 norm, linear correlation, power-law correlation (L4?), explain-away feedback)
	when self-organization is required.
++ Use supervised information
	(eye movement, head movement, any other form of ineraction - manipulation)
	when possible, + assumption of slowly-changing world
		Yes!

	A current hypothesis is that the brain uses sequential one-hot and vector representations; for example, saccades can move an object between hemi-fields, which at the level of V1 will completely change which subsets of neurons are active (kinda like Laplacian pyramids / mipmaps / common graphics pipelines..)

	With L2 addressing & variable width (might be a better way - ed.), then selecting different subsets of V1 neurons based on location is trivial.
	Extracting that information is a bit harder; might need some sort of iterative voting scheme?
	At each level you need to infer some address bits (vectors)
	With cosine / softmax addressing, instead of moving linearly through the space (e.g. position is encoded in rate (mgiht be hard for neurons))
	You instead can rotate through the space.
	But one rotation is still really just parameterized by two vectors: x and y, which are orthogonal (easy, most are) and concevably -x (? maybe not)
	Or could be two sets of vectors X and Y ... Probably this doesn't matter so much, its still a 1D change, what really matters is propagating this to lower levels.
	So, if now we have two hemi-fields, the next vectoral representation needs to convert the full sweep of that vecor (in either scheme) into either nothing (not in hemi-field) or a linear ramp, e.g multiply by 2, add offset, threshold.
	And so on, down the hierarchy.
	Obviously this is not so hard to implement with local or distributed normalization, but how ever could this be learned?

	Distance and keys seems to make this simpler, because you just tile any space as necessary, and learning is obviously hebbian / local.
	But, hierarchy!
	Need to go back to the circles / squares test / toy problems.  Must be a way to adjust keys and queries to auto-organize.

August 8 2022
	(edited above as well)
	To really think about it, the visuo-sensory world is not just 2+D (~2M axons from the eyes), but dependencies extend in time as well, which you can imagine as sampling the world ~60 times / second.
	The task of a memory is to store as much of this information as reasonable (throwing away ecologically appropriate details), and make it as accessible (indexed and auto-encoding) as possible.

	Seems to require a degree of local-greedy algorithms + iterative, up-down reorganization... there necessarily will be multiple heuristics.

----
	Getting back to the circles task, job now is to organize those feature vectors so they can be indexed by local small translations, local orientation changes.
	Maybe an down-up-down algorithm would work here?
	E.g. use the alignment of forward and backward keys to make the maps place similar activations physically close to each other,
	And also address-wise close to each other
		But this is merely a consequence of forward-backward symmetry?
	Bottom-up: integrate information to both disentangle and form coherent whole objects.
	Top-down: organize tuning & responses to make the aggregate feature detection - selection work.
	Also, probably, perform perceptual fill-in to the correct and auto-adjusted level.  (no hallucinations, visual ('optical') illusions OK)
		"Push things back onto the manifold" .. but this has to be coupled with learning the manifold itself!
			I wonder if you lived in a world where the assumptions that visual illusions violate were absent ... would they still be illusions?
		How can a top-down signal work for both organization and fill-in?
	Anyway, currently the allocation of units in all the networks is essentially random, which is unphysiological.
		Even higher in cortex I assume there is a local topology?

	Experiments with UMAP
	conclusion: UMAP can recover the latent 3D structure directly, reasonably robustly, and quickly. (with default parameters!)
	you do not need any hierarchy or whatnot,
	but you do need more samples than the number of dimensions.
	e.g. 2000 works, but 500 does not)
	from the map it is somewhat trivial to invert (map supervised signal to embedded dimensions, do interpolation of the examples in embedded dim)
	but not totally clear that this is an 'algorithm' per se.
	instead, it's a detailed bidirectional map, which *could* concevably be used to make an 'algorithm'
	better would be to use the umap algorithm to discover the compression / decompression algorithm (??)
	make the map then compress it.
		Aside: need to look up how semi-supervised umap works.

August 10 2022
Hi Jascha,


    The nice thing about DP in a normalized space is also that every input will be represented by some set of active features. For RBF kernels in a high dimensional space, most units will be off for most inputs. It's hard to tile a high dimensional space with Gaussian bumps ... you need an exponentially large (in the space's dimensionality) number of Gaussian bumps to do it. (another option would be to use RBF activations, but with something like divisive normalization, so that a few are active for any input. This seems likely more brittle though.)


Yes, that makes good sense, thanks!

I'm trying to reconcile two ideas:

One is that algorithms like umap and t-sne are sample efficient and can (sometimes) learn useful compressions/indexes on the data.  They work on distances, like RBF, and are invertible.  These mappings offer implicit allocation of nonlinearities, I think.

The second is that normalized activations are much more amenable to high-d spaces, like you say, and they can be readily stacked into a hierarchy.
This hierarchy is necessary because each layer offers only one explicit hyperplane nonlinearity per output dim.

Can you somehow combine both?

1: For example, t-sne mappings of the digit '7' organizes into a 2D map of attributes like angle, length of legs, and presence or absence of the continental cross.  These are variances with meaning, with kinda complicated forward nonlinearities -- and very complicated inverse nonlinearities (via manifold approximation... it becomes like a lookup table)

Right, a deep MLP aims to approximate a function via a series of summed, rotated, scaled hinges / nonlinearities (example: fitting a bump, see test_sinc_mlp.py).
	And it sometimes has problems with this, since it can't really multiply terms.
T-sne and umap aim to approximate the manifold of the data directly, with the primary constraint that it needs to map down to a handful of dimensions (usually 2 or 3..)
	Speaking of which: this is exactly what James DiCarlo was talking about!
		J. J. DiCarlo, D. Zoccolan, and N. C. Rust, “How Does the Brain Solve Visual Object Recognition?,” Neuron, vol. 73, no. 3, pp. 415–434, Feb. 2012, doi: 10.1016/j.neuron.2012.01.010.

In the test_sinc_mlp example, there isn't even much rotation, actually!  Nor do weights frequently (if ever?) cross zero.
Furthermore, the way that it approximates the function is sorta non-obvious: take a positive slope (with bias, i assume) and subtract off negative hinges.
Again, I say: "Just barely enough of a nonlinearity"

A RBF approximates the same sinc function using .. one central bump or 'key'.

2. A second big disadvantage of RBF or UMAP is that the resulting transform is much, much slower than if you implement it as a clean fast MLP or even transformer.
This is because I think you need to do a bunch of comparisons with the existing data (??)  Should look ups.
	Yes, umap relies on PyKNNdescent library for finding the N-nearest neighbors of a datdapoint.
	I think for transform, though, it does not use parallel threads (?)
	PyNNdescent is multithreaded.

Here is an example run of circles_umap.py:
umap fit 28 64000 (200704, 25)
UMAP(n_components=3, verbose=True)
Thu Aug 11 12:22:48 2022 Construct fuzzy simplicial set
Thu Aug 11 12:22:48 2022 Finding Nearest Neighbors
Thu Aug 11 12:22:48 2022 Building RP forest with 18 trees
Thu Aug 11 12:22:48 2022 NN descent for 16 iterations
         1  /  16
         2  /  16
         3  /  16
         4  /  16
        Stopping threshold met -- exiting after 4 iterations
Thu Aug 11 12:22:56 2022 Finished Nearest Neighbor Search
Thu Aug 11 12:22:56 2022 Construct embedding
Epochs completed: 100%| ███████████████████████████████████ 200/200 [00:26]
Thu Aug 11 12:23:32 2022 Finished embedding
umap transform
Thu Aug 11 12:23:33 2022 Worst tree score: 0.13764062
Thu Aug 11 12:23:33 2022 Mean tree score: 0.28430208
Thu Aug 11 12:23:33 2022 Best tree score: 0.85714062
Thu Aug 11 12:23:35 2022 Forward diversification reduced edges from 960000 to 714586
Thu Aug 11 12:23:37 2022 Reverse diversification reduced edges from 714586 to 714586
Thu Aug 11 12:23:39 2022 Degree pruning reduced edges from 799768 to 799762
Thu Aug 11 12:23:39 2022 Resorting data and graph based on tree order
Thu Aug 11 12:23:39 2022 Building and compiling search function
Epochs completed: 100%| █████████████████████████████████████ 30/30 [00:09]
Thu Aug 11 12:25:19 2022

Yes so ... "Building and compiling the search function" is an expensive operation (the most expensive?)
	It is not multithreaded.
	And is needed for "transform" function.
	Seems to take about a minute and a half.
	It's in /home/tlh24/.local/lib/python3.10/site-packages/pynndescent/pynndescent_.py", line 1635, in query
Next up is building an embedding, 26 sec.

So NNdescent does have GPU implementations, but they might not be as efficient as bruteforce for high-dimensional data, see
https://docs.dgl.ai/en/0.8.x/api/python/knn_benchmark.html
bruteforce = compute all N(N-1)/2 distances, sort and select the k smallest per row..
It also seems that KNN starts to fail when data dimensionality gets above ~ 50 ... weird, because umap (which uses NNdescent) works fine on MNIST and semi OK on FashionMNIST, and these are 784-dim.. perhaps the paper is too pessimistic on recall, or perhaps recall is not that important to umap?
Seems that umap uses a RP forest for NN descent.
... which is kinda funny as RP forests have much lower recall than pynndescent.  Guess it's useful as a first pass at least.
	ref: https://github.com/erikbern/ann-benchmarks
+ RP forest:
	It works by building a forest of N binary random projection trees.

	In each tree, the set of training points is recursively partitioned into smaller and smaller subsets until a leaf node of at most M points is reached. Each parition is based on the cosine of the angle the points make with a randomly drawn hyperplane: points whose angle is smaller than the median angle fall in the left partition, and the remaining points fall in the right partition.

	The resulting tree has predictable leaf size (no larger than M) and is approximately balanced because of median splits, leading to consistent tree traversal times.

	Querying the model is accomplished by traversing each tree to the query point's leaf node to retrieve ANN candidates from that tree, then merging them and sorting by distance to the query point.

		Note though: pyNNdescent uses built-in rp_trees.py, not an external library.

Well still .. I really like the concept of the NNdescent algorithm.  Start with a random collection of k nearest neighbors, then join over their neirest neighbors, add in reverse neighbors, and select from this larger set neighbors that are closer.
The problem with any NN metric is that proximity becomes less and less meaningful with larger dimensions.
	MNIST probably has locally low dimensionality, which is why it works..

Experiment: Adding transformations to MNIST unsurprisingly makes clustering via UMAP harder.
See screenshots/20220812_umap_mnist_native.png and screenshots/20220812_umap_mnist_transformations.png
	End-goal would be to have something that can approximately extract transformations..

Tested umap on cortex_andor5.py, which does nonlinear compression of the vector space.  It does worse that untransformed data (the digits are smushed into each other), even though one-hot prediction was better than PCA prediction (and presumably PCA is just as clusterable, if not better, by umap).
	Well, this system was not per se engineered to represent invariances;
	instead, it was engineered to represent bilinear probabilities (pairs of pixels that are on at the same time..)
	The pairs of pixels seem like a decent compression, but apparently do not provide processing gain to improve digit recognition.

August 12 2022

Likewise, translation, skew, scale & whatnot do not hinder human perception noticeably, but they strongly effect umap and t-sne.
	Interestingly, I think that we do a version of internal mental rotation (latent variable inference?) when perceiving characters that are highly skewed.
	Much more common, stereotyped letters are basically instantly recognized.
		("amorized inference"... but how to add a de-translate module?
		I'm assuming that it involves a degree of content-agnostic addressing, still)

Read over
H. Tang and K. Ellis, “From Perception to Programs: Regularize, Overparameterize, and Amortize.” arXiv, Jun. 13, 2022. Accessed: Aug. 12, 2022. [Online]. Available: http://arxiv.org/abs/2206.05922
And i think the general scheme, of using reparameterizing, overparameterizing, then regularizing makes sense!
To unpack:
	Make much longer programs than necessary.
	Parameterize them variationally using the gumbell-softmax trick
	Regularize after 10 epochs to encourage parsimony.
However, I think amortizing this inference via MLPs is ... no good. Need something more powerful, at least.

Also notable is https://dselsam.github.io/the-terpret-problem/
"The terpret problem" -- shows how, fundamentally, SGD is liable to getting stuck in local minima.  With a toy problem.
	What if you break ties better??  Might be better solved by changing the representation / substrate, rather than SGD.

Did end up sending an email to Jascha.  His email + draft is above; for completeness, this was sent:
	I'm trying to reconcile two ideas:

	One is that algorithms like umap and t-sne are sample efficient and can (sometimes) learn useful compressions/indexes on the data.  They work on distances, like RBF.  These mappings offer implicit allocation of nonlinearities but are slow to interpolate and slow to invert. I think.

	The second is that normalized activations are much more amenable to high-d spaces, like you say, and they can be readily stacked into a hierarchy.
	This hierarchy is necessary because each layer offers only one explicit hyperplane nonlinearity per output dim.

	Wondering if it's possible to have the advantages of both...

I do think there is some meat there!!!
Transformers have an advantage in that each layer + head has explicit search!
& each of the K, Q, V have both activity and bias components (hence allowing static keys, as needed..)
Wondering outloud, maybe a good approach is to re-engineer transformers to work better or more scalably with image input data.  Since they are so good.
	Use the same tricks as above.

Other thought that I've been kicking around, since I've been digging into UMAP
	(which utilizes pyNNdescent for making a K-NN graph of the data
		(which in turn  based on RP trees)),
is that perhaps adding appropriate levels of algorithmic complexity (e.g. search, fast tree-based datastructures, iterative organization of maps, iterative regularization / program-extraction) might be the right way to improve representation-learning.
Really really keen on iterative regularization!  But for that, you absolutely need a substrate that permits the expression of algorithms.
Which entails, at minimum:
1. Constants (e.g. weights)
2. Arithmetic (+,-,*,/)
		clamp or relu
		maybe log, exp, sin?
3. Variables
		In a MLP, the variables are always bound to a channel = activity.
		In a transformer the variables are implicit / unnamed & exist in the latent space.  Which is a fixed dimension but so what.
			Amazing that SGD can pull useful structure out, given that it doesn't commonly change sign in MLPs...
				And probably never does with ReLus, let's try leaky ReLu.
			(Suggestive that it's just finding existing workable structure from the random initialization)
		In a hypothetical K-Q network, again the variables aren't named; instead, they are addressed. Which leads to..
4. Computed addressing
		aka pointers.
		If you can address variables, you can variably bind them, do scatter / gather, etc
		Maps -- and especially maps with vector representation, so that you can do vector math ( = pointer arithmetic) seem like a good substrate for this)
5. Conditional execution / If-then-else
		This can be accomplished via nonlinearity and then computed addressing, probably as happens in Transformers.
6. Loops?
		Can be constructed via recursion and conditional execution, which is elegant but ...
		These are used in programming languages for reductions (sum), which is a built-in in all neural networks.
		They are also used for projections, another built-in.
		Projections can in turn be used for expansions and outer products.  All built-in.
7. Search?
8. Nice datastructures, like trees, maps, dictionaries etc?
		These are of course both built in to high-level languages such as Python.
		And took a good bit for humans to invent them.
			There are undoubtably more datastructures to be invented
		Seems useful to have them available.
		See also: https://news.ycombinator.com/item?id=32186203
		Union-find?

Seems like the sort of problems that we need to solve, namely make an auto-associative, self-organizing, invariance-finding, compositional-disentangling memory or artificial cortex
	Might be solvable with the right (set of) datastructures?

Most of the datastructures / algorithms in that link above approximate space fractionally, e.g. with a tree.  Sometimes stochastically, sometimes using the curse of dimensionality (eg hash tables), always with some end-use in mind.
Union trees are nice in that they amortize parent finding during the search itself!

The real visual world is of course much more entangled than any of these datastructures can support; they are all more-or-less bijective, whereas in pixel space, everything is super highly multijective.
One initial concept would be to have built-in 'transforms', and have a system that can estimate the parameters to those transforms based on search and later shortcut connections (and if these fail, some sort of complexifying map).
E.g. a map which transforms, controlled by a map that iteratively complexifies.
For this to work, I think, we do need 'computed addressing' ... problem is, that RBF type stuff does not work that well in high-D
	Solution would be to
	-- use dot-products,
	-- stick with lower dimentsionality,
	-- do smarter normalization as is likely happening in the cortex.
Looks sorta like capsule networks
	What ever happened to these?  Why have they not taken off?
	Looks like people implemented them a while ago, and the repos haven't been touched / updated.
	Ah, seems as from here:
	https://analyticsindiamag.com/what-happened-with-capsule-neural-networks/
	that transformers offer a less clunky and more generalized routing algorithm.

August 16 2022

From https://www.nature.com/articles/d41586-022-00018-5
	Janelia separates its large-scale projects, which are professionally staffed and managed, from its academic research projects. The latter are staffed by students and postdocs in small labs led by principal investigators. But such institutes are few and expensive, and are difficult to create. Furthermore, their nimbleness is hard to sustain. Janelia’s first director, Gerald Rubin, wrote in 2019 that “without an opposing force provided by management, there is a slow, steady drift toward a more conventional environment increasingly focused on maintaining successful programs and documenting individual achievement at the expense of risk-taking and collaborative, interdisciplinary work”4.

August 17 2022

Changing direction: rather than trying to figure out a new network architecture / new datastructure / new way of training both,
I'm going to merge these three:
+ Kevin Ellis, DreamCoder
+ d'Ascoli, Deep symbolic regression
+ Ye, Efficient Zero

Namely, to change the MLP in DreamCoder to a Transformer,
	then incorporate RL to guide the program synthesis search.

Todo: make more sense of the program, probably by tracing it out via PDB.
What are the grammar neural nets?  How do they work?  What data is passed between the different modules?
	See: https://github.com/ellisk42/ec/blob/master/docs/software-architecture.md
	from: https://github.com/ellisk42/ec

August 18 2022

Ben S: Nextflow seems really cool!  Works well for the methylseq information.
The reads are 90 million characters long --hence need to run the analyses massively parallelized.
Use AWS batch for this, via the Netxflow / Nextflow tower / Docker image with the requisite software.  Smart!

August 19 2022

Ok, so I'm a big dummy, and removed the Jane Street capital 'core' library and replaced it with the standard library .. .duh.
Turns out the type signatures for the two are quite different, and at this point probably best to stick with the presumably mature JS lib!
Well, at least I have a better understanding of the code -- and have refreshed my Ocaml knonwledge.


August 23 2022

Running into ocaml configuration issues recompiling DreamCoder ... very annoying.
trying a fresh install of ocaml via opam.
run
eval $(opam env --switch=4.14.0)
To select the non-debian-supplied version.
to update an environment after installing things:
eval $(opam env)

September 4 2022
Tuxedo NY
Have the dreamcoder solver directory fully compiling,
and the list / text tasks seem to be running fine --
but the logo task is not working.  It bombs while reading in a huge huge block of json -- something too large for kate to work with.  Way too many parentheses!

Been reading a lot of program synthesis literature; dreamcoder is good (need to revisit the E-graph or equivalence-graph enumeration ideas + the version space algebra (which is?) plus the fragment grammars
	All these seem like essential and clever means of managing the exponential search space of programs.
	Which otherwise is insurmountable.
My instinct is that we need an datastructure + algorithm that is some intermediate between symbolic logic (discrete), programmimng languages (discrete / terse, good generalization) and super easy to fit (liner regression / MLPs etc.
Transformers of course exist in this intermediate space; they can evince algorithms, but can be trained via gradient descent.
Program synthesis of course uses much much fewer examples, compresses greatly, and there are efficient means of search (SAT solvers), but does not have the fluidity and robustness that comes from opearting on features in high-dimensional spaces.
	Of course, part of the effectiveness is synthesized programs have relatively few parameters;
	The complexity is in their *structure*
		Again this is exponential to search.

I keep wondering if there is a way to have a continuous relaxation of "maps" to "algorithms" to "linear classification and projection"
	(to try to tie in some of my older work on unsupervised / compressional learing..)
	Are there ways to hierarchically tag and label datapoints?
	Turn a map or u-map into an algorithm?
	Make functions that operate on high-d spaces?
		Then again, not perfectly clear how essential recurrence is for important things like vision.. ?

Anyway, a more important proximal thing to do is to merge DreamCoder with transformers!!
See https://veo.io/index.pl?pid=1571
Need a demo!!
Then also need to pull in Github codes, ala Copilot.
	And think about using transformers on other axes of the task..

September 10 2022
	Back at it -- logo task is not working, need to debug it.

Note:
– General to specific learning. Start from all true and refine
with the maximal (consistent) generalization.
– Specific to general learning. Start from all false and refine
with the most restrictive specialization.
– Version space learning. Keep all consistent hypothesis
around – the combination of the above two cases.

"Fringe" used in DreamCoder refers to the version space algorithm.
(reference is in Dropbox and Zotero, search for "CS 2750". 

September 13 2022
Ok, so the logo task is working!  
At least, it's stably producing drawings ..
Does not seem to be evolving in any respectable way toward a functional library.  That is, the drawings are getting no more complex, and it's solving none of the 160-odd tasks. 
Still, I'm using the ocaml compressor, and not the pypy compressor as orginally applied. 
Also, when I tried the rust compressor, it errored out -- perhaps there is something wrong with the ocaml compressor / library generator?? 
Suppose I should re-try this with pypy, again ~overnight. 
	(Wonder why they need to use the huge memory instances on google cloud -- maybe the pypy compressor uses a ton of memory? )
	(Also, maybe I should just wait longer?  It's been 10 epochs out of 20, so about consistent with the reported wall time (one day with 64 cpus), but a resonable expectation would be that in this time, it will make progress toward the goal: at least some of the tasks (e.g. drawing squares) will be completed, rather than bopping around with curves and such. 
	Hmm.  Will stop it after swimming.  
	Might be noted that the ocaml runtime is probably not that efficient..


September 15 2022
Ah!  Where does the time go?
Seems like my blocks of time keep getting broken up, and that work is always the last prority..
Anyway.  What I've been meaning to write for a while:
Why not *learn* both the equivalence graphs
	with, e.g. memorization of common semantic structure / dataflow graphs?
		This is more similar to the way that humans do it.  Looking for analogies / loose matches in the dataflow graph, and backing that to semantics and associated syntax.
	or, with a map from the details of the language under test (e.g. python)
	to an underlying e-graph datastructure, which can then be factorized.
	The factorized representation can subsequently be converted back to the written programming language.

Need a datastructure for mapping between domains (syntax, semantics, dataflow) and performing approximate key-query match (with segmentation) on the database.
	Said datastructure needs some degree of compression, ala E-graph (egg library) (redundancy reduction, very smart) or hash consing (with again the segmentation problem.)
		The PAC learning paradigm (upper fringe, lower fringe) offers a good means of learning equivalences based on experimentation.
	Ideally only want ~1 fundamental datastructure, approximately independent of language (modulo functional languages?), and most representative of underlying best-practice computation...
		Then can have multiple maps to and from it.
			I think that this is sorta what transformers are doing internally.

This datastructure or algorithm needs to allow for *abstraction* aka library (or motor primitive learning), as shown powerfully in DreamCoder.
You can only conquer the curse of infinite dimensionality by hierarchically breaking up the problem.

Sept 16 2022

Serious problem with DreamCoder: the dreams are insane!! Some of them are 100kb long!  There is basically no way in hell to learn input-output relations from such complicated and cleary chaotic behavior. 
A human would never go about things in this way.. 
I wonder if I've messed something up.  Well, I can fix it. 
	Right: doesn't the paper say that programs are enumerated, rather than sampled stochastically? 
	Seems like many of them are total run-ons, huge chunks of meaningless code or code that does not affect the output & should be culled. 
	Also it's clear that the syntax of the logo tasks are very loose, e.g. "FWRT" (forward, right turn, which should take a length and an angle) has all sorts of extra arguments, which may or may not be executed??
Idk, this is a bit of insanity, the search should definitely be tree based and breadth-first, with some sort of culling based on equivalence.  
Also, it takes much too long to execute the tasks.  
I will instrument this. 

Sept 19 2022

Trying to understand the implementation of lambda calculus. 
slanted square task. 
Input to parser: 
(
	(move 0d (/a 1a 3))
	(loop k 4 
		(move 1d (/a 1a 4))
	)
)
pretty smiple.  Rotate, then draw a square. 
move takes distance and angle arguments; aka logo_FWRT
Output of parser: 
(lambda 
	(logo_FWRT logo_ZL (logo_DIVA logo_UA 3) 
	(logo_forLoop 4 
		(lambda (lambda 
			(logo_FWRT logo_UL (logo_DIVA logo_UA 4) $0)
		)) $0
	)
	)
)
So, where does that $0 come from, and what does it mean? 
Is it a continuation or something? 
ANd why do we need two lambda's there? 

Try again. 
Input: 
(
	(move 0d (/a 1a 6))
	(loop i 7 
		(move (*l 1l i) (/a 1a 4))
	)
)
Output: 
(lambda 
	(logo_FWRT logo_ZL (logo_DIVA logo_UA 6) 
	(logo_forLoop 7 
		(lambda (lambda 
			(logo_FWRT (logo_MULL logo_UL $1) (logo_DIVA logo_UA 4) $0)
		)) 
	$0
)))
Right, the $0 must be part of the syntax .. it's the de Brujin index of the lambda expression or environment that is implicitly returned. 
Likewise, $1 is the index of the loop counter. 

Now, let's investigate the Grammar generator. 
Why does it produce runon labda expressions of zero meaning?? 
Seems that it's initialized to 'uniform' -- which is not (at all) the way that exploration should be proceeding. 
	(Think I will try a little bit longer to reform DreamCoder before switching to a reimplemenation... 
	Clearly there is a lot of cruft here that can be removed or refactored!)
Looks like ocaml 'solver' program never returns any functioning programs. 
hum. 
Also seems like it might not have been a great architectural choice to split between python and ocaml?  Forces repetition. Makes debugging harder.  Discourages use of pytorch. 
Alright, let's start from scratch, sigh. 
	(Of course need to take as much inspiration from DreamCoder as possible)

Sept 20 2022

I think I'll need to re-implement the interpreter ... the (typed) lambda calculus is too low-level and clunky. 
I deally we'd just use ocaml directly, but making an interpreter gives far more control over error messages and decompressing the execution graph etc. 
Which, I think, is essential. 
Should be pretty easy for just Logo tasks.  
	MAy eventually head back to the DreamCoder implemenation, but for now this will be fun! 
	Realized that last night: snice I'm not really being paid for this, it damn well better be fun! 

Sept 28 2022

Tim Hanson <sideskate@gmail.com>	Wed, Sep 28, 2022 at 11:44 AM
To: Timothy Gardner <timgardner314@gmail.com>

https://www.zdnet.com/article/metas-ai-guru-lecun-most-of-todays-ai-approaches-will-never-lead-to-true-intelligence/

    He has abandoned his prior faith in using generative networks in things such as predicting the next frame in a video. "It has been a complete failure," he says.


I think this is a bit extreme -- clearly we humans are doing some degree of predicting what's coming next in our sensory experience. (e.g. expectation mismatch, P300 etc)

    And the reason it's a complete failure is, again, because of the constraints of probabilistic models where it's relatively easy to predict discrete tokens like words because we can compute the probability distribution over all words in the dictionary. That's easy. But if we ask the system to produce the probability distribution over all possible video frames, we have no idea how to parameterize it, or we have some idea how to parameterize it, but we don't know how to normalize it. It hits an intractable mathematical problem that we don't know how to solve.

Sure seems like diffusion models offer a good solution !

    LeCun decries those he calls the "religious probabilists," who "think probability theory is the only framework that you can use to explain machine learning."

Agree.

    JEPA architecture actually tries to find a tradeoff, a compromise, between extracting representations that are maximally informative about the inputs but also predictable from each other with some level of accuracy or reliability

This is a great insight.

    much of the deep learning community seems fine going ahead with something that doesn't have common sense

Which is fine -- there are plenty of humans, and they generally have common sense.  Our moral imperative is to make systems that do things we can't ... right? (like AlphaFold).  We only follow human intelligence because it's one of the few working examples that we have.

    Those signals, they're called conjugate variables in Lagrangian mechanics, but in fact, they are gradients.

Very interesting, I hadn't realized it.  Many forms of optimization are mathematically equivalent ...
He makes a good point about online optimization of latent variables being an inference problem, and we're definitely doing something like that in our visual system.
Still, 'optimization' is isomorphic to having a cardinality of solutions (I'll argue), which is where you get the connection with data-structures and ordering.  Ordering offers a different intuition for guiding thinking.
The great benefit of ordering is that it's inherently multi-dimensional, vs. energy, which is unidimensional afaik.  For example, with a database of motor-action primitives, you can order sequences of activations over joint angles, then over observed endpoints, then even higher things like target of a thrown object; each ordering is non-exclusive and can be combined.

My working hypothesis of what the cortex is doing is something like an ordered map or dictionary, where part of the ordering is supervised from motor variables, and part is purely invented based on combinations of observed variables.  This is the same as the (distributed) latent variables that LeCun talks about.

I've been learning the game 2048 (it was on my computer during a long flight) and have been instrumenting myself as I get better.  It seems that something like a map of (outcome + action) indexed by (decomposed input configuration) is a key component.

    Borrowing language from statistical physics,  energy-based models posit that the energy between two variables rises if they're incompatible and falls the more they are in accord. This can remove the complexity that arises in "normalizing" a probability distribution.

Aha.

Have to think more about this. And check
https://www.zdnet.com/article/metas-ai-luminary-lecun-explores-deep-learnings-energy-frontier/

On Tue, Sep 27, 2022 at 5:26 PM Timothy Gardner <timgardner314@gmail.com> wrote:

    https://www.zdnet.com/article/metas-ai-guru-lecun-most-of-todays-ai-approaches-will-never-lead-to-true-intelligence/

-------

So!  Now have the parser -> png system working fine, apparently. 
Time for the convnet + transformer. 
So ... python.
Want this to be entirely bootstrapping: start with naive grammar generation, learn input-output mapping... 
Maybe it makes sense to think about this first in terms of databases, rather than transformers.. ? 
	Transformers might not have the sample efficiency for bootstrapping.. eh, still worth trying.  
		Scale might be the solution.

Question: Representing a program as a string is pretty inefficent and redundant. 
Far better to represent as a AST or so. 
Q is .. how to linearize the AST?  Transformers expect vectors of tokens, not trees of tokens.  
I guess you can linearize it with either breadth first or depth first. 
With, perhaps, strict tab rules like python. 

So, a program like: 
(
	(move 0 (ua / 6)) ; 
	(loop 0 7 
		(move (ul * v0) (ua / 4))
	)
)
is expanded to: 
(
	(move 
		0 
		(/
			ua 
			6)) ; 
	(loop 
		0 
		7 
		(move 
			(*
				ul 
				v0) 
			(/
				ua 4))
	)
)
encoded as: 
(AST pos) (Argument no) (Token)
0 0 Seq
1 0 Move
2 0 Int 0
2 1 Divide
3 0 Unit_angle
3 1 Int 6
1 1 Loop
2 0 Int 0
2 1 Int 7
2 2 Move
3 0 Mul
4 0 Unit length
4 1 Variable 0
3 1 Divide
4 0 Unit angle
4 1 Int 4

Since we have more than one channel, can encode the AST location, argument location, and token each separately.  
Argument No. is of course redundant, but I think this could be useful.  

Now, editing the AST ... hmm. 
Just adding one token won't really work; need to add multiple, perhaps variable, tokens at once.  
Or we can use a library, and copy-paste?  
If not, we need the transformer to sequentially output tokens like
Insert 1 1 Move (which moves Loop to 1 2)
Insert 2 ... wait, no. 
'address' in an AST is variable length. 
You specify which one of the leaves at each branch you want to access. 
This is kinda tricky with a transformer, and is probably easier with a datastructure of sorts... 
Then there is also graph neural networks, which can work on branching datastructures (right?)

October 4 2022

OK, have the python <--> ocaml interface working somewhat ok now.
Sure was a lot of plumbing to get this working ... I wonder if it would have been easier to just stay 100% in python (was listening to the Lex Fridman interview of John Carmack, and he emphasized that single-language projects are desireable..) 
Anyway, it's done, and this interface can now be used for other things as well. 
	(Clearly I need to temper my ability at both Python and Ocaml.)
	
Next up: emit programs. 
then, have to emit programs that are well-formed. 
then, have to emit programs that produce images. 
then, have to emit programs that produce (and conditional on) desired images

ok, need to generate random programs & see if they produce any output. 
seems like we just need a vocabulary. 

tokens = ["(", ")",";","+ ","- ","* ","/ ", 
			 "move ","loop ","v","ua ","ul ", 
			 "0 ","1 ","2 ","3 ","4 ","5 ","6 ","7 ","8 ","9 ", 
			 "eof"]

also seems that purely unstructured search is not likely to find anything .. ?
as in, start with ()
insert each of above.
the integers work, everything else doesn't
move plus one extra argument doesn't work
move plus two arguments works for integers and ua / ul
two integers doesn't work, but integer binop integer does
loop needs three arguments 
 	variable, count, body
but even that's complicated, body needs parenthesis
	yet it can be discovered by having a empty body. 
	count meanwhile needs an int or something that evaluates to an int
	likewise for variable. 
plus, need a semicolon between sequences (after pairs of parens)
need to think about this.  Enumerating hundreds of thousands of programs is not really a problem, if we have a sane way of going about it -- and can train a transformer / stat model accordingly. 

In the pool was thinking that we need a library -- templates, like an original genome, to modify from. 
Even if we start from nothing, still want a library of well-formed primitives for directing / constraining the search space / memory for incremental learning (as seen in DreamCoder)
	Of course, ideally want an efficient datastructure for this... maybe just boosted trees? 
The point is to make a model (or: datastructure + model) that "knows enough" about the DSL that most of the time it only emits well-formed programs. 
	Later, agument this to emit well-formed programs that solve a problem. 
Editing (insert, replace, delete) is a good bit more complicated than the typical transformer sequence-to-sequence learning, but is more inline with what humans do (tweaking; flexibility)
	I still think it requires an external library
	and some sort of probabalistic structure. 
	This is bound to be more data-efficient than a transformer. 
		Which is of course what I've been working on for a while .. 
		The crux (& well-known problem) is how to combine statistical / gradient approaches with pure symbolic datastructures. 
		Maybe do a form of distillation?
		Nothing really exceeds end-to-end SGD. 
			For end accuracy.  
			It's not compute efficient --
			But it does learn structure, 
			and it does leverage the lottery-ticket hypothesis, 
			and it's scalable / there is hardware support
	OK irregardless the data-representation, do we start with recursive sequence generation (easier)
	Or with (insert, replace, delete) continuation-based editing? 
	
Getting back to dreamcoder, I really like the idea of using upper and lower bounds for constraining probably approximately correct learning.  It seems like a more efficient way to learn a (conditional) manifold when samples are valuable, 
the space is unconstrained, yet locally there are low-dimensional constraints (e.g. well-form-edness). 

Anyway..  evaluating all possible sequences of 5 tokens is only 5M samples. Not so bad if it can be parallelized. 

October 5 2022

I was thinking in the shower: one reason there is friction is that we are interacting with effectively a AST in it's non-native, linear-string form. 
If we could interact with the AST directly, add / replace/remove nodes, then the program is much more likely to be well formed. 
What if in the library each code snippet has an associated hidden vector value (which can be updated via SGD?)
	This sort of knowledge base seems more interpretable. 
OTOH, working with strings is more general.
And easier to shoehorn a transformer into. 

Looking at https://github.com/huggingface/pytorch-openai-transformer-lm, 
it seems that for a classification head (e.g. class logits output), you just flatten the input over both tokens and embedding dimension, and output n_class. 
Pretty simple; I guess do the same? 
For the language model head, I think they just take the very last token (aka truncate), and predict from that. 
This also makes perfect sense, and might be a better way of going about it
But need to check actual function w/ pdb.

October 6 2022
Looking at the pytorch implementation of "Attention is all you need", 
The relevant bit of code is in Layers.py, DecoderLayer, 

   def forward(
            self, dec_input, enc_output,
            slf_attn_mask=None, dec_enc_attn_mask=None):
        dec_output, dec_slf_attn = self.slf_attn(
            dec_input, dec_input, dec_input, mask=slf_attn_mask)
        dec_output, dec_enc_attn = self.enc_attn(
            dec_output, enc_output, enc_output,  mask=dec_enc_attn_mask) 
        # query, key, value
        # query from the decoder output; key and value from the encoder.  we'll need to do the same.
        dec_output = self.pos_ffn(dec_output)
        return dec_output, dec_slf_attn, dec_enc_attn
        
Where pos_ffn is a position-wise feedforward network. 
	xf.py does the same thing, simply / implicitly. 
Hence: 
	-- query comes from the decoder (e.g. autoregressive), 
	-- keys and values come from the encoder layer. 
	-- mask is there to prevent attending to later tokens. 
	
I might be lazy, but I don't see why we can't do this with an encoder-only architecture, where the full program (perhaps with vector annotations?) serves as the input, and you only have to make edits. 

Question then is how to get the edit information?  
Judging from the pytorch-openai-transformer-lm, just flatten over tokens, and use a MLP output layer to map to encoding.. 
IDK, better to get started? 
	Probably will have to go back and fix later eh? 

Hmm now seems that I broke something... ocaml interpreter is being called twice.  (?)

Oct 7 2022
OK, it seems that the problem is that we're not handling the ocaml response properly - and, moreover, when the python program bonks, it leaves the ocaml program running, which then spits out an unending series of log. 

OK seems to be working fine!! 
Todo: add in option to not log, speed things up further.
Also: plug in the transformer :-)

October 17 2022

Transformer is plugged in!  
But it's not training -- probably because many sequences map to the same image, so there is no way for the transformer to determine which of the many sequences made the image. 
So, we need equivalence classes of some sort.. 
I guess do it now in the form of a datastructure?? 
Problem is that this is not probabalistic. (Whereas neural networks /are/)
Right, so this would suggest an 'equivalence' transformer as well as a translation transformer; or that the whole stack is not penalized until it's done, aka RL. 
	(As usual, I'll think about this for a bit before probably deciding that the simplest / most obvious solution is best..)
	
Well... the transformer really should average, right?  
Even if the printed loss is not zero, the actual loss ( in producing a program ) should be near zero.
	That is, it will learn equivalences implicitly ? 
	This is probably also why just GELU works better than softmax for ultimately reducing the loss. 
Thought: train contrastively?? 
	Yeah, that makes a lot of sense. 
	But first need to extend the database of images to something much much larger. 
	Probably this should be done in ocaml
		but then the issue is communicating between ocaml and python.. thousands of records might be slow? 
		Some sort of fast database, SQLlite?? 
			eh, one thing at a time. 

October 19 2022
	
OK, per notes -- need a better source of training images (at least to start .. want to train a transformer to convert images to segs, segs to programs. )
Definitely need some sort of 'library' ala dreamcoder to boostrap structure .. yes?  But then need an e-graph? 
How to even order them?? 
What would a library look like? 
Say, for drawing a polygon: 
def poly n (
	loop i, n, 
		( move (ul) (ua / n) ) )
and a circle: 
def poly n s (
	loop i, n, 
		(move (ul/n *s) (ua/n) )
def circle r (
	call poly inf r)

Right, so drawing a circle required a specialization to the poly drawing routine. 
Maybe I should implement procedures before getting any further ..
there is a lot riding on the type system in logo now! 

October 21 2022

Alright, procedures are implemented, and seem to be working well.  
	Along with comments, I might add -- needed because of the implicit stack-based (annoying) calling conventions. 
		Might need to change that in the future. 
		At least there are no stack collisions per se. 
	Also need to enforce recursion limits. 
	
Let's see, steps for 'evolving' a program. 
1.  Learn which constructs are syntatically / semantically permissible.  
	That is, learn the syntax of the programming language, typically from examples. 
	We can supply these examples; or, in the case the atoms are supplied directly, we search for syntatically permissible constructs / examples. 
		With, notably, analogistic generalization from other domains (e.g. human language)
	Remember those permissible constructs and -importantly- their effects. In basically a database. 
2.  Do heuristic line search on the constructs to try to improve them / fit them to the specification. 
	Typcially in human learning, there is a cirriculum of increasing complexity to mirror the staged complexity of the world and to allow progressive growth of the library + heuristics. 
	-- Interesting here would be to -bootstrap- this: is there enough guidance from either readily-available data to gain complexity
		Or, can you supply meta-heuristics for directing elaboration (e.g. beauty?)
		Maybe probably, that's how humans work.
	+ Also want to store this in a "cortical database of types" (or, in the meantime, transformer..)
3. Observe patterns (how?) in the execution graph, and refactor (e.g. replace, with specialization) long descriptions with shorter, more 'elegant' modes. 
	This heavily involves analogy-making. 
	Deep nets are seemingly good at analogies through vector algebra / arithmetic, 
		But no guarantees on data efficiency. 
		Things like SVD on sparse matricies of document word-occurrence etc also seem to work, so could be that there are many ways to uncover structures (hence axes / analogies) in datasets. 
		Something that I've been thinking about for a long time ... 
	For example, when converting a specification to a progam, one first step would be to just generate a whole list of line segments. 
	Take this program as an example, it plots 5 cricles. 
	
(d0 : ( // arguments: 0: n sides ; 1: size
	loop 2, v0, ( // 2 is loopvar (unused)
		move ((ul / v0) * v1), (ua / v0))) ;
move 0, (ua / 40) ; // center the circles
c0 20, 3; // these are empirical.. 
c0 20, 5;
c0 20, 8;
c0 20, 12;
c0 20, 17
)
	Those 5 circles are composed of 20 segments each, so we have 100 segments.  
	Each segment, if chained (which it can be) consists of 2 floats, 4 bytes each. 
	So the whole list of segments is 800 bytes. 
	The minimum resolution required to see this image is about 64 x 64, and even then segments of the smaller circles are not legible. 
	64x64 is 4k bytes, so the 100 segments is a decent approximation. 
	This same program is 137 bytes without comments (which again is not currently accurate, those comments are needed -- but if we had a better interpreter / runtime, they could be elided)
		Also, of course, some of the smaller circles are best approximated by lower-count polys. 
	
-------
Alright, given an image, can we output a series of segments? 
Then later, given a series of segments, can we output a program? 
Seems like, to bootstrap, you need a large list of segments to predict or something.  
I guess we can just run the stochastic evaluator and make some heuristics on 'good' images, then train up a transformer .. ? 
This does not avoid the invariance problem (segment order is invariant to starting position ; programs may output the same image with differing starting positions, depending on centration.)
	(Convnets permit a degree of translation invariance of course, but the ViT starts out withthout a whole-field convolution.)
	
Maybe there are clues?  
The mypaint test seemed to figure out stroke directionality OK, somehow.

October 27 2022

From the reading yesterday: invariance across different (task) domains implies causation.  
Not sure how general it is, but the example is a duck: it's form is invariant (up to perspective and lighting) across any number of contexts.  Can be a pond or the air or whatnot. 
Interesting idea. 

Think I just need to evolve the programs to produce something 'useful' -- and pay attention to how I do it, so that we can learn this E2E in the future. 

And really need to figure out the stack overflow. 

October 31 2022

Halloween!  
OK, made the edit-ast (as opposed to generate wholly new one); now need to integrate with the python code, to allow python to order the database of images -- 
images for training.. 

Nov 2 2022
Ok, that mostly works ... debugging, ever debugging. 
Time to itch the half-formed idea (hedon) that has been gradually forming.  

1. Humans default to data-efficient solution of problems, not highly statistical data-intensive random-explorers.  I think this is a fact to lean on.  
2. Humans use examples (templates), and typically modify them incrementally (templates are functions that are approximately linear in their parameters).  These examples are typically memorized; some approximation of the Jacobian is stored, too. 
3. **Examples themselves are little bits of code (functions)** -- that is, you can ‘execute’ or trigger memories to modify workspace representations & create intermediate representations.
	In the programming world, these mappings can be learned via experimentation. 
	This probably should hold for the rest of the world too?  Small-network power-law causality? 
		I don’t know about the visual world.. causes are much broader and more entangled.  Probably better to not think about that for now. 
		Also: many snippets of code cannot be readily inverted; they must be run to see what they do. 
			We’re more interested in the computationally invertible snippets.
4. Intermediate representations are often essential to the process of compression. 
5. Humans cache all the representations, especially the IR, to aid in compression.
6. Iterative search proceeds across different levels of representation. 
7. Slow reasoning & search is gradually converted to long-term memory. Statistical regularities in aggregate experience are abstracted away / compressed (how?!). 
8. The system is meta-aware: statistical regularities in its own processes are also converted to long-term memory for shortcut reasoning. 
9. Aesthetics is absolutely essential for open-ended, unguided search. 
	Though this may or may not work so well for biology... where beauty arises spontaneously, by necessity - rather than as a heuristic to speed creation.   
	Aesthetics would have to be controlled via a meta-parameter. 

How a human programmer would solve the logo task
1. Iteratively convert an image to a set of line segments or ‘move’ commands
2. Sort / order those line segments into continuous curves and shapes
	In the case of commands, might need to alternately append / prepend / insert as needed? 
	This is a little bit more complicated
3. Reproject sequences of segments into length and angle space (per logo protocol)
	This is perhaps the most interesting and critical thing to learn from experimentation -- the causal disentangled space is not coordinates, but angle and distance. 
	Seemingly can learn this by forcing computation in the program space, rather than segment intermediate space (the parameters are length and angle)
4. Iteratively search, perhaps based on analogy or something like DNA-alignment algorithms, for encountered programs that generate parts of the sequence.  
	Segmentation (clustering) can be based on region of space, or similarity in coordinate / length / angle spaces.  
5. Memorize virtually everything, and use that for jump-off points for iterative compressive search.

I’m fairly confident that I can get all this working (except 5) with some tweaking on all of the logo tasks described in DreamCoder. 
But it’s still me doing the reasoning, forward-backward passes through the simulation that constitutes planning. 
Ideally we don’t want to do that, we want to make something that is very open-ended and can be used for arbitrary problems. 
e.g. inverse graphics -- if we can do that, then virtually anything is possible! (And is a substantial step toward AGI, or at least narrow information pumps.) 

Human #5 above is the hardest, but also the most critical -- it’s how humans can take on infinite-dimensional problems, and tackle them iteratively 
	Not that this is easy -- much discussion over the past few days has been the vast challenges of ‘automating’ or speeding up scientific research.
		And one must remember that, even with data-efficient humans, you need 10,000 hours of practice at something to become an “expert”. 
How do you store these memories? 
DreamCoder takes the sensible approach of storing knowledge in both libraries (commonly used and useful code) and neural networks (low-N prototypical networks, could be better).  
Humans of course do both, we remember nice little snippets of code, motifs and templates, and can deploy them by modifying in-place to fit the need at hand. 
Libraries are a bit higher bar for abstraction, and usually serve based on code-and data-flow analysis to see common patterns. 
	They supplement human memory capacity.  
		Even AI will probably need such multi-level memory (with of course (vectoral?) comments or something to make is searchable)
		Yet I think it’s not required for initial POC. 
	This can be trivial or can potentially take large quantities of compute, raw experimentation; as mentioned, not every forward computable function has a computable inverse. (Most don’t , probably!)
	Learning things like polar coordinates requires both experimentation and aesthetics ... which atm is really hard to quantify. 

Also of importance is setting up proximal goals: when fitting the segments, you can have a proximal goal of matching the angle, then a goal of matching the length. 
	Seems to require a bunch of machinery, but that’s the way things are. 

Human programmer input:
Atoms -- these are the things you can say.  
	Pretty sure this has to be provided; otherwise it’s unknowable.
Syntax -- this is how you can say it. 
	Can be learned from examples and from experimentation
Examples -- these are well-formed expressions in the language. 
Specifications -- these are well-formed outputs from executing the language.
Breakpoints, debug etc -- yield intermediate representations of what the program is doing.
	Seems to require “reasoning”!
Similarity metric -- some way of knowing if you’re getting closer (even if it’s dumb..)
	And ways of making local similarity, eg angle

I think, with atoms, syntax/examples, specifications, and similarity, it should be possible to bootstrap & solve all logo tasks. 
This is an RL task?  Output a series of actions to solve a given task... only the problem formalism is messier than what’s assumed in RL (MDP task)
	Hence, need to refine the formalism.  
		Pure supervised doesn’t work well because of the invariances / equivariances. 
		Model-based RL might work, ala AlphaZero / MuZero / EfficientZero?  With BPPT, i think. 
		You ignore the complexity of the program to segments space by simply exploring only a small part of it; 
		When using random enumeration, transformers or whatnot might become confused because the function they are trying to map is degenerate (many programs can generate the same image). 


Ok, formalism: 
Given: 
	A - a current program fragment (possibly none)
	B - a current set of example programs (not none)
	C - a set of atoms (small, finite)
	D - a specification (in this case, an image)
create a program that generates an image that better matches the specification. 
and
do this in such a way that search is amortized. (bootstrapping)
then
operate on the program to maximally compress it. 

Analogies to MuZero: 
	A - state of the game board
	B - examples would need to be turned into action sequences
	C - action vocabulary is predefined (if much larger than games -- includes the option of ‘moving’; see below)
	D - to win against an opponent (or not, in the case of Atari). This is more complicated than a fixed specification, from a game-theoretic standpoint. 

The example programs and atoms serve as ‘actions’ to modify the current program. (through fuzzy difference-taking?)
The role of the network/memory is to form a policy: given the specification and prog fragment, select an action, consisting of tuple: { atom / example + ast location + add/replace/remove }

Later, an email to Richard: 
Should you care to read, I'll try to un-muddy my explanation today of what I'm working on: 

1. Humans can learn through memorizing cause-effect pairs.   (In the realm of program synthesis, this is through experimentation)
2. This memory hence serves as causal or temporal inversion: given an effect, infer causes. 
3. Causes can be converted into actions (e.g. more experimentation) which serves to bootstrap (1)
4. Cause-effect pairs and their inverses can be stacked hierarchically, inline with the compositional (computational) structure of the world (computers)
5. Effects that can't be inverted via memory can be inverted via search (which solves all problems, just slowly).  Search then emits more examples for (1). 
6. Making the system re-entrant allows for larger patterns to be found and compressed. (eg. refactoring large codebases) 

There is a lot more icing to put on that already many-layered cake, but if there were a data structure that supports 'fluid' association of cause-effect chains, then it seems likely you'd be able to implement many of the layers.  

I'm proposing that the human cortex does something like this, vis the consistently polarized nature of pyramidal neurons (basal dendrites = causes; apical = effects / addresses) & it does so in a way (default-memorization) that supports data efficiency.  Representations would be vectors, same as in deep learning, though it's somewhat unsatisfying.  (The world is much more discrete than a bunch of vectors in an isotropic high-d space ... yet if it works it works right?)

Nov 3 2022
Why can’t we use something like a LLM for solving this problem? GPT-3 has some truly remarkable properties! 
----
I think we can’t readily use it because 
(1) We want to bootstrap; the initial dataset is too small. 
(2) Because we’re bootstrapping, the dataset is also non-stationary and non-ergodic
(3) This is not how humans learn; they do experiments. 
(4) Humans learn an expanding pallette of examples to draw from. 
(5) Aesthetics matter significantly 
(6) Transformers don’t have a default way of dealing with equivariance / invariance / degeneracy; contrastive training is inefficient.

Responses: 
(1) Why not just hammer-train with small datasets, memorize it, see what happens?  
The vector hypothesis is really very strong & makes a lot of sense & seems to work super well! 
However, in current networks, the general idea is that the dataset is static, ergodic, and the network is a static size too. 
Why not expand the size of the network as need arises?  
	Well, because that’s another thing to account for; static allocation is simpler... 

Back to the basics; what exactly is a transformer doing? 
It’s executing a message-passing and ‘search’ (through query-key) routing algorithm for calculating next-tokens. 
& transformers are typically autoregressive. 
As Andrej Karpathy accurately points out, it means that they are effectively being ‘programmed’ through examples.  
The FAIR number-sequence paper shows that, given enough examples & enough capacity, a transformer can learn approximate-inverse heuristic algorithms to invert even hard autoregressive numeric algorithms. 
	Very very surprising & something to lean upon! 
Maybe one way of dealing with (1) & (2) is to successively train new transformers of differing sizes as the distribution changes?? 

(3) We can incorporate experiments by using search to iteratively broaden the training corpus. 
Still, this is not what humans do -- they do linear approximations, and beam search or Newton’s method based on locally linear approximations.
	I suppose this can just be hard-coded. 
	Getting something that makes good experiments will require some experiments! 

(4) I think the default -- labeling the examples with random-parameter vectors (which can be changed through SGD - maybe? have to experiment) is one way of making it work.. 

(5) I don’t know how to enforce this, other than putting a strong prior on simplicity of code, e.g. store only the shortest examples that produce certain output.  Again, experimentation! 
	Pretty much did this, it’s still slightly unsatisfying? 

(6) This seems like as large a problem as (2) and (4).  The way that it evinced earlier was that the fully-trained transformer would output “move move ul, ul ..” because there could be a leading space.  
Of course, that’s not actually in the dataset; the transformer is improperly doing interpolation. 
	Maybe just train it for longer? 
	Or likely I have a bug in the code somewhere?  
		Aside: I love ocaml, too bad most of the libraries are in python (though certainly I can learn to use datastructures better in python?)

November 10 2022

Some discoveries!  
Upside-down RL, invented about three years ago -- seems to have a bit of traction in the form of using a Transformer to model. 
Idea is that, you have a bunch of trajectories (state, action, reward) in your replay buffer; rather than taking those and producing a policy of Q(s,a), you instead indicate the desired return and the present state, and use the model to produce an action -- the policy is implicit. 
They have showed that, in some cases (SeaQuest), you can even get better performance than what’s in the training buffer!  E.g. the transformer generalizes out-of-distribution.  (In most cases, it does not generalize).  
The transformer paper does not explicitly tune the replay buffer.  In the Schmidhuber paper, there is truncation selection in that the model is trained only on the to N past trajectories, as organized by total reward.  
Even then, they show the system can remember old rewards and produce appropriate trajectories when appropriately queried. 

Another discovery: https://ogma.ai/2019/08/acting-without-rewards/
This mirrors the UDRL idea, sorta. 
The idea is that you learn tuples: 
(state1, action, state2) 
& from this derive a change in state d_state so 
(state1, state2, action, dstate12).  
Now, say you are in state1 again, and want to get to state2 (maybe approximately)
Simply look it up in your memory store!  And viola, you have an action.  
	Actions generate a change in state, hence can be indexed & memory-inverted by setting a goal state and delta state. 
Add in reward and something like truncation selection, with interpolation, and you can do some reinforcement learning & presumably bootstrapping.  

This gets back to a core feature: most things a neural network can do can be accomplished by a database & suitable search-ordering system.  
Neural networks are generally fast access & afford interpolation, though. 
Transformers are unique in that they exist partway between database / look-up table and algorithm. 
As argued above, the cortex probably does the same; memories as bits of code etc. 
	Additionally, imho the cortex is a ‘more effective’ form of self-organizing database & has systems for fast, parallelized search (which maybe takes advantage of entropy). 
	The cortex implements forms of algorithmic compression maybe by exploiting this innate search + self organization to yield ‘causes’ or ‘addresses’. 
Also to note: even if this general database conception is correct, it still does not obviate the need for great skill in constructing the database & indexing mechanisms
	Or in great skill in constructing the neural network & training mechanisms, as is clear from history 
		(And is emphasized in the UDRL paper, which says that the algorithm works OK even despite not having a decade of research put into it.)

Ogma AI seems to have solved the inter-layer coordination problem, perhaps ala Sidney Lowe (just ignore it?), which is otherwise solved by backprop. 
	Not really a lot of details of his implementation, and indeed no math, which is troubling.. the code is a bit meh tbh.  But he does seem to be a clear thinker at least. 

OK, say we were going to solve this problem exclusively in ocaml. 
can easily make a database of: (state1, state2, action, dstate12)
	Even though I’ve been lax with testing it.. 
There are a LOT of actions that simply do nothing, and hence should not be added to the database.  
Say for solving a task with only 2 segments, this is not hard: you just do a look-up table.  
If you’re approximating the function using a transformer or vision transformer, real problem in that the mapping is not smooth (I think?). 
	And as mentioned above, it’s not how humans do it. 
	Also, a lot of these Atari tasks are not smooth, and yet people have been chipping away at them, quite productively! 
I think instead you want a flexible database, where by default you search the whole thing & later you do some sort of faster associative lookup. 
OK, start with (move ua, ua) -> one horizontal line & the associated segment. 
options are to change move (to a binop?  does nothing)
or change the arguments (to other constants, to binops, or to other variables; the latter does nothing..)
or introduce a loop (which will likely be very confusing to the system); this is an action to refactor the code, really. 

Now, we have the problem that the data looks pretty heterogeneous: images, variable length lists, tree structures, libraries of tree structures...
	e.g. we’d need to do 
	reprojection via Inception for the images;
		Or some BYOL approach.  
	BLAST for the variable length list; 
	some sort of graph matching for the tree structures; 
		maybe later; transformers for now? 
	iterative matching for the libraries. 
Searching and matching on any of these seems highly non-trivial; how is the brain so good at it? 
One obvious thing it does is to serialize otherwise difficult searches (spread the problem out in time) via attention. 
	Maybe attention is just holding one target in memory & seeing what auto-associates with it. 
	I really like the analogy to protein docking in solution.  Takes advantage of entropy! 


November 11 2022

Right, change_ast does seem to do something; ‘appears’ to work properly, though likely have to constrain this a good bit more.  
I like the idea of keeping it encoded as strings -- this is much more manageable than a tree (for the time being). 
Pretty easy to build up a library of these minimum-edits, i guess.  
	In the case of needing to do multiple edit operations before the DUT program works again,
	think that the UDRL + transformer approach might work fine? 
Next task is to allow genetic-algorithm like copying of one segment into another. 
	This needs to be from a bunch of templates.. 
	How to determine the edges of the edits? 
	Think we need a good string library (or regexp library??)
Then, the task is to search over the templates.
	Looked at the Smith-waterman algorithm, idk, almost want to initially take a brute force approach? 

I had better run before it gets dark.  Also need to do some upper-body workout..

November 12 2022

Alright, painted the laptop (Creator 17) for the second time!  This time it will look good! Hopefully.  Have to let it dry very completely (>3 days) if i need to repaint it.  

So, regarding yesterday, I was thinking: 
	1 -- The edges of the edits.  This takes a long time for even humans to learn!  It can be clear based on pauses in speech, but the phonemes  take time to disentangle.  
		Might be as simple as pattern matching (blind signal separation or sparsification) on the input stream, 
		Which is of course something that many people have been working on for a long time; neural networks might well be OK at this.  
		Other sequence alignment spaces, eg temporal convolution used in matching protein motifs, could work well - ? 
		Sequence alignment itself is an intensely studied problem. 
	2 -- If you have small enough convolutional filters, you could do something like the BLAST algorithm (which uses i think a fixed vocabulary?)
	3 -- Should investigate some of these tree RL and tree transformer algorithms.  

November 14 2022

UDRL + transformer approach to solving the problem.  
Can presently easily generate a database of small, well-formed and productive logo programs. 
Take this database, select a subset of it (biased toward smaller samples), and find the closest different program. 
Call them progA and progB. 
Given this, input to the transformer the context (imageA, imageB, deltaAB, progA, progB)
	Add in segments too?  hmm.  seems like too much data. 
Add in location coding for both the images and the strings. 
Add in a cursor location so the model can remember what it was doing
Supervise the model to output delta progA progB <end-of-edit>.  
Pray that it generalizes from the experiences. 
Allow some bootstrapping -- generate random programs, hence imageB, and ask it to suggest edits to recreate the image. 
Keep it around if it happens to run and produce ‘interesting’ output. 

Do this, if it works somewhat OK, then worry about adding in compressional edits. 

Thoughts: 
	Seems like it’s pretty easy to make a system that doesn’t work at the beginning. 
	For example, in the self-predictive representations RL paper (basically BYOL), 
	They find that only using the L2 loss causes representation loss, 
	whereas using cosine loss (normalized L2) seems to work fine & does not collapse. 
	Little things like this differentiate an experienced practicitoner vs .. me haha. 

November 16 2022

Working between Ocaml and Python is a real PITA ... spent the better part of the afternoon debugging a communication problem. 
Anyway, the goal now is pretty clear: 
given the context 
	Images A, B, and their difference -- all black and white
	Program A and program B (very different positional labels + location labels. )
output an 
	Edit command (insert, delete, sub, done -- one hot / softmax output)
	Location (in the same coordinates as the positional labels of course)
	Character (in one hot coordinates, same as the input)

Keep the edits pretty small, 1-5 .. for the time being .. then we can work on dramatically larger edits (aka refactoring!). 
	Make it work first! 

December 1 2022

December already!  
Tried running the full model on lambda cloud -- and it seems slower than my laptop. 
Normally, this wouldn’t be a problem, but it’s cloudy today so am not getting as much solar power. 

Anyway, todo: now need to bootstrap, eg. have the Transformer continually output programs, which then enter into the corpus of training data. 
This requires a second level of communication: 
-- python needs to apply the edits (which it already does), 
-- then it needs to transmit the encoding to ocaml
-- which needs to decode it and convert to a prog, e.g. AST
-- which can then be run by ocaml to produce a (progra, encoding, image) tuple
-- which is then transmitted, interleaved, into the databases of both python and ocaml.  

Sure would be much easier to have another way of distributing this processing, especially given that cloud instances seem to have lots of CPUs and GPUs, and it would be great to take advantage of this.  
This would also eliminate the complexity (read: stupidity) of having two copies of the data in two different places. 
	Such a bug has already bitten me once ... 
	That said, protocol-buffers are probably faster than a database?  
		Is there not a reasonably native database? 
		DAta schemas are easier to change when we don’t have that external dependency.. 
			Maybe punt on this one until we have more resources.  
			In the meantime, just have to be careful. 

Tomorrow: todo: make sure the transformer can memorize nothing -> line segments.  

Dec 2 2022

Made the program database larger -- 10k -- and immediately the initial loss was lower! (50 vs 62; batch_size = 24)
As it was compiling the database of programs, was obvious that many more compliated programs were being replaced by simpler ones.  Good!
Also tripled the number of iterations for training, since it seems we have enough solar / battery for the time being (middle of the day, though overcast)

After 228k iterations, loss is hovering around 42 (batch size 24); It does seem to be gradually decreasing, just not very quickly.
Really need to get the bootstrapping code working; worth cranking on this.
	Note: after 300K iterations, the loss did not decrease anymore.
	Still at 42; assume that it won't go down further?
	Errors seem pretty broadly distributed across type, location, and character.

I think, for this, we make a normal batch w/ unkown program b (sampled from the database) & ask the network to produce edits until it otputs a "fin" token.
Then, send this to ocaml, as above, and ask it to execute.
If it fits, per the same criteria as the ocaml stochastic AST generator, then add it to the database.
I guess do this every so often?

Also very important: things are getting to the point that starting from a blank slate isn't acceptable anymore.
Need to start from a pre-trained transformer & pre-filled database of images.
Redis integration time?

December 5 2022

Testing to see if the transformer can completely memorize the first 100 image -< program pairs, doing only insertion and no deletion or substitution. 
This is with the 10k database, as I tested in the desert. 
Hmm .. something seems off, 'insert' seems to be always the same, when it all should be asynchronous and hence varying on the sample clock (at least, with the sometimes longer programs) 
Got a bug in there! 

Redis doesn't look to hard.  Storing more complicated datastructures can perhaps be done through the JSON library; or by using multiple hash tables (which are an builtin structure)

OK, there are more bugs:  the transformer cannot memorize the simplest 100 program-image pairs: 
	after 300k iterations, slowloss is still 47 (batch 24)
	Todo: plot the loss
Hmm.  Need to test things more. 

December 6 2022

The loss seems to be gradually *increasing* over time ... its spiky though.  
Why would it be increasing?  Something in the model is ill-conditioned?  I need to look at the training data more before poking around with the model. 

I think i need to keep a log of the errors -- where the model goes wrong, and why, to better understand why it's not converging.  
Seems like it's stuck in some sort of equlibirium, where some of the batch members can't be accurately decoded; or, some members are too hard to decode because the space is too convoluted. 

December 7 2022

Seems to be training better... 
Todo: in cases of + and *, can reorg so that the largest operand is first. 
This removes some invariance. 
Check check. 

December 8 2022

Uff, even with a higher learning rate, the system does not converge.  
E.g. it can't even output 'E' at the beginning of the string.  
What's broken?  
Can it do classification?? 
Can it output a fixed string? (e.g. positional dependent production, all "ins". 
	This is easy to check -- train on one example.
		Always seems that I have to touch nearly everything before it starts to work.  Figures. 
Nope, sure doesn't seem to be learning. 
Ah, bug in the insert method ... fixed. 
	Proves that you have to instrument nearly everything!
Plateaus at a loss of 25. 
With occasional blips.. 
-- It always gets the editing position correct, good
-- It's perfect at the 'fin' token
-- When there is no context, at the start of the edits, it gets both the operand and character wrong (loss= 48)
-- At the end of the string, decode is correct, but loss is still ~30. 
-- Other two errors are for putting the operand as 'fin' when it should be 'ins'. 

Seems that the activations are blowing up.  Hmm.  how / why? 
Seems that they blow up really fast at the beginning... very unstable model, not sure why. 
Bad initialization? 
prt_to_edit seems especially wrong! 

WHEW.  Adding appropriate Softmax (don't be lazy, tim) and scaling the learning rate + adding gradient clipping seems to have fixed things, it converges with one example now :-)
Let's try 10 examples.

Yep, all works, even with 1k examples, and gets ~ 50% correct (11 out of 24) when trained for 100k timesteps.  !!Super excited!!
Next todo: take the emitted programs, and run them. 
If they run, see if they have 'about the same' cost and image result as a program in the database. 
If it does, then replace the items in the database. 
If they don't collide, add to the database.  
Then, see if this works... 
	Will definitely need to start writing to disk pretty soon.
	
Also need to add in substitutions and deletions to the list of actions.. 

OK, so this commit (Dec 8 2022, link below) has very good performance -- probably it's memorizing the dataset but so what -- only gets 5/24 errors on the test batch.  2x better with the learning rate schedule! 
https://github.com/tlh24/cortex/commit/d2ab09ba7012c17205eb9b9d574557ccda5b0d34
